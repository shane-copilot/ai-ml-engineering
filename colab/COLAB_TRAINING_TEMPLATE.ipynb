{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a208c20b",
   "metadata": {},
   "source": [
    "# Qwen3 0.8B Training Template - Colab\n",
    "\n",
    "**Purpose:** Proven template for training Qwen3 on A100 GPU\n",
    "\n",
    "**This template works!** Created after successful Phase 1 (Linux Commands) training.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù How to Use This Template:\n",
    "\n",
    "1. **Copy this file** and rename for your phase (e.g., `phase2_python_training.ipynb`)\n",
    "2. **Update configuration cell** (Cell 4):\n",
    "   - `MODEL_NAME` - base model or merged model path\n",
    "   - `OUTPUT_DIR` - unique output directory name\n",
    "   - `DATASET_NAME` - your HuggingFace dataset\n",
    "3. **Customize dataset formatting** (Cell 9) if needed\n",
    "4. **Run all cells** - training will start automatically\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Proven Configuration:\n",
    "- **GPU:** A100 (40GB)\n",
    "- **Training Time:** ~4.5 hours for 3 epochs\n",
    "- **Batch Size:** 4 (effective 16 with gradient accumulation)\n",
    "- **LoRA:** r=16, alpha=32\n",
    "- **Memory:** 4-bit quantization + gradient checkpointing\n",
    "- **Padding Token:** 151645 (CRITICAL - don't change!)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Tested Phases:\n",
    "- ‚úÖ **Phase 1:** Linux Commands (AnishJoshi/nl2bash-custom) - SUCCESS!\n",
    "- üìù **Phase 2:** Python System Automation - Ready to test\n",
    "- üìù **Phase 3:** Advanced Python - Ready to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca299c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl torch\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd810d3b",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Configuration\n",
    "\n",
    "**EDIT THIS CELL for each new phase:**\n",
    "- Change `MODEL_NAME` if using merged model from previous phase\n",
    "- Change `OUTPUT_DIR` to unique name (e.g., `qwen3-phase2-python`)\n",
    "- Change `DATASET_NAME` to your HuggingFace dataset\n",
    "- Adjust `NUM_EPOCHS`, `BATCH_SIZE` if needed (current settings work great!)\n",
    "\n",
    "**DO NOT CHANGE:**\n",
    "- `PAD_TOKEN_ID` - MUST be 151645 for consistency\n",
    "- LoRA config values (proven to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THIS FOR EACH PHASE\n",
    "MODEL_NAME = \"DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B\"  # Change to merged model path if sequential\n",
    "OUTPUT_DIR = \"./qwen3-phase1-output\"  # Change for each phase\n",
    "DATASET_NAME = \"AnishJoshi/nl2bash-custom\"  # Change to your dataset\n",
    "\n",
    "# Training hyperparameters (proven to work on A100)\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# LoRA config (don't change - this works!)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# CRITICAL: Same padding token across ALL phases\n",
    "PAD_TOKEN_ID = 151645\n",
    "\n",
    "print(\"‚úÖ Configuration set\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb25b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer from HuggingFace...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# CRITICAL: Force same padding token\n",
    "tokenizer.pad_token_id = PAD_TOKEN_ID\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"   Pad token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda52937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Model from HuggingFace\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nConfiguring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb29307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "print(\"\\nApplying LoRA configuration...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n‚úÖ LoRA applied\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nSample entry:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8197dc",
   "metadata": {},
   "source": [
    "# üìù Dataset Formatting\n",
    "\n",
    "**CUSTOMIZE THIS CELL for your dataset structure:**\n",
    "\n",
    "Different datasets have different field names:\n",
    "- Some use `description` + `cmd`\n",
    "- Some use `question` + `answer`\n",
    "- Some use `instruction` + `output`\n",
    "\n",
    "Update the `format_instruction()` function to match your dataset's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset - CUSTOMIZE THIS FOR YOUR DATASET STRUCTURE\n",
    "def format_instruction(example):\n",
    "    # Adjust field names based on your dataset\n",
    "    description = example.get('description', example.get('question', example.get('instruction', '')))\n",
    "    command = example.get('cmd', example.get('answer', example.get('output', '')))\n",
    "    \n",
    "    prompt = f\"Instruction: {description}\\n\\nResponse:\"\n",
    "    text = f\"{prompt} {command}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_instruction)\n",
    "print(\"\\n‚úÖ Dataset formatted\")\n",
    "print(\"\\nExample:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6116bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining on {len(dataset)} examples...\")\n",
    "print(f\"Total steps: {len(dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION) * NUM_EPOCHS}\")\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf495ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is on GPU for saving (fixes potential issues)\n",
    "model = model.to(\"cuda\")\n",
    "print(f\"‚úÖ Model moved to GPU: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LoRA adapter\n",
    "print(\"\\nSaving LoRA adapter...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "import os\n",
    "adapter_path = f\"{OUTPUT_DIR}/adapter_model.safetensors\"\n",
    "if os.path.exists(adapter_path):\n",
    "    adapter_size = os.path.getsize(adapter_path) / 1e6\n",
    "    print(f\"‚úÖ Adapter saved: {adapter_size:.1f} MB\")\n",
    "    print(f\"   Location: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Adapter file not found at {adapter_path}\")\n",
    "    \n",
    "# List all saved files\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "for file in os.listdir(OUTPUT_DIR):\n",
    "    size = os.path.getsize(f\"{OUTPUT_DIR}/{file}\") / 1e6\n",
    "    print(f\"   {file}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a375b23",
   "metadata": {},
   "source": [
    "# ‚úÖ Training Complete!\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Download the adapter** from Colab Files panel:\n",
    "   - Navigate to your `OUTPUT_DIR` folder\n",
    "   - Download `adapter_model.safetensors` (should be ~40-50MB)\n",
    "   \n",
    "2. **Save this notebook** to Google Drive (File ‚Üí Save)\n",
    "\n",
    "3. **Sync to local machine:**\n",
    "   ```bash\n",
    "   cp -rfv /home/archlinux/GoogleDrive/* /home/archlinux/code_insiders/ml_ai_engineering/colab_drive/\n",
    "   ```\n",
    "\n",
    "4. **For next phase:**\n",
    "   - Copy this template\n",
    "   - Update configuration cell\n",
    "   - Run all cells\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Performance Notes:\n",
    "- Record your training time\n",
    "- Note any errors or issues\n",
    "- Compare with Kaggle T4 performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
