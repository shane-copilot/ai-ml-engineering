{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e7d499",
   "metadata": {},
   "source": [
    "# Colab Training Test - Qwen3 0.8B on Linux Commands\\n\n",
    "\\n\n",
    "**Purpose:** Compare Colab Pro vs Kaggle for training ease and speed\\n\n",
    "\\n\n",
    "**Dataset:** AnishJoshi/nl2bash-custom (Linux commands)\\n\n",
    "\\n\n",
    "**Expected Time:** 2-3 hours on A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec83452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\\n\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl torch\\n\n",
    "\\n\n",
    "print(\"\\\\n✅ Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3230df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\\n\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\n\n",
    "from datasets import load_dataset\\n\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\\n\n",
    "from trl import SFTTrainer, SFTConfig\\n\n",
    "\\n\n",
    "print(f\"PyTorch version: {torch.__version__}\")\\n\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\\n\n",
    "if torch.cuda.is_available():\\n\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\\n\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de79e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\\n\n",
    "MODEL_NAME = \"DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B\"\\n\n",
    "OUTPUT_DIR = \"./qwen3-colab-linux-test\"\\n\n",
    "DATASET_NAME = \"AnishJoshi/nl2bash-custom\"\\n\n",
    "\\n\n",
    "# Training hyperparameters\\n\n",
    "BATCH_SIZE = 4\\n\n",
    "GRADIENT_ACCUMULATION = 4\\n\n",
    "LEARNING_RATE = 2e-4\\n\n",
    "NUM_EPOCHS = 3\\n\n",
    "MAX_SEQ_LENGTH = 2048\\n\n",
    "\\n\n",
    "# LoRA config\\n\n",
    "LORA_R = 16\\n\n",
    "LORA_ALPHA = 32\\n\n",
    "LORA_DROPOUT = 0.05\\n\n",
    "\\n\n",
    "# CRITICAL: Same padding token\\n\n",
    "PAD_TOKEN_ID = 151645\\n\n",
    "\\n\n",
    "print(\"✅ Configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\\n\n",
    "print(\"Loading tokenizer from HuggingFace...\")\\n\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\\n\n",
    "\\n\n",
    "# CRITICAL: Force same padding token\\n\n",
    "tokenizer.pad_token_id = PAD_TOKEN_ID\\n\n",
    "tokenizer.padding_side = \"right\"\\n\n",
    "\\n\n",
    "print(f\"✅ Tokenizer loaded\")\\n\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee286076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\\n\n",
    "print(\"=\"*60)\\n\n",
    "print(\"Loading Model from HuggingFace\")\\n\n",
    "print(\"=\"*60)\\n\n",
    "\\n\n",
    "print(\"\\\\nConfiguring 4-bit quantization...\")\\n\n",
    "bnb_config = BitsAndBytesConfig(\\n\n",
    "    load_in_4bit=True,\\n\n",
    "    bnb_4bit_quant_type=\"nf4\",\\n\n",
    "    bnb_4bit_compute_dtype=torch.float16,\\n\n",
    "    bnb_4bit_use_double_quant=True\\n\n",
    ")\\n\n",
    "\\n\n",
    "print(f\"Loading {MODEL_NAME}...\")\\n\n",
    "model = AutoModelForCausalLM.from_pretrained(\\n\n",
    "    MODEL_NAME,\\n\n",
    "    quantization_config=bnb_config,\\n\n",
    "    device_map=\"auto\",\\n\n",
    "    trust_remote_code=True\\n\n",
    ")\\n\n",
    "\\n\n",
    "print(\"Preparing model for LoRA training...\")\\n\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\\n\n",
    "\\n\n",
    "print(f\"✅ Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\\n\n",
    "print(\"\\\\nApplying LoRA configuration...\")\\n\n",
    "lora_config = LoraConfig(\\n\n",
    "    r=LORA_R,\\n\n",
    "    lora_alpha=LORA_ALPHA,\\n\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\\n\n",
    "    lora_dropout=LORA_DROPOUT,\\n\n",
    "    bias=\"none\",\\n\n",
    "    task_type=\"CAUSAL_LM\"\\n\n",
    ")\\n\n",
    "\\n\n",
    "model = get_peft_model(model, lora_config)\\n\n",
    "print(\"\\\\n✅ LoRA applied\")\\n\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4cd9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Linux commands dataset\\n\n",
    "print(\"Loading Linux commands dataset...\")\\n\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\\n\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\\n\n",
    "print(f\"\\\\nSample entry:\")\\n\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for Linux command training\\n\n",
    "def format_instruction(example):\\n\n",
    "    description = example.get('description', example.get('question', ''))\\n\n",
    "    command = example.get('cmd', example.get('answer', ''))\\n\n",
    "    \\n\n",
    "    prompt = f\"Instruction: {description}\\\\n\\\\nResponse:\"\\n\n",
    "    text = f\"{prompt} {command}\"\\n\n",
    "    return {\"text\": text}\\n\n",
    "\\n\n",
    "dataset = dataset.map(format_instruction)\\n\n",
    "print(\"\\\\nFormatted example:\")\\n\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53534e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\\n\n",
    "training_args = SFTConfig(\\n\n",
    "    output_dir=OUTPUT_DIR,\\n\n",
    "    num_train_epochs=NUM_EPOCHS,\\n\n",
    "    per_device_train_batch_size=BATCH_SIZE,\\n\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\\n\n",
    "    learning_rate=LEARNING_RATE,\\n\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\\n\n",
    "    logging_steps=10,\\n\n",
    "    save_strategy=\"epoch\",\\n\n",
    "    fp16=True,\\n\n",
    "    optim=\"paged_adamw_8bit\",\\n\n",
    "    warmup_ratio=0.03,\\n\n",
    "    lr_scheduler_type=\"cosine\",\\n\n",
    "    report_to=\"none\",\\n\n",
    "    dataset_text_field=\"text\",\\n\n",
    "    packing=False,\\n\n",
    ")\\n\n",
    "\\n\n",
    "print(\"✅ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4518307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training\\n\n",
    "print(\"\\\\n\" + \"=\"*60)\\n\n",
    "print(\"STARTING TRAINING\")\\n\n",
    "print(\"=\"*60)\\n\n",
    "\\n\n",
    "trainer = SFTTrainer(\\n\n",
    "    model=model,\\n\n",
    "    args=training_args,\\n\n",
    "    train_dataset=dataset,\\n\n",
    "    tokenizer=tokenizer,\\n\n",
    ")\\n\n",
    "\\n\n",
    "print(f\"\\\\nTraining on {len(dataset)} examples...\")\\n\n",
    "print(f\"Total steps: {len(dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION) * NUM_EPOCHS}\")\\n\n",
    "print(\"\\\\nStarting training...\\\\n\")\\n\n",
    "\\n\n",
    "trainer.train()\\n\n",
    "\\n\n",
    "print(\"\\\\n\" + \"=\"*60)\\n\n",
    "print(\"✅ TRAINING COMPLETE!\")\\n\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc432e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LoRA adapter\\n\n",
    "print(\"\\\\nSaving LoRA adapter...\")\\n\n",
    "model.save_pretrained(OUTPUT_DIR)\\n\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\\n\n",
    "\\n\n",
    "import os\\n\n",
    "adapter_path = f\"{OUTPUT_DIR}/adapter_model.safetensors\"\\n\n",
    "if os.path.exists(adapter_path):\\n\n",
    "    adapter_size = os.path.getsize(adapter_path) / 1e6\\n\n",
    "    print(f\"✅ Adapter saved: {adapter_size:.1f} MB\")\\n\n",
    "else:\\n\n",
    "    print(f\"❌ ERROR: Adapter file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fcb66",
   "metadata": {},
   "source": [
    "# Summary\\n\n",
    "\\n\n",
    "**Colab vs Kaggle Comparison:**\\n\n",
    "\\n\n",
    "Record your observations:\\n\n",
    "- Setup time\\n\n",
    "- Training time\\n\n",
    "- Ease of use\\n\n",
    "- GPU type assigned\\n\n",
    "- Any errors encountered\\n\n",
    "- Overall experience"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
