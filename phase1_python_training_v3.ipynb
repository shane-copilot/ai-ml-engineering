{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e2a89d",
   "metadata": {},
   "source": [
    "# Phase 1: Python & Advanced Python Training\n",
    "\n",
    "**NEW APPROACH:** Skip Linux commands (model already knows them)\n",
    "\n",
    "**CRITICAL FIXES:**\n",
    "1. ‚úÖ Uses proper Qwen3 chat template format\n",
    "2. ‚úÖ Lower learning rate (5e-5) to prevent overfitting\n",
    "3. ‚úÖ Weight decay (0.1) for regularization\n",
    "4. ‚úÖ Validation split to monitor generalization\n",
    "5. ‚úÖ Save every 100 steps for monitoring\n",
    "6. ‚úÖ Google Drive auto-backup\n",
    "\n",
    "**Dataset:** Python code examples (system automation, scripting)\n",
    "\n",
    "**Expected Time:** 2-3 hours on A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4339aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive FIRST\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_OUTPUT = \"/content/drive/MyDrive/qwen3_phase1_python_adapters\"\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted\")\n",
    "print(f\"‚úÖ Output will be saved to: {DRIVE_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl torch\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b58cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - PYTHON DATASET (TEST RUN)\n",
    "MODEL_NAME = \"DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B\"\n",
    "OUTPUT_DIR = \"./qwen3-phase1-python-TEST\"\n",
    "DATASET_NAME = \"iamtarun/python_code_instructions_18k_alpaca\"  # Reliable Python dataset\n",
    "\n",
    "# Training hyperparameters - SHORT TEST RUN\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 5e-5  # Testing this learning rate\n",
    "WEIGHT_DECAY = 0.1\n",
    "NUM_EPOCHS = 1  # CHANGED: Only 1 epoch for quick test (was 3)\n",
    "MAX_STEPS = 300  # ADDED: Stop after 300 steps (~20 min)\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "MAX_DATASET_SIZE = 50000  # Limit dataset size for test run\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# CRITICAL: Use proper padding token\n",
    "PAD_TOKEN_ID = 151645\n",
    "print(f\"üìä Dataset: {DATASET_NAME} ({DATASET_SUBSET})\")\n",
    "print(\"‚úÖ Configuration set\")\n",
    "print(f\"‚ö†Ô∏è  Max dataset size: {MAX_DATASET_SIZE} examples\")\n",
    "print(f\"‚ö†Ô∏è  LEARNING RATE: {LEARNING_RATE}\")\n",
    "\n",
    "print(f\"‚ö†Ô∏è  WEIGHT DECAY: {WEIGHT_DECAY}\")print(f\"‚ö†Ô∏è  LEARNING RATE: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# CRITICAL: Force same padding token\n",
    "tokenizer.pad_token_id = PAD_TOKEN_ID\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "if tokenizer.pad_token_id != 151645:\n",
    "    raise ValueError(f\"‚ùå Wrong pad token! Got {tokenizer.pad_token_id}, expected 151645\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "print(\"\\nApplying LoRA configuration...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n‚úÖ LoRA applied\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Python dataset and split into train/validation\n",
    "print(\"Loading Python code dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Total dataset size: {len(dataset)} examples\")\n",
    "\n",
    "# Create train/validation split (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} examples\")\n",
    "print(f\"Validation set: {len(val_dataset)} examples\")\n",
    "print(f\"\\nSample entry:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13946be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset using PROPER Qwen3 chat template\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Format using Qwen3 chat template for Python code\"\"\"\n",
    "    instruction = example.get('instruction', '')\n",
    "    input_text = example.get('input', '')\n",
    "    output = example.get('output', '')\n",
    "    # Combine instruction and input if input exists (Alpaca format)\n",
    "    if input_text:\n",
    "        user_content = f\"{instruction}\\n\\nInput: {input_text}\"\n",
    "    else:\n",
    "        user_content = instruction\n",
    "    \n",
    "    # Create messages in chat format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": output}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template WITHOUT adding generation prompt (we have the full conversation)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "print(\"Formatting datasets with Qwen3 chat template...\")\n",
    "train_dataset = train_dataset.map(format_chat_template)\n",
    "val_dataset = val_dataset.map(format_chat_template)\n",
    "\n",
    "print(\"\\n‚úÖ Formatted with chat template\")\n",
    "print(\"\\nFormatted example:\")\n",
    "\n",
    "print(train_dataset[0]['text'][:500])print(train_dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - SHORT TEST RUN\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=MAX_STEPS,  # ADDED: Stop after 300 steps\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "\n",
    "    # CRITICAL FIXES:\n",
    "    learning_rate=LEARNING_RATE,  # 5e-5\n",
    "    weight_decay=WEIGHT_DECAY,    # 0.1 for regularization\n",
    "\n",
    "    # Evaluation settings\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # Training settings\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # CRITICAL: Load best model at end\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"‚ö†Ô∏è  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"‚ö†Ô∏è  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"‚ö†Ô∏è  Will evaluate and save every 100 steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with validation set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TRAINER WITH VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining on {len(train_dataset)} examples...\")\n",
    "print(f\"Validating on {len(val_dataset)} examples...\")\n",
    "print(f\"Total steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION) * NUM_EPOCHS}\")\n",
    "print(\"\\n‚ö†Ô∏è  MONITOR: Watch for loss = 0.0 before step 500 (indicates overfitting)\")\n",
    "print(\"‚ö†Ô∏è  HEALTHY: Loss should drop gradually and plateau around 0.3-0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eadc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show final metrics\n",
    "final_train_loss = trainer.state.log_history[-1].get('loss', 'N/A')\n",
    "final_eval_loss = trainer.state.log_history[-1].get('eval_loss', 'N/A')\n",
    "print(f\"\\nFinal train loss: {final_train_loss}\")\n",
    "print(f\"Final eval loss: {final_eval_loss}\")\n",
    "\n",
    "if isinstance(final_train_loss, float) and final_train_loss < 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Train loss is very low - model may have overfit!\")\n",
    "    print(\"‚ö†Ô∏è  Check validation loss to confirm generalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804650af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LoRA adapter to LOCAL storage first\n",
    "print(\"\\nSaving LoRA adapter to local storage...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "import os\n",
    "adapter_path = f\"{OUTPUT_DIR}/adapter_model.safetensors\"\n",
    "if os.path.exists(adapter_path):\n",
    "    adapter_size = os.path.getsize(adapter_path) / 1e6\n",
    "    print(f\"‚úÖ Adapter saved locally: {adapter_size:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Adapter file not found at {adapter_path}\")\n",
    "    raise FileNotFoundError(\"Adapter not saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Copy to Google Drive for permanent storage\n",
    "import shutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COPYING TO GOOGLE DRIVE (PERMANENT BACKUP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Copy entire adapter directory to Drive\n",
    "drive_backup = DRIVE_OUTPUT\n",
    "if os.path.exists(drive_backup):\n",
    "    shutil.rmtree(drive_backup)\n",
    "\n",
    "shutil.copytree(OUTPUT_DIR, drive_backup)\n",
    "\n",
    "# Verify the copy\n",
    "drive_adapter_path = f\"{drive_backup}/adapter_model.safetensors\"\n",
    "if os.path.exists(drive_adapter_path):\n",
    "    drive_size = os.path.getsize(drive_adapter_path) / 1e6\n",
    "    print(f\"\\n‚úÖ BACKUP COMPLETE!\")\n",
    "    print(f\"‚úÖ Adapter backed up to Google Drive: {drive_size:.1f} MB\")\n",
    "    print(f\"‚úÖ Location: {drive_backup}\")\n",
    "    print(f\"\\nüéâ You can now find the adapters in Google Drive under:\")\n",
    "    print(f\"   MyDrive/qwen3_phase1_python_adapters/\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Backup failed!\")\n",
    "    raise FileNotFoundError(\"Drive backup failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c195b",
   "metadata": {},
   "source": [
    "# ‚úÖ Training Complete!\n",
    "\n",
    "Your LoRA adapters are now safely stored in:\n",
    "- **Google Drive:** `MyDrive/qwen3_phase1_python_adapters/`\n",
    "- **Local sync:** Should appear in `~/GoogleDrive/qwen3_phase1_python_adapters/`\n",
    "\n",
    "## This Phase:\n",
    "\n",
    "1. ‚úÖ **Python code training** - System automation, scripting\n",
    "2. ‚úÖ **Proper chat template** - Qwen3 format\n",
    "3. ‚úÖ **Same settings** - LR 5e-5, weight decay 0.1\n",
    "4. ‚úÖ **Monitoring** - Watch for overfitting at step 100, 200, etc.\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "If this works without overfitting, the model should learn Python patterns without memorizing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
