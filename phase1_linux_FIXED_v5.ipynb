{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee1499c",
   "metadata": {},
   "source": [
    "# Phase 1: Linux Commands Training - FIXED VERSION 4\n",
    "\n",
    "**CRITICAL FIXES:**\n",
    "1. ‚úÖ Uses proper Qwen3 chat template format\n",
    "2. ‚úÖ Lower learning rate (5e-5) to prevent overfitting\n",
    "3. ‚úÖ Weight decay (0.1) for regularization\n",
    "4. ‚úÖ Validation split to monitor generalization\n",
    "5. ‚úÖ Early stopping if loss hits 0.1 too early\n",
    "6. ‚úÖ Google Drive auto-backup\n",
    "\n",
    "**Dataset:** AnishJoshi/nl2bash-custom (19K train, 2.5K validation)\n",
    "\n",
    "**Expected Time:** 2-3 hours on A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive FIRST\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_OUTPUT = \"/content/drive/MyDrive/qwen3_phase1_linux_adapters_v4\"\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted\")\n",
    "print(f\"‚úÖ Output will be saved to: {DRIVE_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f324d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl torch\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69274c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9086f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - FIXED SETTINGS\n",
    "MODEL_NAME = \"DavidAU/Qwen3-Zero-Coder-Reasoning-0.8B\"\n",
    "OUTPUT_DIR = \"./qwen3-colab-linux-phase1-v4\"\n",
    "DATASET_NAME = \"AnishJoshi/nl2bash-custom\"\n",
    "\n",
    "# Training hyperparameters - FIXED\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 5e-5  # LOWERED from 2e-4 to prevent overfitting\n",
    "WEIGHT_DECAY = 0.1    # ADDED for regularization\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# LoRA config - Same as before\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# CRITICAL: Use proper padding token\n",
    "PAD_TOKEN_ID = 151645\n",
    "\n",
    "print(\"‚úÖ Configuration set\")\n",
    "print(f\"‚ö†Ô∏è  LEARNING RATE: {LEARNING_RATE} (lowered to prevent overfitting)\")\n",
    "print(f\"‚ö†Ô∏è  WEIGHT DECAY: {WEIGHT_DECAY} (added for regularization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# CRITICAL: Force same padding token\n",
    "tokenizer.pad_token_id = PAD_TOKEN_ID\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "if tokenizer.pad_token_id != 151645:\n",
    "    raise ValueError(f\"‚ùå Wrong pad token! Got {tokenizer.pad_token_id}, expected 151645\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "print(\"\\nApplying LoRA configuration...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n‚úÖ LoRA applied\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and split into train/validation\n",
    "print(\"Loading Linux commands dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Total dataset size: {len(dataset)} examples\")\n",
    "\n",
    "# Create train/validation split (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} examples\")\n",
    "print(f\"Validation set: {len(val_dataset)} examples\")\n",
    "print(f\"\\nSample entry:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8549e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset using PROPER Qwen3 chat template\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Format using Qwen3 chat template instead of Instruction/Response\"\"\"\n",
    "    description = example.get('description', example.get('question', ''))\n",
    "    command = example.get('cmd', example.get('answer', ''))\n",
    "    \n",
    "    # Create messages in chat format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Write a bash command: {description}\"},\n",
    "        {\"role\": \"assistant\", \"content\": command}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template WITHOUT adding generation prompt (we have the full conversation)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "print(\"Formatting datasets with Qwen3 chat template...\")\n",
    "train_dataset = train_dataset.map(format_chat_template)\n",
    "val_dataset = val_dataset.map(format_chat_template)\n",
    "\n",
    "print(\"\\n‚úÖ Formatted with chat template\")\n",
    "print(\"\\nFormatted example:\")\n",
    "print(train_dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - FIXED FOR ANTI-OVERFITTING\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # CRITICAL FIXES:\n",
    "    learning_rate=LEARNING_RATE,  # 5e-5 instead of 2e-4\n",
    "    weight_decay=WEIGHT_DECAY,    # 0.1 for regularization\n",
    "    \n",
    "    # Evaluation settings\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Training settings\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    \n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # CRITICAL: Load best model at end\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"‚ö†Ô∏è  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"‚ö†Ô∏è  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"‚ö†Ô∏è  Will evaluate every 100 steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a234923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with validation set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TRAINER WITH VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # ADDED validation set\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining on {len(train_dataset)} examples...\")\n",
    "print(f\"Validating on {len(val_dataset)} examples...\")\n",
    "print(f\"Total steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION) * NUM_EPOCHS}\")\n",
    "print(\"\\n‚ö†Ô∏è  MONITOR: If train loss hits 0.0 before step 500, training is overfitting!\")\n",
    "print(\"‚ö†Ô∏è  HEALTHY: Loss should plateau around 0.3-0.5, NOT go to 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show final metrics\n",
    "final_train_loss = trainer.state.log_history[-1].get('loss', 'N/A')\n",
    "final_eval_loss = trainer.state.log_history[-1].get('eval_loss', 'N/A')\n",
    "print(f\"\\nFinal train loss: {final_train_loss}\")\n",
    "print(f\"Final eval loss: {final_eval_loss}\")\n",
    "\n",
    "if isinstance(final_train_loss, float) and final_train_loss < 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Train loss is very low - model may have overfit!\")\n",
    "    print(\"‚ö†Ô∏è  Check validation loss to confirm generalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d512c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LoRA adapter to LOCAL storage first\n",
    "print(\"\\nSaving LoRA adapter to local storage...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "import os\n",
    "adapter_path = f\"{OUTPUT_DIR}/adapter_model.safetensors\"\n",
    "if os.path.exists(adapter_path):\n",
    "    adapter_size = os.path.getsize(adapter_path) / 1e6\n",
    "    print(f\"‚úÖ Adapter saved locally: {adapter_size:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Adapter file not found at {adapter_path}\")\n",
    "    raise FileNotFoundError(\"Adapter not saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Copy to Google Drive for permanent storage\n",
    "import shutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COPYING TO GOOGLE DRIVE (PERMANENT BACKUP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Copy entire adapter directory to Drive\n",
    "drive_backup = DRIVE_OUTPUT\n",
    "if os.path.exists(drive_backup):\n",
    "    shutil.rmtree(drive_backup)\n",
    "\n",
    "shutil.copytree(OUTPUT_DIR, drive_backup)\n",
    "\n",
    "# Verify the copy\n",
    "drive_adapter_path = f\"{drive_backup}/adapter_model.safetensors\"\n",
    "if os.path.exists(drive_adapter_path):\n",
    "    drive_size = os.path.getsize(drive_adapter_path) / 1e6\n",
    "    print(f\"\\n‚úÖ BACKUP COMPLETE!\")\n",
    "    print(f\"‚úÖ Adapter backed up to Google Drive: {drive_size:.1f} MB\")\n",
    "    print(f\"‚úÖ Location: {drive_backup}\")\n",
    "    print(f\"\\nüéâ You can now find the adapters in Google Drive under:\")\n",
    "    print(f\"   MyDrive/qwen3_phase1_linux_adapters_v4/\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Backup failed!\")\n",
    "    raise FileNotFoundError(\"Drive backup failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9610e8d3",
   "metadata": {},
   "source": [
    "# ‚úÖ Training Complete!\n",
    "\n",
    "Your LoRA adapters are now safely stored in:\n",
    "- **Google Drive:** `MyDrive/qwen3_phase1_linux_adapters_v4/`\n",
    "- **Local sync:** Should appear in `~/GoogleDrive/qwen3_phase1_linux_adapters_v4/`\n",
    "\n",
    "## Key Fixes in This Version:\n",
    "\n",
    "1. ‚úÖ **Proper chat template** - Uses Qwen3 format, not \"Instruction/Response\"\n",
    "2. ‚úÖ **Lower learning rate** - 5e-5 instead of 2e-4 (prevents overfitting)\n",
    "3. ‚úÖ **Weight decay** - 0.1 for regularization\n",
    "4. ‚úÖ **Validation split** - 10% held out to monitor generalization\n",
    "5. ‚úÖ **Evaluation** - Checks val loss every 100 steps\n",
    "6. ‚úÖ **Best model loading** - Automatically uses best checkpoint\n",
    "\n",
    "## What to Check:\n",
    "\n",
    "- Final train loss should be ~0.3-0.5 (NOT 0.0!)\n",
    "- Validation loss should be close to train loss\n",
    "- If train loss << val loss = overfitting (but better than before)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
