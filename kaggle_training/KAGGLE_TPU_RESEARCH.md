# Kaggle TPU Training - Comprehensive Research

**Date:** October 6, 2025  
**Purpose:** Research TPU training for parallel LoRA approach  
**Requirement:** Get it RIGHT the first time - only 20 TPU hours available

---

## üéØ Executive Summary

### Key Findings:
1. ‚úÖ **TPU v3-8 available on Kaggle:** 8 cores, 128GB memory, 9h max session
2. ‚ö†Ô∏è **PyTorch TPU requires torch_xla** - different from GPU training
3. ‚ö†Ô∏è **HuggingFace Transformers + PEFT + TPU** - need to verify compatibility
4. ‚úÖ **20 TPU hours/week quota** - enough for 4x 3-4h trainings
5. ‚ö†Ô∏è **9-hour session limit** - must complete each phase in one run
6. ‚ùì **TPU compatibility with our stack unclear** - needs verification

---

## üìã Kaggle TPU Specifications

### Hardware (TPU v3-8):
- **Cores:** 8 TPU cores (4 dual-core chips)
- **Memory:** 128GB high-speed memory (vs 15.89GB T4 GPU)
- **Matrix Multipliers:** 128x128 hardware accelerators per core
- **Batch Size Rule:** 128 elements per core = 1,024 batch size for TPU v3-8
- **Minimum Efficient Batch:** 8 per core = 64 total

### Time Limits:
- **Max Session:** 9 hours (vs 12 hours GPU)
- **Weekly Quota:** 20 TPU hours total
- **Our Need:** 4 phases √ó 3-4h = 12-16h (FITS in quota)

### Pricing:
- **Free!** (like GPU)
- Weekly quota resets

---

## üîß TPU Configuration in Kernel

### kernel-metadata.json:
```json
{
  "id": "username/kernel-slug",
  "title": "Phase 1 Training - TPU",
  "code_file": "phase1_training_tpu.ipynb",
  "language": "python",
  "kernel_type": "notebook",
  "is_private": true,
  "enable_gpu": false,
  "enable_tpu": true,        # <- KEY FLAG (added in API v1.5.8)
  "enable_internet": true,
  "dataset_data_sources": [
    "username/qwen3-08b-coder-reasoning",
    "sahil2801/CodeAlpaca-20k"
  ]
}
```

**CRITICAL:** `enable_tpu=true` (not `enable_gpu`)

---

## üêç PyTorch TPU Setup (Kaggle-Specific)

### Step 1: Install torch_xla

```python
# REQUIRED: Install Torch-XLA for TPU support
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev
```

**Time Cost:** ~2-3 minutes per notebook

###Step 2: Import XLA Libraries

```python
import torch
import torch_xla
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.xla_multiprocessing as xmp
```

### Step 3: Define Training Function (mp_fn)

```python
def _mp_fn(index):
    # Get TPU device
    device = xm.xla_device()
    
    # Load model to TPU
    model = YourModel()
    model = model.to(device)
    
    # Training loop
    for epoch in range(EPOCHS):
        for batch in train_loader:
            # Move data to TPU
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            # Forward/backward
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # TPU-specific optimizer step
            xm.optimizer_step(optimizer)
            
            # TPU-specific print
            xm.master_print(f"Loss: {loss.item()}")
```

### Step 4: Spawn Multi-Core Training

```python
# Run on all 8 TPU cores
xmp.spawn(_mp_fn, nprocs=8, start_method='fork')
```

---

## ‚ö†Ô∏è CRITICAL TPU Differences from GPU

### 1. Distributed Training is MANDATORY
```python
# GPU (single device)
model = model.to('cuda')

# TPU (8 cores required)
xmp.spawn(_mp_fn, nprocs=8, start_method='fork')
```

### 2. Data Loading
```python
# Create distributed sampler
train_sampler = torch.utils.data.distributed.DistributedSampler(
    train_dataset,
    num_replicas=xm.xrt_world_size(),
    rank=xm.get_ordinal()
)

# Use ParallelLoader
para_loader = pl.ParallelLoader(train_data_loader, [device])
train_fn(para_loader.per_device_loader(device))
```

### 3. Model Wrapper
```python
# GPU
model = YourModel()

# TPU
model = xmp.MpModelWrapper(YourModel())
```

### 4. Printing/Logging
```python
# GPU
print(f"Loss: {loss.item()}")

# TPU
xm.master_print(f"Loss: {loss.item()}")
```

### 5. Model Saving
```python
# Memory-optimized (recommended)
import torch_xla.utils.serialization as xser
xser.save(model.state_dict(), "model.bin", master_only=True)
model.load_state_dict(xser.load("model.bin"))

# Standard (works but less efficient)
xm.save(model.state_dict(), "model.bin")
```

---

## ‚úÖ CRITICAL FINDINGS FROM REAL TPU NOTEBOOK

### ANALYZED: PyTorch Official Training Tutorial on TPU

**Key Discoveries:**

### 1. TPU Device Setup
```python
import torch_xla as xla
import torch_xla.core.xla_model as xm

# Move model to TPU
model.to(xla.device())

# Move data to TPU
inputs, labels = inputs.to(xla.device()), labels.to(xla.device())
```

‚úÖ **Finding:** Simple `.to(xla.device())` works - no complex distributed setup needed for single-core usage!

### 2. Standard PyTorch Training Loop Works!
```python
def train_one_epoch(epoch_index, tb_writer):
    model.to(xla.device())  # Move model once
    for i, data in enumerate(training_loader):
        inputs, labels = data
        inputs, labels = inputs.to(xla.device()), labels.to(xla.device())
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
```

‚úÖ **Finding:** Normal PyTorch training loop with just device placement changes!

### 3. NO torch_xla.distributed Required
**CRITICAL:** The example notebook does NOT use:
- ‚ùå `xmp.spawn()` - Not needed
- ‚ùå `MpModelWrapper` - Not needed
- ‚ùå `DistributedSampler` - Not needed
- ‚ùå `ParallelLoader` - Not needed

‚úÖ **Finding:** Can use TPU like a single GPU device for simple training!

### 4. Standard torch.utils.data.DataLoader Works
```python
training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)
```

‚úÖ **Finding:** Regular DataLoader works - no special TPU data pipeline needed!

### 5. Model Saving/Loading
```python
# Save
torch.save(model.state_dict(), model_path)

# Load
model.load_state_dict(torch.load(PATH))
```

‚úÖ **Finding:** Standard PyTorch model I/O works!

---

## üéØ IMPLICATIONS FOR OUR PROJECT

### What This Means:

**GOOD NEWS:**
1. ‚úÖ Can use TPU like a bigger, faster GPU
2. ‚úÖ Standard PyTorch patterns work
3. ‚úÖ No complex distributed training code needed
4. ‚úÖ DataLoader works normally

**STILL UNKNOWN:**
1. ‚ùì HuggingFace `AutoModelForCausalLM` compatibility
2. ‚ùì PEFT `get_peft_model()` compatibility
3. ‚ùì `SFTTrainer` support (likely NO - would need custom loop)
4. ‚ùå BitsAndBytes quantization (confirmed CUDA-only)

### Realistic Assessment:

**High Probability Success Path:**
```python
# Load HuggingFace model
model = AutoModelForCausalLM.from_pretrained(...)
model = model.to(xla.device())  # Should work

# Apply LoRA
model = get_peft_model(model, lora_config)  # Might work

# Custom training loop (SFTTrainer likely won't work)
for batch in dataloader:
    inputs = inputs.to(xla.device())
    outputs = model(inputs)
    loss.backward()
    optimizer.step()
```

**Major Concern:**
- ‚ùå No 4-bit quantization available
- ‚ö†Ô∏è 0.8B model in fp16 = ~1.6GB
- ‚ö†Ô∏è With batch_size=64 (TPU optimal), might use 20-40GB
- ‚úÖ TPU has 128GB, so likely OK

---

## üîç TPU Compatibility Research Needed

### Priority 1: HuggingFace Transformers on TPU
**Search for:**
- "HuggingFace transformers Kaggle TPU"
- "AutoModelForCausalLM TPU"
- "Qwen TPU training"

### Priority 2: PEFT LoRA on TPU
**Search for:**
- "PEFT LoRA TPU"
- "Parameter efficient fine-tuning TPU"
- "torch_xla PEFT compatibility"

### Priority 3: Check Existing Kaggle Notebooks
**Look for:**
- Language model fine-tuning on TPU
- LoRA training on TPU
- HuggingFace + TPU examples

---

## üìä Comparison: GPU vs TPU Training

| Aspect | GPU (T4) | TPU (v3-8) | Notes |
|--------|----------|------------|-------|
| **Memory** | 15.89 GB | 128 GB | 8x more memory |
| **Session Limit** | 12 hours | 9 hours | TPU 3h shorter |
| **Weekly Quota** | 30 hours | 20 hours | Both sufficient |
| **Batch Size** | 2 (memory limited) | 64-1024 (recommended) | Huge difference |
| **Setup Complexity** | Simple | Complex (torch_xla) | TPU needs special setup |
| **HF Compatibility** | ‚úÖ Native | ‚ùì Unknown | Need to verify |
| **PEFT Compatibility** | ‚úÖ Native | ‚ùì Unknown | Need to verify |
| **4-bit Quantization** | ‚úÖ BitsAndBytes | ‚ùå Likely not | Major concern |
| **Code Changes** | None | Significant | Distributed training required |

---

## üö® MAJOR CONCERNS

### Concern #1: Quantization Incompatibility
**Problem:** Our GPU approach uses 4-bit quantization (BitsAndBytes)
- BitsAndBytes is CUDA-only (doesn't support TPU)
- Without quantization, 0.8B model might not fit even in 128GB with large batch sizes
- **Impact:** Might need completely different approach

**Possible Solutions:**
1. Train without quantization (full precision or fp16)
2. Use smaller batch sizes
3. Use TPU-specific quantization (if exists)

### Concern #2: HuggingFace Stack Compatibility
**Problem:** Our entire stack is HuggingFace-based
- `transformers`: AutoModelForCausalLM
- `datasets`: load_dataset
- `peft`: get_peft_model, LoraConfig
- `trl`: SFTTrainer, SFTConfig

**Unknown:** Which parts work on TPU out-of-the-box?

**Risk:** Might need to rewrite large portions of code

### Concern #3: Time to Port Code
**Problem:** Only 20 TPU hours total
- Can't waste time debugging TPU setup
- Each failed run costs 1-2 hours (setup + early failure)
- Need to get it right in ‚â§3 attempts

---

## üéØ Research Action Plan

### Phase 1: Verify Compatibility (BEFORE writing code)

1. **Search Kaggle Notebooks:**
   - Find examples of HuggingFace + TPU
   - Find examples of LoRA on TPU
   - Check if anyone uses PEFT on TPU

2. **Check Documentation:**
   - PEFT library TPU support
   - HuggingFace Trainer TPU backend
   - torch_xla compatibility with transformers

3. **Test Simple Example:**
   - Create minimal TPU notebook
   - Load Qwen3 model
   - Try basic forward pass
   - **Time investment:** 1-2 hours, but saves 10+ hours later

### Phase 2: Design TPU Training Approach

Based on compatibility research:

**Option A: Full HuggingFace Stack (if compatible)**
```python
# Minimal changes from GPU code
model = AutoModelForCausalLM.from_pretrained(...)
model = model.to(xm.xla_device())
# ... rest similar to GPU
```

**Option B: Custom Training Loop (if Trainer doesn't support TPU)**
```python
# Rewrite training loop with torch_xla primitives
xmp.spawn(_mp_fn, nprocs=8, start_method='fork')
# Manual gradient accumulation
# Manual logging/checkpointing
```

**Option C: Hybrid Approach**
```python
# Use HuggingFace for model/dataset loading
# Custom training loop for TPU
```

### Phase 3: Create Test Notebook

**Goal:** Verify entire pipeline works before committing to 4-phase training

**Test Checklist:**
- ‚úÖ Model loads on TPU
- ‚úÖ LoRA config applies
- ‚úÖ Dataset loads and batches correctly
- ‚úÖ Training loop runs without errors
- ‚úÖ Model saves successfully
- ‚úÖ Memory usage acceptable
- ‚úÖ Speed is reasonable (check MXU utilization)

**Time investment:** 2-3 hours, but de-risks entire approach

---

## üí° Alternative: Stick with GPU

### GPU Parallel Strategy (No TPU):
Instead of TPU, could we run 4 GPU notebooks simultaneously?

**Pros:**
- No code changes
- Proven to work (Phase 1 succeeded)
- Familiar debugging

**Cons:**
- Only 30 GPU hours/week
- Can't run 4 notebooks at once (resource contention)
- Would take 12-16h sequentially

**Verdict:** Still explore TPU, but keep GPU as backup

---

## üìù Next Steps (BEFORE Creating TPU Notebooks)

1. ‚úÖ **Complete this research document** (DONE)

2. **Search Kaggle for TPU + HuggingFace examples**
   - Look for language model fine-tuning
   - Check if PEFT is used
   - See what works and what doesn't

3. **Check PEFT documentation for TPU**
   - Official docs
   - GitHub issues
   - Community discussions

4. **Create compatibility matrix**
   - Which components work on TPU
   - Which need replacement
   - Code complexity estimate

5. **Make GO/NO-GO decision**
   - If TPU compatible: Create notebooks
   - If not compatible: Stick with GPU parallel
   - If partial compatibility: Assess effort vs benefit

6. **IF GO: Create test notebook first**
   - Minimal viable training loop
   - Verify end-to-end
   - Measure performance
   - THEN create all 4 phase notebooks

---

## üéì Key Learnings from GPU Experience

### What Worked:
- ‚úÖ Phase 1 completed successfully (3-4h)
- ‚úÖ LoRA approach (42MB adapters)
- ‚úÖ PEFT library
- ‚úÖ HuggingFace stack
- ‚úÖ pad_token_id=151645 consistency

### What Failed:
- ‚ùå Sequential merging (6x slowdown)
- ‚ùå Phase 2/3 hung during model loading
- ‚ùå Quantization after merge caused device placement issues

### Lessons for TPU:
1. **Test minimal example first** (don't assume compatibility)
2. **Verify memory usage** (even with 128GB)
3. **Check logging/monitoring works** (can we see progress?)
4. **Measure actual speed** (TPU might not be faster for small models)
5. **Have backup plan** (GPU parallel if TPU doesn't work)

---

## ‚ö° Quick Decision Matrix

| Factor | TPU | GPU Parallel |
|--------|-----|--------------|
| **Code Changes** | High (torch_xla) | None |
| **Compatibility Risk** | High (unknown) | None (proven) |
| **Memory** | 128GB | 15.89GB |
| **Speed Potential** | Higher (if optimized) | Known (3-4h/phase) |
| **Time to Implement** | 4-6 hours research+test | 0 hours |
| **Hours Available** | 20 TPU hours | 27 GPU hours remaining |
| **Parallelization** | Can run 2-4 at once | Only 1 at a time |
| **Success Probability** | 50-70% (unknown) | 95% (Phase 1 worked) |

---

## üéØ UPDATED RECOMMENDATION (After Analyzing Real TPU Notebook)

### CONFIDENCE INCREASED: 70% ‚Üí 85%

**What We Learned:**
1. ‚úÖ TPU can be used like a single GPU device
2. ‚úÖ No complex distributed training required
3. ‚úÖ Standard PyTorch patterns work
4. ‚úÖ DataLoader works normally
5. ‚ö†Ô∏è Need custom training loop (SFTTrainer likely won't work)
6. ‚ùå No 4-bit quantization (but 128GB should handle fp16)

### REVISED STRATEGY:

**Phase 1: Create Minimal Test Notebook (1-2h)**
```python
# Test this exact sequence:
1. Load Qwen3 model with AutoModelForCausalLM
2. Move to xla.device()
3. Apply LoRA with get_peft_model()
4. Run 10 training steps
5. Verify loss decreases
6. Save LoRA adapter
```

**Phase 2: If Test Succeeds, Create 4 Production Notebooks**
- Each phase: Custom training loop based on PyTorch official example
- Replace SFTTrainer with manual loop
- Use larger batch sizes (64-128 vs GPU's 2)
- Expected time: 2-3h per phase (faster than GPU)

**Phase 3: Parallel Execution**
- Run 2-4 phases simultaneously
- Complete all 4 in 3-4 hours total (vs 12-16h sequential)

### Code Template for Our Use Case:

```python
import torch_xla as xla
import torch_xla.core.xla_model as xm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# Load model (no quantization)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    torch_dtype=torch.float16  # fp16 instead of 4-bit
)

# Move to TPU
device = xla.device()
model = model.to(device)

# Apply LoRA
lora_config = LoraConfig(r=16, lora_alpha=32, ...)
model = get_peft_model(model, lora_config)

# Training loop
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

for epoch in range(NUM_EPOCHS):
    for batch in dataloader:
        inputs = batch['input_ids'].to(device)
        labels = batch['labels'].to(device)
        
        outputs = model(inputs, labels=labels)
        loss = outputs.loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if step % 50 == 0:
            xm.master_print(f"Loss: {loss.item()}")

# Save LoRA adapter
model.save_pretrained(OUTPUT_DIR)
```

### Risk Assessment:
- **Test notebook failure risk:** 30% (HuggingFace + PEFT might not work)
- **If test succeeds, production failure risk:** 10% (just scaling up)
- **Time wasted if fails:** 2 hours (test only)
- **Time saved if succeeds:** 8-12 hours (parallel training)

### DECISION:
**PROCEED WITH TEST NOTEBOOK** - Risk/reward ratio is excellent

---

## üìö References to Check

1. **Kaggle Official:**
   - https://www.kaggle.com/docs/tpu
   - PyTorch TPU section

2. **Example Notebooks:**
   - "The Ultimate PyTorch TPU Tutorial"
   - "Super Duper Fast PyTorch TPU Kernel"
   - Search: "transformers TPU Kaggle"

3. **PEFT Library:**
   - GitHub issues mentioning TPU
   - Documentation for device compatibility

4. **torch_xla:**
   - PyTorch XLA documentation
   - Compatibility with common libraries

---

## ‚úÖ Updated Status

- [x] Document created
- [x] Kaggle TPU examples researched ‚úÖ **ANALYZED PYTORCH OFFICIAL TUTORIAL**
- [x] Basic TPU usage verified ‚úÖ **SIMPLE DEVICE PLACEMENT WORKS**
- [ ] PEFT TPU compatibility verified (test notebook needed)
- [ ] HuggingFace AutoModel TPU compatibility verified (test notebook needed)
- [x] Training loop pattern identified ‚úÖ **CUSTOM LOOP REQUIRED**
- [ ] GO/NO-GO decision made (pending test notebook)
- [ ] Test notebook created
- [ ] Performance validated
- [ ] Production notebooks created

---

## üéØ NEXT ACTIONS

### Immediate (Now):
1. **Create minimal TPU test notebook** (1-2h)
   - Test: Load Qwen3 ‚Üí Apply LoRA ‚Üí Train 10 steps
   - Verify: Model loads, LoRA applies, loss decreases
   - Measure: Memory usage, speed

### If Test Succeeds:
2. **Create Phase 1 production notebook** (2h)
   - Full CodeAlpaca training
   - Custom training loop
   - Batch size optimization

3. **Clone for Phases 2-4** (1h)
   - Modify datasets
   - Keep training logic identical

4. **Run all 4 phases in parallel** (3-4h total)

### If Test Fails:
2. **Fall back to GPU parallel strategy**
   - Run phases sequentially
   - Each phase independent training
   - Use multi-LoRA merge at end

---

**READY TO PROCEED:** Create test notebook now? (YES/NO decision point)
