{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a8a8ea",
   "metadata": {},
   "source": [
    "# Phase 3: Python System Automation\n",
    "\n",
    "Training Phase 2 output on Python system automation datasets\n",
    "\n",
    "**Input:**\n",
    "- Phase 2 Merged Model: `/kaggle/input/qwen3-phase2-linux-merged` (Base + CodeAlpaca + Linux Commands)\n",
    "\n",
    "**Workflow:**\n",
    "1. Load Phase 2 merged model\n",
    "2. Apply LoRA and train on Python automation datasets\n",
    "3. Save Phase 3 LoRA adapter\n",
    "4. Merge LoRA with base model for Phase 4 input\n",
    "\n",
    "**Datasets:**\n",
    "- HuggingFace: `RazinAleks/SO-Python_QA-System_Administration_and_DevOps_class` ‚≠ê Perfect fit!\n",
    "- HuggingFace: `infinite-dataset-hub/ShellScriptDataset`\n",
    "- HuggingFace: `flytech/python-codes-25k` (FILTERED for system-relevant code only)\n",
    "\n",
    "**Filtering Strategy:**\n",
    "- ‚úÖ Keep: File I/O, process management, system calls, automation, subprocess, os module\n",
    "- ‚ùå Remove: Web scraping, data science, algorithms, LeetCode, matplotlib/pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431df605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90702ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen3-08b-coder-reasoning\"  # Original base model for final merge\n",
    "PHASE2_MERGED_PATH = \"/kaggle/input/qwen3-phase2-linux-merged\"  # Phase 2 merged model\n",
    "OUTPUT_DIR = \"/kaggle/working/qwen3-08b-phase3-python\"\n",
    "\n",
    "# Training hyperparameters (OPTIMIZED FOR MEMORY)\n",
    "BATCH_SIZE = 2  # Reduced from 4 to save memory\n",
    "GRADIENT_ACCUMULATION = 8  # Increased to keep effective batch size = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd92ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phase 2 merged model\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: LOADING PHASE 2 MERGED MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nLoading Phase 2 merged model (Base + CodeAlpaca + Linux Commands)...\")\n",
    "merged_phase2_model = AutoModelForCausalLM.from_pretrained(\n",
    "    PHASE2_MERGED_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Phase 2 merged model loaded\")\n",
    "print(\"‚úì Model includes: Base + CodeAlpaca + Linux Commands\")\n",
    "\n",
    "# This merged model will be used for Phase 3 training\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8f7f1",
   "metadata": {},
   "source": [
    "## Step 1: Load Phase 2 Merged Model\n",
    "\n",
    "We start with the merged model from Phase 2, which already contains knowledge from:\n",
    "- Base Qwen3-0.8B model\n",
    "- Phase 1: CodeAlpaca training\n",
    "- Phase 2: Linux Commands training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer with padding token verification\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: LOADING TOKENIZER\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nLoading tokenizer from base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "# CRITICAL: Force padding token to match Phase 1 & 2 exactly\n",
    "# ALL PHASES use pad_token_id = 151645 (eos_token)\n",
    "print(\"‚ö†Ô∏è  Forcing pad_token to eos_token (ID: 151645) to match Phase 1 & 2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 151645\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\nüîç VERIFICATION:\")\n",
    "print(f\"   pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"   eos_token_id: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Phase 1 & 2 used pad_token_id = 151645\n",
    "if tokenizer.pad_token_id == 151645:\n",
    "    print(f\"   ‚úÖ CORRECT: Matches Phase 1 & 2 configuration (ID: 151645)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Different from Phase 1 & 2 (expected 151645, got {tokenizer.pad_token_id})\")\n",
    "    print(f\"   This may cause training inconsistencies!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare merged model for training with 4-bit quantization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: PREPARING MERGED MODEL FOR TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nApplying 4-bit quantization to merged model...\")\n",
    "# Note: We already have the merged model from previous cell\n",
    "# Now we need to prepare it for k-bit training\n",
    "model = prepare_model_for_kbit_training(merged_phase2_model, use_gradient_checkpointing=True)\n",
    "print(f\"‚úì Model prepared for training on device: {model.device}\")\n",
    "print(f\"‚úì Gradient checkpointing enabled (saves memory)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clean up to save memory\n",
    "del merged_phase2_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úì Original models cleaned from memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for Phase 3 training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: CONFIGURING PHASE 3 LORA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSetting up LoRA for Python automation training...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2f348",
   "metadata": {},
   "source": [
    "## Step 2: Load and Filter Datasets\n",
    "\n",
    "We'll load three datasets focused on Python system automation:\n",
    "1. **StackOverflow Python QA** - System Administration & DevOps\n",
    "2. **Shell Script Dataset** - Shell scripting patterns\n",
    "3. **Python Codes** - Filtered for system-relevant code only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9493b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace datasets (requires internet enabled in notebook settings)\n",
    "print(\"Loading HuggingFace datasets...\")\n",
    "\n",
    "# Dataset 1: StackOverflow Python QA - System Administration\n",
    "try:\n",
    "    print(\"Loading RazinAleks/SO-Python_QA-System_Administration_and_DevOps_class...\")\n",
    "    hf_so = load_dataset(\"RazinAleks/SO-Python_QA-System_Administration_and_DevOps_class\", split=\"train\")\n",
    "    # Sample if too large to speed up processing\n",
    "    if len(hf_so) > 10000:\n",
    "        hf_so = hf_so.shuffle(seed=42).select(range(10000))\n",
    "        print(f\"‚úì StackOverflow Python QA: {len(hf_so)} examples (sampled for efficiency)\")\n",
    "    else:\n",
    "        print(f\"‚úì StackOverflow Python QA: {len(hf_so)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"! Could not load StackOverflow dataset: {e}\")\n",
    "    hf_so = None\n",
    "\n",
    "# Dataset 2: Shell Script Dataset\n",
    "try:\n",
    "    print(\"Loading infinite-dataset-hub/ShellScriptDataset...\")\n",
    "    hf_shell = load_dataset(\"infinite-dataset-hub/ShellScriptDataset\", split=\"train\")\n",
    "    # Sample if too large\n",
    "    if len(hf_shell) > 5000:\n",
    "        hf_shell = hf_shell.shuffle(seed=42).select(range(5000))\n",
    "        print(f\"‚úì Shell Script Dataset: {len(hf_shell)} examples (sampled for efficiency)\")\n",
    "    else:\n",
    "        print(f\"‚úì Shell Script Dataset: {len(hf_shell)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"! Could not load Shell Script dataset: {e}\")\n",
    "    hf_shell = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cec934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 3: Python Codes (will filter for system-relevant)\n",
    "try:\n",
    "    print(\"Loading flytech/python-codes-25k...\")\n",
    "    hf_python_raw = load_dataset(\"flytech/python-codes-25k\", split=\"train\")\n",
    "    print(f\"‚úì Python Codes: {len(hf_python_raw)} examples (will filter)\")\n",
    "    \n",
    "    # Filter for system-relevant keywords with negative filtering\n",
    "    print(\"Filtering for system-relevant Python code...\")\n",
    "    \n",
    "    # POSITIVE keywords - MUST have at least one\n",
    "    system_keywords = [\n",
    "        'os.', 'sys.', 'subprocess', 'shutil', 'pathlib', 'glob',\n",
    "        'file', 'directory', 'process', 'path', 'chmod', 'chown',\n",
    "        'environ', 'execute', 'script', 'automation', 'system',\n",
    "        'open(', 'read', 'write', 'json', 'yaml', 'config',\n",
    "        'argparse', 'logging', 'socket', 'threading', 'multiprocessing'\n",
    "    ]\n",
    "    \n",
    "    # NEGATIVE keywords - exclude if ANY are present\n",
    "    exclude_keywords = [\n",
    "        'hexagonal', 'tile', 'palindrome', 'fibonacci', 'leetcode',\n",
    "        'algorithm', 'dataframe', 'matplotlib', 'pandas', 'seaborn',\n",
    "        'pyplot', 'plot', 'graph', 'visualization', 'sklearn',\n",
    "        'tensorflow', 'keras', 'pytorch', 'neural', 'machine learning',\n",
    "        'deep learning', 'scraping', 'beautifulsoup', 'selenium',\n",
    "        'theorem', 'proof', 'equation', 'matrix multiplication',\n",
    "        'binary tree', 'linked list', 'stack', 'queue', 'heap'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    def is_system_relevant(example):    hf_python = None\n",
    "\n",
    "        \"\"\"Check if code contains system keywords and excludes non-system topics\"\"\"    print(f\"! Could not load/filter Python Codes dataset: {e}\")\n",
    "\n",
    "        code = str(example.get('code', '') or except Exception as e:\n",
    "\n",
    "                  example.get('text', '') or     print(f\"‚úì Filtered to {len(hf_python)} system-relevant examples (from {len(hf_python_raw)})\")\n",
    "\n",
    "                  example.get('instruction', '') or    hf_python = hf_python_raw.filter(is_system_relevant)\n",
    "\n",
    "                  example.get('output', '') or '').lower()    \n",
    "\n",
    "                return has_system and not has_exclude\n",
    "\n",
    "        # Must have at least one positive keyword        \n",
    "\n",
    "        has_system = any(keyword.lower() in code for keyword in system_keywords)        has_exclude = any(keyword.lower() in code for keyword in exclude_keywords)\n",
    "\n",
    "                # Must NOT have any negative keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all datasets to common format\n",
    "print(\"\\nNormalizing datasets to common format...\")\n",
    "\n",
    "def normalize_so_python(dataset):\n",
    "    \"\"\"Normalize StackOverflow Python QA dataset\"\"\"\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    \n",
    "    def format_so(example):\n",
    "        question = example.get('question', example.get('instruction', example.get('input', '')))\n",
    "        answer = example.get('answer', example.get('output', example.get('response', '')))\n",
    "        return {\"text\": f\"Instruction: {question}\\n\\nResponse: {answer}\"}\n",
    "    \n",
    "    return dataset.map(format_so, remove_columns=dataset.column_names)\n",
    "\n",
    "def normalize_shell(dataset):\n",
    "    \"\"\"Normalize Shell Script dataset\"\"\"\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    \n",
    "    def format_shell(example):\n",
    "        instruction = example.get('instruction', example.get('question', example.get('input', '')))\n",
    "        script = example.get('script', example.get('output', example.get('code', '')))\n",
    "        return {\"text\": f\"Instruction: {instruction}\\n\\nResponse: {script}\"}\n",
    "    \n",
    "    return dataset.map(format_shell, remove_columns=dataset.column_names)\n",
    "\n",
    "def normalize_python(dataset):\n",
    "    \"\"\"Normalize Python Codes dataset\"\"\"\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    \n",
    "    def format_python(example):\n",
    "        instruction = example.get('instruction', example.get('question', example.get('input', '')))\n",
    "        code = example.get('code', example.get('output', example.get('text', '')))\n",
    "        return {\"text\": f\"Instruction: {instruction}\\n\\nResponse: {code}\"}\n",
    "    \n",
    "    return dataset.map(format_python, remove_columns=dataset.column_names)\n",
    "\n",
    "# Normalize all datasets\n",
    "datasets_normalized = []\n",
    "\n",
    "if hf_so is not None:\n",
    "    ds1 = normalize_so_python(hf_so)\n",
    "    if ds1:\n",
    "        datasets_normalized.append(ds1)\n",
    "        print(f\"‚úì Normalized StackOverflow: {len(ds1)}\")\n",
    "\n",
    "if hf_shell is not None:\n",
    "    ds2 = normalize_shell(hf_shell)\n",
    "    if ds2:\n",
    "        datasets_normalized.append(ds2)\n",
    "        print(f\"‚úì Normalized Shell Scripts: {len(ds2)}\")\n",
    "\n",
    "if hf_python is not None:\n",
    "    ds3 = normalize_python(hf_python)\n",
    "    if ds3:\n",
    "        datasets_normalized.append(ds3)\n",
    "        print(f\"‚úì Normalized Python Codes: {len(ds3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all normalized datasets\n",
    "if not datasets_normalized:\n",
    "    raise ValueError(\"No datasets loaded successfully! Check data sources and internet connection.\")\n",
    "\n",
    "print(f\"\\nCombining {len(datasets_normalized)} datasets...\")\n",
    "dataset = concatenate_datasets(datasets_normalized)\n",
    "print(f\"‚úì Combined dataset size: {len(dataset)}\")\n",
    "\n",
    "# CRITICAL: Cap dataset size to avoid OOM errors\n",
    "# Phase 1 used 20K, Phase 2 capped at 25K from 76K\n",
    "# Phase 3: Target 20K to avoid capping and ensure high quality\n",
    "MAX_DATASET_SIZE = 20000\n",
    "if len(dataset) > MAX_DATASET_SIZE:\n",
    "    print(f\"‚ö†Ô∏è  Dataset too large ({len(dataset)} examples)\")\n",
    "    print(f\"‚ö†Ô∏è  Sampling {MAX_DATASET_SIZE} examples to fit in GPU memory...\")\n",
    "    dataset = dataset.shuffle(seed=42).select(range(MAX_DATASET_SIZE))\n",
    "    print(f\"‚úì Reduced to {len(dataset)} examples\")\n",
    "else:\n",
    "    # Shuffle for better training\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    print(f\"‚úì Dataset shuffled\")\n",
    "\n",
    "print(f\"\\n‚úì Final training size: {len(dataset)} examples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample training example:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (using SFTConfig for SFT-specific parameters)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=0.3,\n",
    "    # SFT-specific parameters\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting training...\")\n",
    "print(f\"Starting training on {len(dataset)} examples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a87f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving model...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR + \"/final\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"/final\")\n",
    "print(f\"‚úì Model saved to {OUTPUT_DIR}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapter into base model for next phase\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGING LORA ADAPTER INTO BASE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reload the trained model in full precision for merging\n",
    "print(\"\\nLoading trained model for merging...\")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model (full precision)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter we just trained\n",
    "lora_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR + \"/final\")\n",
    "\n",
    "# Merge LoRA weights into base model\n",
    "print(\"Merging LoRA adapter into base model...\")\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "MERGED_OUTPUT = OUTPUT_DIR + \"/merged\"\n",
    "print(f\"Saving merged model to {MERGED_OUTPUT}...\")\n",
    "merged_model.save_pretrained(MERGED_OUTPUT)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT)\n",
    "\n",
    "print(f\"\\n‚úì Merged model saved to {MERGED_OUTPUT}\")\n",
    "print(\"‚úì This merged model should be used as input for Phase 4\")\n",
    "print(f\"‚úì Pad token ID: {tokenizer.pad_token_id} (preserved for next phase)\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del base_model, lora_model, merged_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úì Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae4f50",
   "metadata": {},
   "source": [
    "## Merge LoRA Adapter into Base Model\n",
    "\n",
    "For sequential training (Phase 3 ‚Üí Phase 4), we merge the LoRA adapter into the base model so Phase 4 can build on all accumulated knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f407f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(\"\\nTesting the fine-tuned model...\")\n",
    "test_prompts = [\n",
    "    \"Instruction: Write a Python script to backup all files in a directory\\n\\nResponse:\",\n",
    "    \"Instruction: Create a Python script to monitor CPU usage\\n\\nResponse:\",\n",
    "    \"Instruction: Write Python code to automate file organization by extension\\n\\nResponse:\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"OUTPUT: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive BOTH outputs for download\n",
    "print(\"\\nArchiving outputs...\")\n",
    "!zip -r qwen3-08b-phase3-python-lora.zip {OUTPUT_DIR}/final\n",
    "!zip -r qwen3-08b-phase3-python-merged.zip {OUTPUT_DIR}/merged\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úì LoRA adapter archived: qwen3-08b-phase3-python-lora.zip\")\n",
    "print(\"‚úì Merged model archived: qwen3-08b-phase3-python-merged.zip\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Upload the MERGED model as input for Phase 4\")\n",
    "print(f\"‚ö†Ô∏è  Pad token ID {tokenizer.pad_token_id} is preserved in merged model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
