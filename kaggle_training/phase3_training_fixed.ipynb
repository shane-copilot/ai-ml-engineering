{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac47d7e",
   "metadata": {},
   "source": [
    "# Phase 3: Python System Automation\n",
    "\n",
    "Training Phase 2 output on Python automation and system scripting datasets\n",
    "\n",
    "**Input:**\n",
    "- Base model: `/kaggle/input/qwen3-08b-coder-reasoning` (1.6GB)\n",
    "- Phase 1 LoRA: `/kaggle/input/qwen3-phase1-lora-adapter` (42MB - CodeAlpaca)\n",
    "- Phase 2 LoRA: `/kaggle/input/qwen3-phase2-linux-lora-adapter` (27MB - Linux commands, 1 epoch)\n",
    "\n",
    "**PERFORMANCE FIX:**\n",
    "- **Problem:** Phase 2 was 6x slower due to merge ‚Üí save ‚Üí reload ‚Üí quantize cycle\n",
    "- **Solution:** Keep merged model in memory, apply quantization once, no disk I/O\n",
    "- **Expected:** ~2 hours (down from 10+ hours)\n",
    "\n",
    "**Workflow:**\n",
    "1. Load base model with 4-bit quantization (ONCE)\n",
    "2. Load Phase 1 LoRA adapter\n",
    "3. Merge Phase 1 (in memory, no save)\n",
    "4. Load Phase 2 LoRA adapter  \n",
    "5. Merge Phase 2 (in memory, no save)\n",
    "6. Apply Phase 3 LoRA training directly\n",
    "\n",
    "**Critical:** All phases use **pad_token_id=151645** (EOS token) for consistency\n",
    "\n",
    "**Expected Time:** 2 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d05d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace62d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen3-08b-coder-reasoning\"\n",
    "PHASE1_LORA_PATH = \"/kaggle/input/qwen3-phase1-lora-adapter\"\n",
    "PHASE2_LORA_PATH = \"/kaggle/input/qwen3-phase2-linux-lora-adapter\"\n",
    "OUTPUT_DIR = \"/kaggle/working/qwen3-08b-phase3-python\"\n",
    "\n",
    "# Training hyperparameters (optimized for memory + time)\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION = 8  # Effective batch = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 1  # Reduced to 1 epoch (3h time constraint)\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# CRITICAL: Padding token (consistent across all phases)\n",
    "PAD_TOKEN_ID = 151645  # EOS token used in Phase 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb930cc",
   "metadata": {},
   "source": [
    "## Step 1: Load Base Model with 4-bit Quantization (PERFORMANCE FIX)\n",
    "\n",
    "**Key Change:** Load base model with quantization ONCE at the start. This eliminates the costly reload cycle from Phase 2.\n",
    "\n",
    "**Previous (slow):** Base ‚Üí merge P1 ‚Üí save ‚Üí reload+quantize ‚Üí train P2\n",
    "**Now (fast):** Base+quantize ‚Üí merge P1 ‚Üí merge P2 ‚Üí train P3 (all in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0451cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 1: LOAD BASE MODEL WITH 4-BIT QUANTIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configure 4-bit quantization ONCE\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(\"\\nüîÑ Loading base model with 4-bit quantization...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base model loaded and quantized\")\n",
    "print(f\"   Model device: {base_model.device}\")\n",
    "print(f\"   Model dtype: {base_model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96058a7",
   "metadata": {},
   "source": [
    "## Step 2: Load Tokenizer with Consistent Padding\n",
    "\n",
    "**CRITICAL:** Must use pad_token_id=151645 (EOS token) to match Phase 1 & 2 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21257d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: LOAD TOKENIZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "# CRITICAL: Force padding token to match Phase 1 & 2\n",
    "# Phase 1 & 2 used pad_token_id = 151645 (eos_token)\n",
    "print(f\"\\n‚ö†Ô∏è  CRITICAL: Setting pad_token_id={PAD_TOKEN_ID} (EOS token)\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = PAD_TOKEN_ID\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\nüîç VERIFICATION:\")\n",
    "print(f\"   pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"   eos_token_id: {tokenizer.eos_token_id}\")\n",
    "\n",
    "if tokenizer.pad_token_id == PAD_TOKEN_ID:\n",
    "    print(f\"   ‚úÖ CORRECT: Matches Phase 1 & 2 (ID: {PAD_TOKEN_ID})\")\n",
    "else:\n",
    "    raise ValueError(f\"‚ùå PADDING TOKEN MISMATCH! Expected {PAD_TOKEN_ID}, got {tokenizer.pad_token_id}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c89d0",
   "metadata": {},
   "source": [
    "## Step 3: Sequential Merging (In-Memory, No Disk I/O)\n",
    "\n",
    "**Performance optimization:** Merge Phase 1 and Phase 2 LoRAs in memory without saving to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: MERGE PHASE 1 & 2 LORAS (IN MEMORY)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load and merge Phase 1 LoRA\n",
    "print(\"\\nüîÑ Loading Phase 1 LoRA adapter...\")\n",
    "model_with_phase1 = PeftModel.from_pretrained(base_model, PHASE1_LORA_PATH)\n",
    "\n",
    "print(\"üîÑ Merging Phase 1 LoRA (CodeAlpaca knowledge)...\")\n",
    "model = model_with_phase1.merge_and_unload()\n",
    "del model_with_phase1\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Phase 1 merged (in memory)\")\n",
    "\n",
    "# Load and merge Phase 2 LoRA\n",
    "print(\"\\nüîÑ Loading Phase 2 LoRA adapter...\")\n",
    "model_with_phase2 = PeftModel.from_pretrained(model, PHASE2_LORA_PATH)\n",
    "\n",
    "print(\"üîÑ Merging Phase 2 LoRA (Linux commands, 1 epoch)...\")\n",
    "model = model_with_phase2.merge_and_unload()\n",
    "del model_with_phase2\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Phase 2 merged (in memory)\")\n",
    "\n",
    "print(\"\\n‚úÖ Model now includes:\")\n",
    "print(\"   ‚Ä¢ Base Qwen2.5-0.5B knowledge\")\n",
    "print(\"   ‚Ä¢ Phase 1: CodeAlpaca training\")\n",
    "print(\"   ‚Ä¢ Phase 2: Linux command training (1 epoch)\")\n",
    "print(\"\\nüöÄ Ready for Phase 3 training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada46ee",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Model for LoRA Training\n",
    "\n",
    "Enable gradient checkpointing and prepare for k-bit training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: PREPARE MODEL FOR LORA TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"‚úÖ Gradient checkpointing enabled\")\n",
    "print(\"‚úÖ Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf2f19",
   "metadata": {},
   "source": [
    "## Step 5: Configure Phase 3 LoRA\n",
    "\n",
    "New LoRA adapter for Python automation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f471280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: CONFIGURE PHASE 3 LORA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n‚úÖ Phase 3 LoRA adapter configured\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d246f",
   "metadata": {},
   "source": [
    "## Step 6: Load Python Automation Datasets\n",
    "\n",
    "Focus on Python system automation, scripting, and DevOps tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: LOAD PYTHON AUTOMATION DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Python automation datasets\n",
    "print(\"\\nüîÑ Loading Python automation datasets...\")\n",
    "\n",
    "# 1. CodeAlpaca (filter for Python)\n",
    "print(\"\\nLoading CodeAlpaca (Python only)...\")\n",
    "codealpha_ds = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\n",
    "python_alpaca = codealpha_ds.filter(\n",
    "    lambda x: 'python' in x['instruction'].lower() or 'script' in x['instruction'].lower()\n",
    ")\n",
    "print(f\"‚úì CodeAlpaca Python: {len(python_alpaca)} examples\")\n",
    "\n",
    "# 2. CodeFeedback (filter for Python)\n",
    "print(\"\\nLoading CodeFeedback (filtering for Python)...\")\n",
    "codefeedback_ds = load_dataset(\"m-a-p/CodeFeedback-Filtered-Instruction\", split=\"train\")\n",
    "python_cf = codefeedback_ds.filter(\n",
    "    lambda x: x.get('lang') == 'python' or 'python' in x.get('query', '').lower()\n",
    ")\n",
    "print(f\"‚úì CodeFeedback Python: {len(python_cf)} examples\")\n",
    "\n",
    "# Combine and cap at 25K for memory\n",
    "print(\"\\nüîÑ Combining datasets...\")\n",
    "def normalize_python(example):\n",
    "    return {\n",
    "        'instruction': example.get('instruction') or example.get('query', ''),\n",
    "        'output': example.get('output') or example.get('answer', '')\n",
    "    }\n",
    "\n",
    "python_alpaca_norm = python_alpaca.map(normalize_python)\n",
    "python_cf_norm = python_cf.map(normalize_python)\n",
    "\n",
    "combined = concatenate_datasets([python_alpaca_norm, python_cf_norm])\n",
    "combined = combined.shuffle(seed=42).select(range(min(25000, len(combined))))\n",
    "\n",
    "print(f\"\\n‚úÖ Total training examples: {len(combined):,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366dfbf",
   "metadata": {},
   "source": [
    "## Step 7: Format Dataset\n",
    "\n",
    "Convert to instruction-response format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c837deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: FORMAT DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def format_instruction(example):\n",
    "    return {\n",
    "        \"text\": f\"Instruction: {example['instruction']}\\n\\nResponse: {example['output']}\"\n",
    "    }\n",
    "\n",
    "train_dataset = combined.map(format_instruction)\n",
    "print(f\"‚úÖ Dataset formatted: {len(train_dataset):,} examples\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1128fe",
   "metadata": {},
   "source": [
    "## Step 8: Configure Training\n",
    "\n",
    "Set up training parameters with memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: CONFIGURE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=0.3,\n",
    "    # SFT-specific parameters\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Trainer configured\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"   Total steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION) * NUM_EPOCHS}\")\n",
    "print(f\"   Expected time: ~2 hours (with performance fix)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b9044",
   "metadata": {},
   "source": [
    "## Step 9: Train Phase 3\n",
    "\n",
    "Start training with performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: TRAINING PHASE 3\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(f\"   Dataset: {len(train_dataset):,} examples\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION})\")\n",
    "print(\"\\n‚è±Ô∏è  PERFORMANCE MONITORING:\")\n",
    "print(f\"   Phase 2 baseline: 23.48 sec/step (6x slow)\")\n",
    "print(f\"   Phase 3 target: <5 sec/step (with fix)\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "\n",
    "training_hours = (end_time - start_time) / 3600\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Total time: {training_hours:.2f} hours\")\n",
    "print(f\"   Avg sec/step: {(end_time - start_time) / trainer.state.global_step:.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8281f",
   "metadata": {},
   "source": [
    "## Step 10: Save Phase 3 LoRA Adapter\n",
    "\n",
    "Save only the LoRA adapter weights (~27-40MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 10: SAVE PHASE 3 LORA ADAPTER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüíæ Saving Phase 3 LoRA adapter...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "import os\n",
    "adapter_size = os.path.getsize(f\"{OUTPUT_DIR}/adapter_model.safetensors\") / (1024 * 1024)\n",
    "print(f\"\\n‚úÖ Phase 3 adapter saved!\")\n",
    "print(f\"   Location: {OUTPUT_DIR}\")\n",
    "print(f\"   Size: {adapter_size:.1f} MB\")\n",
    "print(f\"   Ready for Phase 4 training\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aaeb0",
   "metadata": {},
   "source": [
    "## Training Complete! üéâ\n",
    "\n",
    "**Phase 3 Results:**\n",
    "- Model has learned: Base + CodeAlpaca + Linux Commands + Python Automation\n",
    "- Adapter saved for Phase 4 (Advanced Troubleshooting)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download this Phase 3 adapter (~27-40 MB)\n",
    "2. Upload to Kaggle as dataset\n",
    "3. Create Phase 4 notebook\n",
    "4. Final training phase!\n",
    "\n",
    "**Performance Check:**\n",
    "- If this phase completed in ~2 hours: ‚úÖ Fix worked!\n",
    "- If this phase took 10+ hours: ‚ö†Ô∏è Need further debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
