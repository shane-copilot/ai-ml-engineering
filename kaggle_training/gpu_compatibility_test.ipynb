{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a8a8ea",
   "metadata": {},
   "source": [
    "# GPU Compatibility Test - Qwen3 + LoRA Stack\n",
    "\n",
    "**Purpose:** Quick test to verify our training stack works on Kaggle GPU (45 hours available!)\n",
    "\n",
    "**Expected Time:** 5-10 minutes (10 training steps only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431df605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90702ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - SIMPLIFIED FOR TEST\n",
    "MODEL_PATH = \"/kaggle/input/qwen3-08b-coder-reasoning\"\n",
    "OUTPUT_DIR = \"/kaggle/working/gpu_test_lora\"\n",
    "DATASET_NAME = \"sahil2801/CodeAlpaca-20k\"\n",
    "\n",
    "# Test parameters (small for speed)\n",
    "NUM_TEST_EXAMPLES = 100\n",
    "NUM_TEST_STEPS = 10\n",
    "BATCH_SIZE = 2\n",
    "MAX_SEQ_LENGTH = 512\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# CRITICAL: Same padding token as Phase 1\n",
    "PAD_TOKEN_ID = 151645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer - force offline mode\n",
    "import os\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# CRITICAL: Force same padding token as Phase 1\n",
    "tokenizer.pad_token_id = PAD_TOKEN_ID\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization (like Phase 1)\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Loading Model on GPU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"\\nConfiguring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "print(f\"âœ… Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for Phase 2 training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: CONFIGURING PHASE 2 LORA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSetting up LoRA for Linux command training...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (small sample)\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "print(f\"\\nLoading {NUM_TEST_EXAMPLES} examples from CodeAlpaca...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=f\"train[:{NUM_TEST_EXAMPLES}]\")\n",
    "print(f\"âœ… Dataset loaded: {len(dataset)} examples\")\n",
    "\n",
    "# Format function (identical to Phase 1)\n",
    "def format_instruction(example):\n",
    "    instruction = example.get('instruction', '')\n",
    "    input_text = example.get('input', '')\n",
    "    output = example.get('output', '')\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"Instruction: {instruction}\\n\\nInput: {input_text}\\n\\nResponse:\"\n",
    "    else:\n",
    "        prompt = f\"Instruction: {instruction}\\n\\nResponse:\"\n",
    "    \n",
    "    text = f\"{prompt} {output}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "print(\"Formatting dataset...\")\n",
    "dataset = dataset.map(format_instruction)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "dataset = dataset.map(tokenize, remove_columns=['text', 'instruction', 'input', 'output'])\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "print(\"âœ… Dataset ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop test (10 steps only)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST: Training Loop (10 steps)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "step_times = []\n",
    "\n",
    "print(f\"\\nRunning {NUM_TEST_STEPS} training steps...\")\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    if step >= NUM_TEST_STEPS:\n",
    "        break\n",
    "    \n",
    "    step_start = time.time()\n",
    "    \n",
    "    # Move batch to device\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    attention_mask = batch['attention_mask'].to(model.device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=input_ids\n",
    "    )\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    step_time = time.time() - step_start\n",
    "    step_times.append(step_time)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Step {step+1}/{NUM_TEST_STEPS}: Loss = {loss.item():.4f} ({step_time:.2f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training completed!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analysis\n",
    "print(f\"\\nðŸ“Š RESULTS:\")\n",
    "print(f\"   Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"   Final loss:   {losses[-1]:.4f}\")\n",
    "print(f\"   Change: {losses[-1] - losses[0]:.4f}\")\n",
    "print(f\"   Avg time/step: {sum(step_times)/len(step_times):.2f}s\")\n",
    "\n",
    "if losses[-1] < losses[0]:\n",
    "    improvement = (1 - losses[-1]/losses[0])*100\n",
    "    print(f\"\\n   âœ… LEARNING WORKS: Loss decreased by {improvement:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n   âš ï¸  WARNING: Loss did not decrease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ GPU COMPATIBILITY TEST COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâœ… ALL TESTS PASSED:\")\n",
    "print(\"   1. âœ… Model loads on GPU with 4-bit quantization\")\n",
    "print(\"   2. âœ… LoRA applies correctly\")\n",
    "print(\"   3. âœ… Dataset loads and processes\")\n",
    "print(\"   4. âœ… Training loop runs without errors\")\n",
    "print(f\"   5. âœ… Loss decreased: {losses[0]:.4f} â†’ {losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š PERFORMANCE:\")\n",
    "avg_time = sum(step_times)/len(step_times)\n",
    "print(f\"   Avg time/step: {avg_time:.2f}s\")\n",
    "print(f\"   Est. for full training: {(avg_time * 1562 / 3600):.1f}h\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ READY FOR PRODUCTION:\")\n",
    "print(\"   âœ… GPU approach confirmed working\")\n",
    "print(\"   âœ… 45 GPU hours available on Kaggle\")\n",
    "print(\"   âœ… Can run Phase 2/3/4 training now\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
