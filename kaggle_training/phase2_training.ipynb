{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a8a8ea",
   "metadata": {},
   "source": [
    "# Phase 2: Linux Command Mastery\n",
    "\n",
    "Training Phase 1 output on Linux command and bash datasets\n",
    "\n",
    "**Input:**\n",
    "- Base model: `/kaggle/input/qwen3-08b-coder-reasoning` (1.6GB - already uploaded)\n",
    "- Phase 1 LoRA: `/kaggle/input/qwen3-phase1-lora-adapter` (42MB - just uploaded!)\n",
    "\n",
    "**Workflow:**\n",
    "1. Load base model + Phase 1 LoRA adapter\n",
    "2. Merge them on Kaggle (saves bandwidth!)\n",
    "3. Train Phase 2 LoRA on Linux datasets\n",
    "4. Save Phase 2 LoRA adapter for Phase 3\n",
    "\n",
    "**Datasets:**\n",
    "- Kaggle: complex-linux-commands-from-natural-language (1M examples)\n",
    "- Kaggle: linux-terminal-commands-dataset\n",
    "- HuggingFace: aelhalili/bash-commands-dataset\n",
    "- HuggingFace: m-a-p/CodeFeedback-Filtered-Instruction (bash/shell filtered)\n",
    "\n",
    "**Expected Time:** 3-4 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431df605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90702ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen3-08b-coder-reasoning\"  # Original base model\n",
    "PHASE1_LORA_PATH = \"/kaggle/input/qwen3-phase1-lora-adapter\"  # Phase 1 LoRA adapter\n",
    "MERGED_TEMP_PATH = \"/kaggle/working/phase1_merged_temp\"  # Temporary path for merged model\n",
    "OUTPUT_DIR = \"/kaggle/working/qwen3-08b-phase2-linux\"\n",
    "\n",
    "# Training hyperparameters (OPTIMIZED FOR MEMORY)\n",
    "BATCH_SIZE = 2  # Reduced from 4 to save memory\n",
    "GRADIENT_ACCUMULATION = 8  # Increased to keep effective batch size = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd92ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model in full precision for merging\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: MERGING PHASE 1 LORA WITH BASE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"\\nLoading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Loading Phase 1 LoRA adapter...\")\n",
    "phase1_model = PeftModel.from_pretrained(base_model, PHASE1_LORA_PATH)\n",
    "\n",
    "print(\"Merging Phase 1 LoRA into base model...\")\n",
    "merged_phase1_model = phase1_model.merge_and_unload()\n",
    "\n",
    "print(\"‚úì Phase 1 knowledge merged into model\")\n",
    "print(\"‚úì Model now includes: Base + CodeAlpaca training\")\n",
    "\n",
    "# Save merged model to disk (needed for quantized reload)\n",
    "print(f\"\\nSaving merged model to {MERGED_TEMP_PATH}...\")\n",
    "merged_phase1_model.save_pretrained(MERGED_TEMP_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Merged model saved\")\n",
    "print(\"‚úì Memory cleared\")\n",
    "\n",
    "# Clean up to free memory\n",
    "print(\"\\nCleaning up original models from memory...\")\n",
    "del base_model, phase1_model, merged_phase1_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8f7f1",
   "metadata": {},
   "source": [
    "## Step 1: Merge Phase 1 LoRA with Base Model\n",
    "\n",
    "Before training Phase 2, we merge the Phase 1 LoRA adapter into the base model, then save it. This merged model contains all the CodeAlpaca knowledge from Phase 1.\n",
    "\n",
    "**Critical Fix:** The merged model is saved to disk and then reloaded with 4-bit quantization. This prevents device placement issues and enables proper LoRA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer with padding token verification\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: LOADING TOKENIZER\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nLoading tokenizer from base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "# CRITICAL: Force padding token to match Phase 1 exactly\n",
    "# Phase 1 used pad_token_id = 151645 (eos_token)\n",
    "print(\"‚ö†Ô∏è  Forcing pad_token to eos_token (ID: 151645) to match Phase 1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 151645\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\nüîç VERIFICATION:\")\n",
    "print(f\"   pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"   eos_token_id: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Phase 1 used pad_token_id = 151645\n",
    "if tokenizer.pad_token_id == 151645:\n",
    "    print(f\"   ‚úÖ CORRECT: Matches Phase 1 configuration (ID: 151645)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Different from Phase 1 (expected 151645, got {tokenizer.pad_token_id})\")\n",
    "    print(f\"   This may cause training inconsistencies!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload merged model with 4-bit quantization for training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: RELOADING MERGED MODEL WITH 4-BIT QUANTIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"\\nConfiguring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(f\"Loading merged model from {MERGED_TEMP_PATH} with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MERGED_TEMP_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Preparing quantized model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úì Model ready for Phase 2 LoRA training\")\n",
    "print(f\"‚úì Gradient checkpointing enabled (saves memory)\")\n",
    "print(f\"‚úì Model loaded and quantized on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for Phase 2 training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: CONFIGURING PHASE 2 LORA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSetting up LoRA for Linux command training...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2f348",
   "metadata": {},
   "source": [
    "## Step 5: Load and Prepare Datasets\n",
    "\n",
    "We'll load multiple datasets and combine them:\n",
    "1. **Kaggle datasets** - Add as data sources in notebook settings\n",
    "2. **HuggingFace datasets** - Load directly (requires internet enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9493b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Kaggle datasets (add these as data sources in notebook settings)\n",
    "print(\"Loading Kaggle datasets...\")\n",
    "\n",
    "# Dataset 1: Complex Linux commands from natural language\n",
    "try:\n",
    "    df_complex = pd.read_json('/kaggle/input/complex-linux-commands-from-natual-language/complex_linux_commands_million.json')\n",
    "    print(f\"‚úì Complex Linux commands: {len(df_complex)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"! Could not load complex commands: {e}\")\n",
    "    df_complex = None\n",
    "\n",
    "# Dataset 2: Linux terminal commands\n",
    "try:\n",
    "    df_terminal = pd.read_json('/kaggle/input/linux-terminal-commands-dataset/LINUX_TERMINAL_COMMANDS.jsonl', lines=True)\n",
    "    print(f\"‚úì Terminal commands: {len(df_terminal)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"! Could not load terminal commands: {e}\")\n",
    "    df_terminal = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cec934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace datasets (requires internet enabled in notebook settings)\n",
    "print(\"\\nLoading HuggingFace datasets...\")\n",
    "\n",
    "# Dataset 3: Bash commands dataset\n",
    "try:\n",
    "    hf_bash = load_dataset(\"aelhalili/bash-commands-dataset\", split=\"train\")\n",
    "    print(f\"‚úì Bash commands: {len(hf_bash)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"! Could not load bash commands: {e}\")\n",
    "    hf_bash = None\n",
    "\n",
    "# Dataset 4: CodeFeedback filtered for bash/shell\n",
    "try:\n",
    "    print(\"Loading CodeFeedback (filtering for bash/shell)...\")\n",
    "    codefeedback = load_dataset(\"m-a-p/CodeFeedback-Filtered-Instruction\", split=\"train\")\n",
    "    \n",
    "    # Filter for bash/shell examples\n",
    "    def is_bash_shell(example):\n",
    "        query = example.get('query', '').lower()\n",
    "        answer = example.get('answer', '').lower()\n",
    "        return any(keyword in query or keyword in answer \n",
    "                  for keyword in ['bash', 'shell', 'linux', 'command', 'terminal', 'script'])\n",
    "    \n",
    "    hf_codefeedback = codefeedback.filter(is_bash_shell)\n",
    "    print(f\"‚úì CodeFeedback bash/shell: {len(hf_codefeedback)} examples (filtered from {len(codefeedback)})\")\n",
    "except Exception as e:\n",
    "    print(f\"! Could not load CodeFeedback: {e}\")\n",
    "    hf_codefeedback = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and combine all datasets\n",
    "print(\"\\nNormalizing datasets to common format...\")\n",
    "\n",
    "def normalize_kaggle_complex(df):\n",
    "    \"\"\"Normalize complex Linux commands dataset\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Sample if too large (we want ~30-40K total)\n",
    "    if len(df) > 40000:\n",
    "        df = df.sample(n=40000, random_state=42)\n",
    "    \n",
    "    # Assuming format: {\"instruction\": \"...\", \"command\": \"...\"}\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Adjust these field names based on actual dataset structure\n",
    "        instruction = row.get('instruction', row.get('query', row.get('nl', '')))\n",
    "        command = row.get('command', row.get('cmd', row.get('bash', '')))\n",
    "        \n",
    "        text = f\"Instruction: {instruction}\\n\\nResponse: {command}\"\n",
    "        data.append({\"text\": text})\n",
    "    \n",
    "    return Dataset.from_dict({\"text\": [d[\"text\"] for d in data]})\n",
    "\n",
    "def normalize_kaggle_terminal(df):\n",
    "    \"\"\"Normalize terminal commands dataset\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Adjust these field names based on actual dataset structure\n",
    "        instruction = row.get('instruction', row.get('query', row.get('description', '')))\n",
    "        command = row.get('command', row.get('cmd', ''))\n",
    "        \n",
    "        text = f\"Instruction: {instruction}\\n\\nResponse: {command}\"\n",
    "        data.append({\"text\": text})\n",
    "    \n",
    "    return Dataset.from_dict({\"text\": [d[\"text\"] for d in data]})\n",
    "\n",
    "def normalize_hf_bash(dataset):\n",
    "    \"\"\"Normalize HuggingFace bash commands\"\"\"\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    \n",
    "    def format_bash(example):\n",
    "        instruction = example.get('instruction', example.get('input', example.get('question', '')))\n",
    "        command = example.get('output', example.get('command', example.get('answer', '')))\n",
    "        return {\"text\": f\"Instruction: {instruction}\\n\\nResponse: {command}\"}\n",
    "    \n",
    "    return dataset.map(format_bash, remove_columns=dataset.column_names)\n",
    "\n",
    "def normalize_hf_codefeedback(dataset):\n",
    "    \"\"\"Normalize CodeFeedback bash/shell examples\"\"\"\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    \n",
    "    def format_feedback(example):\n",
    "        query = example.get('query', '')\n",
    "        answer = example.get('answer', '')\n",
    "        return {\"text\": f\"Instruction: {query}\\n\\nResponse: {answer}\"}\n",
    "    \n",
    "    return dataset.map(format_feedback, remove_columns=dataset.column_names)\n",
    "\n",
    "# Normalize all datasets\n",
    "datasets_normalized = []\n",
    "\n",
    "if df_complex is not None:\n",
    "    ds1 = normalize_kaggle_complex(df_complex)\n",
    "    if ds1:\n",
    "        datasets_normalized.append(ds1)\n",
    "        print(f\"‚úì Normalized complex commands: {len(ds1)}\")\n",
    "\n",
    "if df_terminal is not None:\n",
    "    ds2 = normalize_kaggle_terminal(df_terminal)\n",
    "    if ds2:\n",
    "        datasets_normalized.append(ds2)\n",
    "        print(f\"‚úì Normalized terminal commands: {len(ds2)}\")\n",
    "\n",
    "if hf_bash is not None:\n",
    "    ds3 = normalize_hf_bash(hf_bash)\n",
    "    if ds3:\n",
    "        datasets_normalized.append(ds3)\n",
    "        print(f\"‚úì Normalized bash commands: {len(ds3)}\")\n",
    "\n",
    "if hf_codefeedback is not None:\n",
    "    ds4 = normalize_hf_codefeedback(hf_codefeedback)\n",
    "    if ds4:\n",
    "        datasets_normalized.append(ds4)\n",
    "        print(f\"‚úì Normalized CodeFeedback: {len(ds4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all normalized datasets\n",
    "if not datasets_normalized:\n",
    "    raise ValueError(\"No datasets loaded successfully! Check data sources and internet connection.\")\n",
    "\n",
    "print(f\"\\nCombining {len(datasets_normalized)} datasets...\")\n",
    "dataset = concatenate_datasets(datasets_normalized)\n",
    "print(f\"‚úì Combined dataset size: {len(dataset)}\")\n",
    "\n",
    "# CRITICAL: Cap dataset size to avoid OOM errors\n",
    "# Phase 1 used 20K, we'll use 25K for Phase 2 (to fit in T4 memory)\n",
    "MAX_DATASET_SIZE = 25000\n",
    "if len(dataset) > MAX_DATASET_SIZE:\n",
    "    print(f\"‚ö†Ô∏è  Dataset too large ({len(dataset)} examples)\")\n",
    "    print(f\"‚ö†Ô∏è  Sampling {MAX_DATASET_SIZE} examples to fit in GPU memory...\")\n",
    "    dataset = dataset.shuffle(seed=42).select(range(MAX_DATASET_SIZE))\n",
    "    print(f\"‚úì Reduced to {len(dataset)} examples\")\n",
    "else:\n",
    "    # Shuffle for better training\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    print(f\"‚úì Dataset shuffled\")\n",
    "\n",
    "print(f\"\\n‚úì Final training size: {len(dataset)} examples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample training example:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (using SFTConfig for SFT-specific parameters)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=0.3,\n",
    "    # SFT-specific parameters\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting training...\")\n",
    "print(f\"Starting training on {len(dataset)} examples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a87f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving model...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR + \"/final\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"/final\")\n",
    "print(f\"‚úì Model saved to {OUTPUT_DIR}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapter into base model for next phase\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGING LORA ADAPTER INTO BASE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reload the trained model in full precision for merging\n",
    "print(\"\\nLoading trained model for merging...\")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model (full precision)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter we just trained\n",
    "lora_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR + \"/final\")\n",
    "\n",
    "# Merge LoRA weights into base model\n",
    "print(\"Merging LoRA adapter into base model...\")\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "MERGED_OUTPUT = OUTPUT_DIR + \"/merged\"\n",
    "print(f\"Saving merged model to {MERGED_OUTPUT}...\")\n",
    "merged_model.save_pretrained(MERGED_OUTPUT)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT)\n",
    "\n",
    "print(f\"\\n‚úì Merged model saved to {MERGED_OUTPUT}\")\n",
    "print(\"‚úì This merged model should be used as input for Phase 3\")\n",
    "print(f\"‚úì Pad token ID: {tokenizer.pad_token_id} (preserved for next phase)\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del base_model, lora_model, merged_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úì Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae4f50",
   "metadata": {},
   "source": [
    "## Merge LoRA Adapter into Base Model\n",
    "\n",
    "For sequential training (Phase 2 ‚Üí Phase 3 ‚Üí Phase 4), we need to merge the LoRA adapter into the base model so the next phase can build on this knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f407f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(\"\\nTesting the fine-tuned model...\")\n",
    "test_prompts = [\n",
    "    \"Instruction: How do I list all files including hidden ones?\\n\\nResponse:\",\n",
    "    \"Instruction: Install nginx web server\\n\\nResponse:\",\n",
    "    \"Instruction: Find all Python files in current directory\\n\\nResponse:\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"OUTPUT: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive BOTH outputs for download\n",
    "print(\"\\nArchiving outputs...\")\n",
    "!zip -r qwen3-08b-phase2-linux-lora.zip {OUTPUT_DIR}/final\n",
    "!zip -r qwen3-08b-phase2-linux-merged.zip {OUTPUT_DIR}/merged\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úì LoRA adapter archived: qwen3-08b-phase2-linux-lora.zip\")\n",
    "print(\"‚úì Merged model archived: qwen3-08b-phase2-linux-merged.zip\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Upload the MERGED model as input for Phase 3\")\n",
    "print(f\"‚ö†Ô∏è  Pad token ID {tokenizer.pad_token_id} is preserved in merged model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
