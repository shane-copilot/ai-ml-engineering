[
  {
    "id": "scenario_python_web_scraping_160",
    "command": "python",
    "scenario": "Web scraping with Python - extract data from websites, parse HTML, handle pagination.",
    "problem": "Need to extract data from websites without APIs, scrape product prices, collect news articles, monitor website changes, parse HTML tables, handle dynamic content, respect rate limits, and automate data collection from multiple pages.",
    "solution": "Use requests and BeautifulSoup for HTML parsing, handle pagination, extract specific elements, parse tables, save results to CSV/JSON, implement polite scraping with delays, and handle errors gracefully.",
    "steps": [
      "USER SAYS: 'I need to scrape product prices from an e-commerce site and save them to a CSV. How do I do this safely and efficiently?'",
      "AGENT: 'Let me show you web scraping with Python! We'll use BeautifulSoup to parse HTML and extract data responsibly.'",
      "AGENT: ''",
      "AGENT: '🔍 RESEARCH PHASE: Web scraping...'",
      "AGENT: '[Searching] \"python web scraping beautifulsoup requests html parsing\"'",
      "AGENT: '[Found] requests for HTTP, BeautifulSoup for HTML parsing.'",
      "AGENT: '[Found] CSS selectors and find methods to locate elements.'",
      "AGENT: '[Found] Always respect robots.txt and rate limits.'",
      "AGENT: '[Found] Add delays between requests to be polite.'",
      "AGENT: '✓ Research complete! Let\\'s scrape responsibly.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Basic HTML Scraping ---'",
      "AGENT: ''",
      "Run command: `pip3 install requests beautifulsoup4 lxml`",
      "AGENT: ''",
      "Create tool: `cat > scrape_basic.py << 'EOF'`",
      "#!/usr/bin/env python3\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_page(url):\n    \"\"\"Scrape a simple webpage.\"\"\"\n    print(f\"Fetching: {url}\\n\")\n    \n    # Add headers to mimic a browser\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    response = requests.get(url, headers=headers)\n    \n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Extract title\n        title = soup.find('title')\n        print(f\"Page title: {title.text if title else 'No title'}\\n\")\n        \n        # Extract all links\n        links = soup.find_all('a', href=True)\n        print(f\"Found {len(links)} links:\\n\")\n        \n        for i, link in enumerate(links[:10], 1):\n            print(f\"{i}. {link.text.strip()[:50]} -> {link['href']}\")\n        \n        # Extract all paragraphs\n        paragraphs = soup.find_all('p')\n        print(f\"\\nFound {len(paragraphs)} paragraphs\")\n        \n        return soup\n    else:\n        print(f\"Error: {response.status_code}\")\n        return None\n\nif __name__ == \"__main__\":\n    import sys\n    \n    if len(sys.argv) < 2:\n        url = \"https://example.com\"\n    else:\n        url = sys.argv[1]\n    \n    scrape_page(url)\nEOF",
      "AGENT: ''",
      "Run command: `python3 scrape_basic.py https://example.com`",
      "AGENT: 'Fetching: https://example.com'",
      "AGENT: ''",
      "AGENT: 'Page title: Example Domain'",
      "AGENT: ''",
      "AGENT: 'Found 1 links:'",
      "AGENT: ''",
      "AGENT: '1. More information... -> https://www.iana.org/domains/example'",
      "AGENT: ''",
      "AGENT: 'Found 2 paragraphs'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Extract Specific Data ---'",
      "AGENT: ''",
      "Create tool: `cat > scrape_products.py << 'EOF'`",
      "#!/usr/bin/env python3\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\n\ndef scrape_products(url):\n    \"\"\"Scrape product information from a page.\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    print(f\"Scraping products from: {url}\\n\")\n    \n    response = requests.get(url, headers=headers)\n    \n    if response.status_code != 200:\n        print(f\"Error: {response.status_code}\")\n        return []\n    \n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    products = []\n    \n    # Example: Find all product cards (adjust selectors for target site)\n    # This is a generic example\n    product_elements = soup.find_all('div', class_='product')\n    \n    if not product_elements:\n        print(\"No products found with class 'product'. Trying alternative selectors...\")\n        # Try alternative selectors\n        product_elements = soup.find_all('article') or soup.find_all('li', class_='item')\n    \n    print(f\"Found {len(product_elements)} potential products\\n\")\n    \n    for element in product_elements:\n        # Extract product name\n        name_elem = element.find(['h2', 'h3', 'h4', 'a'])\n        name = name_elem.text.strip() if name_elem else 'Unknown'\n        \n        # Extract price (common patterns)\n        price_elem = element.find(class_=['price', 'cost', 'amount'])\n        if not price_elem:\n            price_elem = element.find(string=lambda text: '$' in str(text))\n        price = price_elem.strip() if price_elem else 'N/A'\n        \n        # Extract link\n        link_elem = element.find('a', href=True)\n        link = link_elem['href'] if link_elem else ''\n        \n        products.append({\n            'name': name,\n            'price': price,\n            'link': link\n        })\n        \n        print(f\"Product: {name}\")\n        print(f\"  Price: {price}\")\n        print(f\"  Link: {link[:60]}...\" if len(link) > 60 else f\"  Link: {link}\")\n        print()\n    \n    return products\n\nif __name__ == \"__main__\":\n    import sys\n    \n    url = sys.argv[1] if len(sys.argv) > 1 else \"https://example.com\"\n    products = scrape_products(url)\n    print(f\"\\nTotal products scraped: {len(products)}\")\nEOF",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Scrape Multiple Pages (Pagination) ---'",
      "AGENT: ''",
      "Create tool: `cat > scrape_paginated.py << 'EOF'`",
      "#!/usr/bin/env python3\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport csv\n\ndef scrape_with_pagination(base_url, max_pages=5):\n    \"\"\"Scrape multiple pages with pagination.\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    all_items = []\n    \n    for page in range(1, max_pages + 1):\n        # Common pagination patterns\n        # Pattern 1: ?page=N\n        url = f\"{base_url}?page={page}\"\n        # Pattern 2: /page/N\n        # url = f\"{base_url}/page/{page}\"\n        \n        print(f\"Scraping page {page}/{max_pages}: {url}\")\n        \n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            \n            if response.status_code != 200:\n                print(f\"  Error: {response.status_code}\")\n                break\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Extract items (adjust selector for target site)\n            items = soup.find_all('div', class_='item')\n            \n            if not items:\n                print(\"  No items found. End of pagination.\")\n                break\n            \n            for item in items:\n                title_elem = item.find(['h2', 'h3', 'a'])\n                title = title_elem.text.strip() if title_elem else 'N/A'\n                \n                all_items.append({\n                    'page': page,\n                    'title': title\n                })\n            \n            print(f\"  Found {len(items)} items\")\n            \n            # Be polite: wait between requests\n            time.sleep(2)\n            \n        except requests.RequestException as e:\n            print(f\"  Request failed: {e}\")\n            break\n    \n    print(f\"\\nTotal items scraped: {len(all_items)}\")\n    return all_items\n\nif __name__ == \"__main__\":\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python3 scrape_paginated.py <base_url> [max_pages]\")\n        sys.exit(1)\n    \n    base_url = sys.argv[1]\n    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 5\n    \n    items = scrape_with_pagination(base_url, max_pages)\nEOF",
      "AGENT: ''",
      "AGENT: '--- Approach 4: Parse HTML Tables ---'",
      "AGENT: ''",
      "Create tool: `cat > scrape_table.py << 'EOF'`",
      "#!/usr/bin/env python3\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef scrape_table(url, table_index=0):\n    \"\"\"Scrape HTML table and convert to DataFrame.\"\"\"\n    print(f\"Scraping table from: {url}\\n\")\n    \n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find all tables\n    tables = soup.find_all('table')\n    print(f\"Found {len(tables)} tables\\n\")\n    \n    if not tables:\n        print(\"No tables found!\")\n        return None\n    \n    if table_index >= len(tables):\n        print(f\"Table index {table_index} out of range\")\n        return None\n    \n    # Get specific table\n    table = tables[table_index]\n    \n    # Extract headers\n    headers = []\n    header_row = table.find('thead')\n    if header_row:\n        headers = [th.text.strip() for th in header_row.find_all(['th', 'td'])]\n    else:\n        # Try first row\n        first_row = table.find('tr')\n        if first_row:\n            headers = [th.text.strip() for th in first_row.find_all(['th', 'td'])]\n    \n    print(f\"Headers: {headers}\\n\")\n    \n    # Extract rows\n    rows = []\n    tbody = table.find('tbody') or table\n    \n    for tr in tbody.find_all('tr')[1 if not table.find('thead') else 0:]:\n        cells = tr.find_all(['td', 'th'])\n        row = [cell.text.strip() for cell in cells]\n        if row:\n            rows.append(row)\n    \n    print(f\"Extracted {len(rows)} rows\\n\")\n    \n    # Create DataFrame\n    if headers:\n        df = pd.DataFrame(rows, columns=headers[:len(rows[0])] if rows else [])\n    else:\n        df = pd.DataFrame(rows)\n    \n    print(df.head())\n    \n    return df\n\nif __name__ == \"__main__\":\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python3 scrape_table.py <url> [table_index]\")\n        sys.exit(1)\n    \n    url = sys.argv[1]\n    table_index = int(sys.argv[2]) if len(sys.argv) > 2 else 0\n    \n    df = scrape_table(url, table_index)\n    \n    if df is not None:\n        # Save to CSV\n        output = 'scraped_table.csv'\n        df.to_csv(output, index=False)\n        print(f\"\\n✓ Saved to {output}\")\nEOF",
      "AGENT: ''",
      "AGENT: '--- Approach 5: Save Results to CSV ---'",
      "AGENT: ''",
      "Create tool: `cat > scrape_to_csv.py << 'EOF'`",
      "#!/usr/bin/env python3\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\nfrom datetime import datetime\n\ndef scrape_and_save(url, output_csv='scraped_data.csv'):\n    \"\"\"Scrape data and save to CSV.\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    print(f\"Scraping: {url}\\n\")\n    \n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Extract data (customize for your target site)\n    items = []\n    \n    # Example: scrape all article titles and links\n    articles = soup.find_all('article') or soup.find_all('div', class_='post')\n    \n    for article in articles:\n        title_elem = article.find(['h1', 'h2', 'h3', 'a'])\n        title = title_elem.text.strip() if title_elem else 'N/A'\n        \n        link_elem = article.find('a', href=True)\n        link = link_elem['href'] if link_elem else ''\n        \n        # Make absolute URLs\n        if link and not link.startswith('http'):\n            from urllib.parse import urljoin\n            link = urljoin(url, link)\n        \n        date_elem = article.find('time') or article.find(class_=['date', 'published'])\n        date = date_elem.text.strip() if date_elem else datetime.now().strftime('%Y-%m-%d')\n        \n        items.append({\n            'title': title,\n            'link': link,\n            'date': date,\n            'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        })\n    \n    # Write to CSV\n    if items:\n        with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n            fieldnames = ['title', 'link', 'date', 'scraped_at']\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            \n            writer.writeheader()\n            for item in items:\n                writer.writerow(item)\n        \n        print(f\"✓ Scraped {len(items)} items\")\n        print(f\"✓ Saved to {output_csv}\\n\")\n        \n        # Show preview\n        print(\"Preview:\")\n        for item in items[:5]:\n            print(f\"  • {item['title'][:60]}\")\n    else:\n        print(\"No items found\")\n    \n    return items\n\nif __name__ == \"__main__\":\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python3 scrape_to_csv.py <url> [output.csv]\")\n        sys.exit(1)\n    \n    url = sys.argv[1]\n    output = sys.argv[2] if len(sys.argv) > 2 else 'scraped_data.csv'\n    \n    scrape_and_save(url, output)\nEOF",
      "AGENT: ''",
      "AGENT: '--- Approach 6: Polite Scraping with Rate Limiting ---'",
      "AGENT: ''",
      "Create tool: `cat > polite_scraper.py << 'EOF'`",
      "#!/usr/bin/env python3\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nfrom urllib.parse import urljoin, urlparse\nimport urllib.robotparser\n\nclass PoliteScraper:\n    def __init__(self, delay=2, user_agent=None):\n        self.delay = delay\n        self.last_request_time = 0\n        self.user_agent = user_agent or 'Mozilla/5.0 (compatible; PoliteScraper/1.0)'\n        self.session = requests.Session()\n        self.session.headers.update({'User-Agent': self.user_agent})\n    \n    def can_fetch(self, url):\n        \"\"\"Check robots.txt to see if we can scrape this URL.\"\"\"\n        try:\n            parsed = urlparse(url)\n            robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n            \n            rp = urllib.robotparser.RobotFileParser()\n            rp.set_url(robots_url)\n            rp.read()\n            \n            return rp.can_fetch(self.user_agent, url)\n        except:\n            # If we can't read robots.txt, assume we can fetch\n            return True\n    \n    def fetch(self, url):\n        \"\"\"Fetch URL with rate limiting.\"\"\"\n        # Check robots.txt\n        if not self.can_fetch(url):\n            print(f\"⚠️  robots.txt disallows scraping: {url}\")\n            return None\n        \n        # Rate limiting: wait if needed\n        elapsed = time.time() - self.last_request_time\n        if elapsed < self.delay:\n            sleep_time = self.delay - elapsed\n            print(f\"⏳ Waiting {sleep_time:.1f}s...\")\n            time.sleep(sleep_time)\n        \n        print(f\"📥 Fetching: {url}\")\n        \n        try:\n            response = self.session.get(url, timeout=10)\n            self.last_request_time = time.time()\n            \n            if response.status_code == 200:\n                return BeautifulSoup(response.content, 'html.parser')\n            else:\n                print(f\"  Error: {response.status_code}\")\n                return None\n        except requests.RequestException as e:\n            print(f\"  Request failed: {e}\")\n            return None\n    \n    def scrape_urls(self, urls):\n        \"\"\"Scrape multiple URLs politely.\"\"\"\n        results = []\n        \n        for i, url in enumerate(urls, 1):\n            print(f\"\\n[{i}/{len(urls)}]\")\n            soup = self.fetch(url)\n            \n            if soup:\n                title = soup.find('title')\n                results.append({\n                    'url': url,\n                    'title': title.text if title else 'No title',\n                    'status': 'success'\n                })\n            else:\n                results.append({\n                    'url': url,\n                    'title': None,\n                    'status': 'failed'\n                })\n        \n        return results\n\nif __name__ == \"__main__\":\n    scraper = PoliteScraper(delay=2)\n    \n    urls = [\n        'https://example.com',\n        'https://example.org',\n        'https://example.net'\n    ]\n    \n    results = scraper.scrape_urls(urls)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"RESULTS\")\n    print(\"=\"*60)\n    for result in results:\n        status_icon = '✓' if result['status'] == 'success' else '✗'\n        print(f\"{status_icon} {result['url']}\")\n        if result['title']:\n            print(f\"  {result['title']}\")\nEOF",
      "AGENT: ''",
      "AGENT: '--- Bonus: Complete Web Scraper ---'",
      "AGENT: ''",
      "Create tool: `cat > web_scraper.py << 'EOF'`",
      "#!/usr/bin/env python3\n\"\"\"Complete web scraping tool.\"\"\"\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport json\nimport time\nimport argparse\nfrom datetime import datetime\n\nclass WebScraper:\n    def __init__(self, delay=2):\n        self.delay = delay\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n    \n    def fetch(self, url):\n        \"\"\"Fetch and parse URL.\"\"\"\n        try:\n            response = self.session.get(url, timeout=10)\n            if response.status_code == 200:\n                return BeautifulSoup(response.content, 'html.parser')\n        except Exception as e:\n            print(f\"Error fetching {url}: {e}\")\n        return None\n    \n    def extract_links(self, soup, base_url):\n        \"\"\"Extract all links from page.\"\"\"\n        from urllib.parse import urljoin\n        \n        links = []\n        for a in soup.find_all('a', href=True):\n            href = a['href']\n            full_url = urljoin(base_url, href)\n            text = a.text.strip()\n            links.append({'url': full_url, 'text': text})\n        \n        return links\n    \n    def scrape_to_json(self, url, output='output.json'):\n        \"\"\"Scrape and save to JSON.\"\"\"\n        soup = self.fetch(url)\n        if not soup:\n            return\n        \n        data = {\n            'url': url,\n            'title': soup.find('title').text if soup.find('title') else None,\n            'links': self.extract_links(soup, url),\n            'scraped_at': datetime.now().isoformat()\n        }\n        \n        with open(output, 'w') as f:\n            json.dump(data, f, indent=2)\n        \n        print(f\"✓ Saved to {output}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Web Scraper\")\n    parser.add_argument('url', help=\"URL to scrape\")\n    parser.add_argument('--output', default='output.json', help=\"Output file\")\n    parser.add_argument('--delay', type=int, default=2, help=\"Delay between requests\")\n    \n    args = parser.parse_args()\n    \n    scraper = WebScraper(delay=args.delay)\n    scraper.scrape_to_json(args.url, args.output)\nEOF",
      "AGENT: ''",
      "Run command: `chmod +x web_scraper.py`",
      "Run command: `./web_scraper.py https://example.com --output example_data.json`",
      "AGENT: '✓ Saved to example_data.json'",
      "AGENT: ''",
      "AGENT: '✓ SOLUTION SUMMARY:'",
      "AGENT: '  ✓ Used requests and BeautifulSoup for web scraping.'",
      "AGENT: '  ✓ Extracted specific elements with CSS selectors.'",
      "AGENT: '  ✓ Handled pagination across multiple pages.'",
      "AGENT: '  ✓ Parsed HTML tables into DataFrames.'",
      "AGENT: '  ✓ Saved results to CSV and JSON formats.'",
      "AGENT: '  ✓ Implemented polite scraping with rate limiting.'",
      "AGENT: '  ✓ Respected robots.txt and added delays.'",
      "AGENT: '  ✓ Built complete web scraper class.'",
      "AGENT: '  ✓ You now master web scraping with Python! 🕷️'"
    ],
    "commands": ["python", "python3", "pip"],
    "packages": {
      "debian_ubuntu": ["python3", "python3-pip"],
      "arch": ["python", "python-pip"]
    },
    "related_commands": ["curl", "wget"],
    "complexity": "intermediate",
    "tags": ["python", "web-scraping", "beautifulsoup", "requests", "html-parsing", "pagination", "data-extraction", "automation"]
  }
]
