[
  {
    "id": "scenario_research_first_log_analyzer_001",
    "command": "python",
    "scenario": "Developer: Need to analyze 50GB of application logs for errors",
    "problem": "Too much data to manually review, need automated analysis",
    "solution": "Research log parsing, create custom Python analyzer",
    "steps": [
      "USER SAYS: 'I have 50GB of application logs and need to find why the app crashed last night!'",
      "AGENT: 'Perfect case for a custom Python log analyzer. Let me research the best approach...'",
      "AGENT: ''",
      "AGENT: '🔍 RESEARCH PHASE: Log analysis best practices...'",
      "AGENT: '[Searching] \"python log parsing large files 2025\"'",
      "AGENT: '[Searching] \"memory efficient file processing python\"'",
      "AGENT: '[Found] Use generators for memory efficiency'",
      "AGENT: '[Found] re.compile() for faster pattern matching'",
      "AGENT: '[Found] multiprocessing for parallel processing'",
      "AGENT: '[Checking] Latest logging patterns and formats...'",
      "AGENT: ''",
      "AGENT: '✓ Research complete! Strategy:'",
      "AGENT: '  - Stream files line-by-line (memory efficient)'",
      "AGENT: '  - Use compiled regex for speed'",
      "AGENT: '  - Process multiple files in parallel'",
      "AGENT: '  - Extract patterns: errors, warnings, timestamps'",
      "AGENT: ''",
      "AGENT: 'First, let\\'s check the log format...'",
      "Check sample: head -20 /var/log/myapp/app.log",
      "AGENT: '2025-10-07 02:15:23 INFO [main] Application started'",
      "AGENT: '2025-10-07 02:15:24 DEBUG [db] Connected to database'",
      "AGENT: '2025-10-07 03:42:15 ERROR [api] NullPointerException at line 342'",
      "AGENT: '2025-10-07 03:42:15 ERROR [api] Stack trace: ...'",
      "AGENT: ''",
      "AGENT: '✓ Identified format: timestamp, level, component, message'",
      "AGENT: ''",
      "AGENT: 'Creating log_analyzer.py...'",
      "Create tool: cat > log_analyzer.py << 'EOF'",
      "#!/usr/bin/env python3",
      "\"\"\"",
      "Log Analyzer - Efficiently analyze large log files",
      "Handles GB-sized files with streaming and pattern matching",
      "\"\"\"",
      "import re",
      "import sys",
      "from pathlib import Path",
      "from datetime import datetime",
      "from collections import Counter, defaultdict",
      "import argparse",
      "",
      "class LogAnalyzer:",
      "    def __init__(self):",
      "        # Compile patterns once (performance)",
      "        self.log_pattern = re.compile(",
      "            r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) '",
      "            r'(\\w+) '",
      "            r'\\[([^\\]]+)\\] '",
      "            r'(.*)'",
      "        )",
      "        ",
      "        self.error_keywords = re.compile(",
      "            r'(exception|error|fail|crash|panic|fatal)',",
      "            re.IGNORECASE",
      "        )",
      "        ",
      "        # Statistics",
      "        self.stats = {",
      "            'total_lines': 0,",
      "            'errors': 0,",
      "            'warnings': 0,",
      "            'by_level': Counter(),",
      "            'by_component': Counter(),",
      "            'error_messages': []",
      "        }",
      "    ",
      "    def parse_line(self, line):",
      "        \"\"\"Parse single log line\"\"\"",
      "        match = self.log_pattern.match(line)",
      "        if match:",
      "            timestamp, level, component, message = match.groups()",
      "            return {",
      "                'timestamp': timestamp,",
      "                'level': level,",
      "                'component': component,",
      "                'message': message",
      "            }",
      "        return None",
      "    ",
      "    def analyze_file(self, filepath, time_window=None):",
      "        \"\"\"",
      "        Analyze log file efficiently (streaming)",
      "        Args:",
      "            filepath: Path to log file",
      "            time_window: Optional tuple (start_time, end_time) to filter",
      "        \"\"\"",
      "        print(f'Analyzing: {filepath}...')",
      "        ",
      "        with open(filepath, 'r', errors='ignore') as f:",
      "            for line in f:  # Stream line-by-line (memory efficient!)",
      "                self.stats['total_lines'] += 1",
      "                ",
      "                parsed = self.parse_line(line)",
      "                if not parsed:",
      "                    continue",
      "                ",
      "                # Time filtering",
      "                if time_window:",
      "                    log_time = datetime.fromisoformat(parsed['timestamp'])",
      "                    start, end = time_window",
      "                    if not (start <= log_time <= end):",
      "                        continue",
      "                ",
      "                # Statistics",
      "                level = parsed['level']",
      "                self.stats['by_level'][level] += 1",
      "                self.stats['by_component'][parsed['component']] += 1",
      "                ",
      "                # Track errors",
      "                if level == 'ERROR' or self.error_keywords.search(parsed['message']):",
      "                    self.stats['errors'] += 1",
      "                    self.stats['error_messages'].append({",
      "                        'time': parsed['timestamp'],",
      "                        'component': parsed['component'],",
      "                        'message': parsed['message'][:200]  # Truncate long messages",
      "                    })",
      "                ",
      "                elif level == 'WARNING':",
      "                    self.stats['warnings'] += 1",
      "    ",
      "    def analyze_directory(self, directory, pattern='*.log'):",
      "        \"\"\"Analyze all log files in directory\"\"\"",
      "        log_files = list(Path(directory).glob(pattern))",
      "        print(f'Found {len(log_files)} log files')",
      "        ",
      "        for log_file in log_files:",
      "            self.analyze_file(log_file)",
      "    ",
      "    def print_report(self):",
      "        \"\"\"Generate analysis report\"\"\"",
      "        print('\\n' + '='*80)",
      "        print('LOG ANALYSIS REPORT')",
      "        print('='*80)",
      "        print(f'Total lines processed: {self.stats[\"total_lines\"]:,}')",
      "        print(f'Errors found: {self.stats[\"errors\"]:,}')",
      "        print(f'Warnings found: {self.stats[\"warnings\"]:,}')",
      "        print()",
      "        ",
      "        print('Log Levels:')",
      "        for level, count in self.stats['by_level'].most_common():",
      "            print(f'  {level:10s}: {count:,}')",
      "        print()",
      "        ",
      "        print('Top Components:')",
      "        for component, count in self.stats['by_component'].most_common(10):",
      "            print(f'  {component:20s}: {count:,}')",
      "        print()",
      "        ",
      "        if self.stats['error_messages']:",
      "            print('Recent Errors (last 20):')",
      "            for err in self.stats['error_messages'][-20:]:",
      "                print(f'  [{err[\"time\"]}] {err[\"component\"]}: {err[\"message\"]}')",
      "            print()",
      "        ",
      "        # Identify crash time",
      "        if self.stats['error_messages']:",
      "            last_error = self.stats['error_messages'][-1]",
      "            print(f'💥 LAST ERROR BEFORE CRASH:')",
      "            print(f'   Time: {last_error[\"time\"]}')",
      "            print(f'   Component: {last_error[\"component\"]}')",
      "            print(f'   Message: {last_error[\"message\"]}')",
      "    ",
      "    def export_errors(self, output_file='errors.txt'):",
      "        \"\"\"Export all errors to file\"\"\"",
      "        with open(output_file, 'w') as f:",
      "            for err in self.stats['error_messages']:",
      "                f.write(f'[{err[\"time\"]}] {err[\"component\"]}: {err[\"message\"]}\\n')",
      "        print(f'\\nErrors exported to: {output_file}')",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(description='Analyze log files')",
      "    parser.add_argument('path', help='Log file or directory')",
      "    parser.add_argument('--pattern', default='*.log', help='File pattern for directory')",
      "    parser.add_argument('--export', help='Export errors to file')",
      "    parser.add_argument('--after', help='Only analyze logs after this time (YYYY-MM-DD HH:MM:SS)')",
      "    parser.add_argument('--before', help='Only analyze logs before this time')",
      "    ",
      "    args = parser.parse_args()",
      "    ",
      "    # Parse time window",
      "    time_window = None",
      "    if args.after or args.before:",
      "        start = datetime.fromisoformat(args.after) if args.after else datetime.min",
      "        end = datetime.fromisoformat(args.before) if args.before else datetime.max",
      "        time_window = (start, end)",
      "    ",
      "    analyzer = LogAnalyzer()",
      "    ",
      "    path = Path(args.path)",
      "    if path.is_file():",
      "        analyzer.analyze_file(path, time_window)",
      "    elif path.is_dir():",
      "        analyzer.analyze_directory(path, args.pattern)",
      "    else:",
      "        print(f'Error: {path} not found')",
      "        sys.exit(1)",
      "    ",
      "    analyzer.print_report()",
      "    ",
      "    if args.export:",
      "        analyzer.export_errors(args.export)",
      "",
      "if __name__ == '__main__':",
      "    main()",
      "EOF",
      "chmod +x log_analyzer.py",
      "AGENT: '✓ Created log_analyzer.py'",
      "AGENT: ''",
      "AGENT: 'Testing on your logs...'",
      "Run: python3 log_analyzer.py /var/log/myapp/ --after '2025-10-07 03:00:00'",
      "AGENT: 'Found 47 log files'",
      "AGENT: 'Analyzing: /var/log/myapp/app.log...'",
      "AGENT: 'Analyzing: /var/log/myapp/app.log.1...'",
      "AGENT: '...'",
      "AGENT: ''",
      "AGENT: '═══════════════════════════════════════════════════════════════'",
      "AGENT: 'LOG ANALYSIS REPORT'",
      "AGENT: '═══════════════════════════════════════════════════════════════'",
      "AGENT: 'Total lines processed: 2,458,392'",
      "AGENT: 'Errors found: 1,247'",
      "AGENT: 'Warnings found: 3,892'",
      "AGENT: ''",
      "AGENT: 'Log Levels:'",
      "AGENT: '  INFO      : 2,234,567'",
      "AGENT: '  DEBUG     : 215,403'",
      "AGENT: '  WARNING   : 3,892'",
      "AGENT: '  ERROR     : 1,247'",
      "AGENT: '  CRITICAL  : 3'",
      "AGENT: ''",
      "AGENT: 'Top Components:'",
      "AGENT: '  api                 : 1,458,392'",
      "AGENT: '  db                  : 842,301'",
      "AGENT: '  cache               : 98,234'",
      "AGENT: '  auth                : 45,392'",
      "AGENT: ''",
      "AGENT: '💥 LAST ERROR BEFORE CRASH:'",
      "AGENT: '   Time: 2025-10-07 03:42:15'",
      "AGENT: '   Component: api'",
      "AGENT: '   Message: OutOfMemoryError: Java heap space - unable to allocate 2048MB'",
      "AGENT: ''",
      "AGENT: '✓ FOUND THE ISSUE! Out of memory error at 03:42:15!'",
      "AGENT: ''",
      "Export errors: python3 log_analyzer.py /var/log/myapp/ --after '2025-10-07 03:00:00' --export crash_errors.txt",
      "AGENT: 'Errors exported to: crash_errors.txt'",
      "AGENT: ''",
      "USER: 'Can we also track patterns over time? Like error spikes?'",
      "AGENT: '🔍 RESEARCH: Time-series analysis for log patterns...'",
      "AGENT: '[Searching] \"python time series anomaly detection\"'",
      "AGENT: '[Found] pandas for time-series grouping'",
      "AGENT: '[Found] simple moving averages for spike detection'",
      "AGENT: ''",
      "AGENT: 'Creating enhanced version with trend analysis...'",
      "Create enhanced: cat > log_analyzer_trends.py << 'EOF'",
      "#!/usr/bin/env python3",
      "\"\"\"",
      "Log Analyzer with Trend Analysis",
      "Detects error spikes and patterns over time",
      "\"\"\"",
      "import re",
      "from pathlib import Path",
      "from datetime import datetime, timedelta",
      "from collections import defaultdict",
      "import sys",
      "",
      "class TrendAnalyzer:",
      "    def __init__(self, bucket_minutes=10):",
      "        self.bucket_minutes = bucket_minutes",
      "        self.log_pattern = re.compile(",
      "            r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (\\w+) \\[([^\\]]+)\\] (.*)'",
      "        )",
      "        ",
      "        # Time-bucketed statistics",
      "        self.error_buckets = defaultdict(int)",
      "        self.warning_buckets = defaultdict(int)",
      "        self.all_timestamps = []",
      "    ",
      "    def round_to_bucket(self, timestamp):",
      "        \"\"\"Round timestamp to nearest bucket\"\"\"",
      "        dt = datetime.fromisoformat(timestamp)",
      "        minutes = (dt.minute // self.bucket_minutes) * self.bucket_minutes",
      "        bucket = dt.replace(minute=minutes, second=0, microsecond=0)",
      "        return bucket",
      "    ",
      "    def analyze_file(self, filepath):",
      "        with open(filepath, 'r', errors='ignore') as f:",
      "            for line in f:",
      "                match = self.log_pattern.match(line)",
      "                if not match:",
      "                    continue",
      "                ",
      "                timestamp, level, component, message = match.groups()",
      "                bucket = self.round_to_bucket(timestamp)",
      "                self.all_timestamps.append(bucket)",
      "                ",
      "                if level == 'ERROR':",
      "                    self.error_buckets[bucket] += 1",
      "                elif level == 'WARNING':",
      "                    self.warning_buckets[bucket] += 1",
      "    ",
      "    def detect_spikes(self, threshold_multiplier=3):",
      "        \"\"\"Detect abnormal error spikes\"\"\"",
      "        if not self.error_buckets:",
      "            return []",
      "        ",
      "        # Calculate average and threshold",
      "        values = list(self.error_buckets.values())",
      "        avg = sum(values) / len(values)",
      "        threshold = avg * threshold_multiplier",
      "        ",
      "        # Find spikes",
      "        spikes = []",
      "        for timestamp, count in sorted(self.error_buckets.items()):",
      "            if count > threshold:",
      "                spikes.append((timestamp, count, avg))",
      "        ",
      "        return spikes",
      "    ",
      "    def print_timeline(self):",
      "        \"\"\"Print error timeline\"\"\"",
      "        print('\\nERROR TIMELINE (10-minute buckets):')",
      "        print('-' * 60)",
      "        ",
      "        for timestamp in sorted(set(self.all_timestamps)):",
      "            errors = self.error_buckets.get(timestamp, 0)",
      "            warnings = self.warning_buckets.get(timestamp, 0)",
      "            ",
      "            if errors > 0 or warnings > 0:",
      "                bar = '█' * min(errors, 50)  # Visual bar",
      "                print(f'{timestamp.strftime(\"%H:%M\")} │ {bar:50s} E:{errors:4d} W:{warnings:4d}')",
      "        ",
      "        print('-' * 60)",
      "    ",
      "    def print_spike_report(self):",
      "        spikes = self.detect_spikes()",
      "        ",
      "        if spikes:",
      "            print('\\n🚨 ANOMALOUS ERROR SPIKES DETECTED:')",
      "            for timestamp, count, avg in spikes:",
      "                print(f'  {timestamp} - {count} errors (avg: {avg:.1f}, {count/avg:.1f}x normal)')",
      "        else:",
      "            print('\\n✓ No unusual error spikes detected')",
      "",
      "def main():",
      "    import argparse",
      "    parser = argparse.ArgumentParser()",
      "    parser.add_argument('path', help='Log file or directory')",
      "    parser.add_argument('--bucket', type=int, default=10, help='Time bucket size (minutes)')",
      "    args = parser.parse_args()",
      "    ",
      "    analyzer = TrendAnalyzer(bucket_minutes=args.bucket)",
      "    ",
      "    path = Path(args.path)",
      "    if path.is_file():",
      "        analyzer.analyze_file(path)",
      "    elif path.is_dir():",
      "        for log_file in path.glob('*.log*'):",
      "            analyzer.analyze_file(log_file)",
      "    ",
      "    analyzer.print_timeline()",
      "    analyzer.print_spike_report()",
      "",
      "if __name__ == '__main__':",
      "    main()",
      "EOF",
      "chmod +x log_analyzer_trends.py",
      "AGENT: '✓ Created trend analyzer!'",
      "AGENT: ''",
      "Run trends: python3 log_analyzer_trends.py /var/log/myapp/",
      "AGENT: ''",
      "AGENT: 'ERROR TIMELINE (10-minute buckets):'",
      "AGENT: '────────────────────────────────────────────────────────────'",
      "AGENT: '03:00 │ █                                                  E:   2 W:   5'",
      "AGENT: '03:10 │ ██                                                 E:   3 W:   8'",
      "AGENT: '03:20 │ ███                                                E:   4 W:  12'",
      "AGENT: '03:30 │ █████████████████████████████████████████████████  E: 458 W: 127'",
      "AGENT: '03:40 │ ██████████████████████████████████████████████████ E: 742 W: 189'",
      "AGENT: '03:50 │                                                    E:   0 W:   0'",
      "AGENT: '────────────────────────────────────────────────────────────'",
      "AGENT: ''",
      "AGENT: '🚨 ANOMALOUS ERROR SPIKES DETECTED:'",
      "AGENT: '  2025-10-07 03:30:00 - 458 errors (avg: 3.2, 143.1x normal)'",
      "AGENT: '  2025-10-07 03:40:00 - 742 errors (avg: 3.2, 231.9x normal)'",
      "AGENT: ''",
      "AGENT: '✓ ANALYSIS COMPLETE!'",
      "AGENT: ''",
      "AGENT: 'FINDINGS:'",
      "AGENT: '  1. Normal error rate: 2-4 errors/10min'",
      "AGENT: '  2. Massive spike at 03:30-03:40 (100x+ normal)'",
      "AGENT: '  3. System crashed at 03:42:15'",
      "AGENT: '  4. Root cause: OutOfMemoryError'",
      "AGENT: ''",
      "AGENT: 'RECOMMENDATION:'",
      "AGENT: '  - Increase Java heap size: -Xmx4g'",
      "AGENT: '  - Review memory leaks in api component'",
      "AGENT: '  - Add memory monitoring alerts'",
      "AGENT: ''",
      "AGENT: '✓ SOLUTION SUMMARY:'",
      "AGENT: '  ✓ Researched log parsing strategies (streaming for 50GB files)'",
      "AGENT: '  ✓ Created custom Python analyzer (memory efficient)'",
      "AGENT: '  ✓ Processed 2.4M log lines in seconds'",
      "AGENT: '  ✓ Identified crash: OOM at 03:42:15'",
      "AGENT: '  ✓ Added trend analysis (detected 100x error spike)'",
      "AGENT: '  ✓ Generated actionable recommendations'",
      "AGENT: ''",
      "AGENT: 'Tools created:'",
      "AGENT: '  - log_analyzer.py (efficient log parsing + reporting)'",
      "AGENT: '  - log_analyzer_trends.py (time-series + anomaly detection)'",
      "AGENT: '  - crash_errors.txt (exported error list)'",
      "AGENT: ''",
      "AGENT: 'This custom Python tool solved a problem that would take hours manually! ✓'"
    ],
    "commands": ["python3", "head", "grep"],
    "packages": {"debian_ubuntu": ["python3"], "arch": ["python"]},
    "related_commands": ["grep", "awk", "sed", "python3"],
    "complexity": "advanced",
    "tags": ["python", "log-analysis", "custom-tool", "research-first", "big-data", "troubleshooting", "real-world", "ai-agent", "integration"]
  }
]
