[
  {
    "id": "scenario_understand_load_avg_061",
    "command": "uptime",
    "scenario": "A junior admin is confused about high load averages on a server with low CPU usage.",
    "problem": "The `uptime` or `top` command shows a high load average (e.g., 15.0 on a 4-core system), but the CPU usage (`%us` + `%sy`) is low. The user incorrectly assumes the server is overloaded with CPU tasks and doesn't know what to investigate next.",
    "solution": "Explain the fundamental difference between CPU utilization and load average. Load average includes processes in a runnable state *and* processes in an uninterruptible sleep state (like waiting for disk I/O). Then, use tools like `vmstat` and `iotop` to prove that the high load is caused by I/O wait, not a CPU bottleneck.",
    "steps": [
      "USER SAYS: 'My server is really slow! `uptime` shows a load average of 15, but `top` says the CPU is 80% idle. This makes no sense. Is the server overloaded or not?'",
      "AGENT: 'That's an excellent and very common question! You've discovered the important difference between CPU usage and load average. A high load average doesn't always mean a high CPU load. Let me research a clear definition.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Linux load average vs CPU utilization...'",
      "AGENT: '[Searching] \"linux load average vs cpu usage explained\"'",
      "AGENT: '[Found] **CPU Utilization**: A real-time measurement of how many cycles the CPU is actively processing instructions.'",
      "AGENT: '[Found] **Load Average**: A measurement of the number of tasks in the kernel's run queue (waiting for CPU time) *plus* the number of tasks in uninterruptible sleep (usually waiting for disk I/O).'",
      "AGENT: '[Found] **Analogy**: Imagine a supermarket checkout. CPU usage is how busy the cashier is scanning items. Load average is how many people are in the line *plus* any people whose carts are stuck and can't move.'",
      "AGENT: '[Found] If load is high and CPU is low, the bottleneck is almost always disk I/O.'",
      "AGENT: '\u2713 Research complete! Let's prove this theory on your system.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Confirm the Symptom with `uptime` and `top` ---'",
      "AGENT: 'Let's look at the numbers again.'",
      "Run command: `uptime`",
      "AGENT: ' 20:10:01 up 5 days, 2:30,  1 user,  load average: 15.10, 10.50, 5.25'",
      "AGENT: 'The first number, 15.10, is the 1-minute load average. On a 4-core system, a load over 4.0 means tasks are having to wait.'",
      "Run command: `top -bn1 | grep '%Cpu'`",
      "AGENT: '%Cpu(s):  5.0 us,  2.1 sy,  0.0 ni, 15.0 id, 77.5 wa,  0.0 hi,  0.4 si,  0.0 st'",
      "AGENT: '\ud83d\udca1 Your CPU is 15% idle (`id`), but 77.5% in I/O wait (`wa`). This confirms the analogy: your cashier is mostly idle, waiting for stuck carts. The problem is I/O.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Verify with `vmstat` ---'",
      "AGENT: 'The `vmstat` command gives us another view. The `procs` section has `r` (runnable) and `b` (blocked) columns.'",
      "Run command: `vmstat 2 5`",
      "AGENT: 'procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----'",
      "AGENT: ' r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st'",
      "AGENT: ' 2 14 0      1.2G   1.5G   2.3G    0    0  8192 123456 1234 5678  5  2 15 78  0'",
      "AGENT: ' 1 15 0      1.2G   1.5G   2.3G    0    0  9123 134567 2345 6789  4  3 13 80  0'",
      "AGENT: '...'",
      "AGENT: 'The `b` column shows ~15 processes are in the 'blocked' state, waiting for I/O. The `r` column (runnable, waiting for CPU) is low. This confirms the I/O bottleneck.'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Find the Cause with `iotop` ---'",
      "AGENT: 'Now let's find out *which* process is causing all this I/O.'",
      "Run command: `sudo iotop -b -n 1`",
      "AGENT: 'Total DISK READ: 8.00 M/s | Total DISK WRITE: 121.00 M/s'",
      "AGENT: '  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO>    COMMAND'",
      "AGENT: ' 4567 be/4 backupusr  0.00 B/s  120.50 M/s  0.00 % 98.10 % rsync -a /data /mnt/backup'",
      "AGENT: '\ud83d\udca1 The `rsync` process, run by the `backupusr`, is writing heavily to disk and is spending 98% of its time waiting on I/O. This is the source of your high load average.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Explained that Load Average is not the same as CPU Utilization; it also includes processes waiting for I/O.'",
      "AGENT: '  \u2713 Used `top` and `vmstat` to confirm that the high load was due to a large number of processes being blocked (`b` column) and high I/O wait (`%wa`).'",
      "AGENT: '  \u2713 Used `iotop` to pinpoint the specific `rsync` process that was saturating the disk.'",
      "AGENT: '  \u2713 You now understand how to correctly interpret load average and diagnose I/O-bound performance problems. The solution is to either speed up the disk (e.g., faster hardware) or reduce the I/O load (e.g., run the backup at off-peak hours). \ud83d\udea6'"
    ],
    "commands": [
      "uptime",
      "top",
      "vmstat",
      "iotop"
    ],
    "packages": {
      "debian_ubuntu": [
        "procps",
        "sysstat",
        "iotop"
      ],
      "arch": [
        "procps-ng",
        "sysstat",
        "iotop"
      ]
    },
    "related_commands": [
      "iostat",
      "ps",
      "dstat"
    ],
    "complexity": "beginner",
    "tags": [
      "performance",
      "troubleshooting",
      "load-average",
      "cpu",
      "io-wait",
      "beginner",
      "linux-basics"
    ]
  },
  {
    "id": "scenario_git_merge_conflict_062",
    "command": "git",
    "scenario": "Resolve a simple Git merge conflict.",
    "problem": "A developer tries to merge a feature branch into the main branch, but Git halts the process with the error: `CONFLICT (content): Merge conflict in <file>`. They are unsure how to proceed and fix the conflict.",
    "solution": "Explain what a merge conflict is and walk through the standard resolution process: 1. Identify the conflicted file using `git status`. 2. Open the file and inspect the conflict markers (`<<<<<<<`, `=======`, `>>>>>>>`). 3. Manually edit the file to choose the correct code. 4. Use `git add` to mark the conflict as resolved. 5. Complete the merge with `git commit`.",
    "steps": [
      "USER SAYS: 'I tried to `git merge feature-branch` into `main`, and now I have a scary 'MERGE CONFLICT' error. My terminal says I'm in a 'merging' state. How do I fix this?'",
      "AGENT: 'Don't worry, this is a normal part of working with Git and is very fixable! It just means you and someone else edited the same lines in the same file, and Git doesn't know which version to keep. It needs a human to make the decision. Let's walk through the resolution process.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Standard Git merge conflict resolution workflow...'",
      "AGENT: '[Searching] \"how to fix git merge conflict\"'",
      "AGENT: '[Found] `git status` is the first command to run. It will clearly show which files are 'unmerged'.'",
      "AGENT: '[Found] Git adds special markers to the conflicted files: `<<<<<<< HEAD` (what's in your current branch), `=======` (a separator), and `>>>>>>> feature-branch` (what's in the branch you're merging).'",
      "AGENT: '[Found] The process is: Edit the file to look exactly how you want it, removing the Git markers.'",
      "AGENT: '[Found] `git add <file>` tells Git you have resolved the conflict in that file.'",
      "AGENT: '[Found] `git commit` finalizes the merge.'",
      "AGENT: '[Found] `git merge --abort` is a safe escape hatch to cancel the merge and go back to the state before you started.'",
      "AGENT: '\u2713 Research complete! Let's resolve your conflict.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Identify the Conflict ---'",
      "AGENT: 'First, let's see exactly what Git is telling us.'",
      "Run command: `git status`",
      "AGENT: 'On branch main'",
      "AGENT: 'Your branch is up to date with 'origin/main'.'",
      "AGENT: ''",
      "AGENT: 'You have unmerged paths.'",
      "AGENT: '  (fix conflicts and run \"git commit\")'",
      "AGENT: '  (use \"git merge --abort\" to abort the merge)'",
      "AGENT: ''",
      "AGENT: 'Unmerged paths:'",
      "AGENT: '  (use \"git add <file>...\" to mark resolution)'",
      "AGENT: '    both modified:   config.js'",
      "AGENT: ''",
      "AGENT: 'no changes added to commit (use \"git add\" and/or \"git commit -a\")'",
      "AGENT: '\ud83d\udca1 `git status` clearly tells us that `config.js` is the file with the conflict.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Edit the Conflicted File ---'",
      "AGENT: 'Now, let's open `config.js` in a text editor. You will see something like this:'",
      "Run command: `cat config.js`",
      "AGENT: 'const config = {'",
      "AGENT: '  api_version: 2,'",
      "AGENT: '<<<<<<< HEAD'",
      "AGENT: '  endpoint: \"https://api.prod.example.com\",'",
      "AGENT: '======='",
      "AGENT: '  endpoint: \"https://api.dev.example.com\",'",
      "AGENT: '>>>>>>> feature-branch'",
      "AGENT: '  timeout: 5000'",
      "AGENT: '};'",
      "AGENT: ''",
      "AGENT: 'Git is showing you both versions. The code between `<<<<<<< HEAD` and `=======` is what's in your `main` branch. The code between `=======` and `>>>>>>> feature-branch` is from the branch you're merging. You must now edit this file to be correct. For example, if you want to keep the production endpoint, you would make the file look like this:'",
      "AGENT: '```javascript'",
      "AGENT: 'const config = {'",
      "AGENT: '  api_version: 2,'",
      "AGENT: '  endpoint: \"[https://api.prod.example.com](https://api.prod.example.com)\",'",
      "AGENT: '  timeout: 5000'",
      "AGENT: '};'",
      "AGENT: '```'",
      "AGENT: 'Just delete the lines you don't want, and also delete the `<<<<<<<`, `=======`, and `>>>>>>>` markers themselves. Save the file.'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Mark as Resolved and Commit ---'",
      "AGENT: 'Now that you've fixed the file, you need to tell Git that you're done. This is called 'staging' the file.'",
      "Run command: `git add config.js`",
      "AGENT: ''",
      "AGENT: 'Let's check the status again.'",
      "Run command: `git status`",
      "AGENT: 'On branch main'",
      "AGENT: 'All conflicts fixed but you are still merging.'",
      "AGENT: '  (use \"git commit\" to conclude merge)'",
      "AGENT: ''",
      "AGENT: 'Changes to be committed:'",
      "AGENT: '    modified:   config.js'",
      "AGENT: ''",
      "AGENT: 'The message is now 'All conflicts fixed'. The final step is to create the merge commit.'",
      "Run command: `git commit`",
      "AGENT: '(This will open your text editor with a pre-populated commit message like `Merge branch 'feature-branch'`. You can just save and close it.)'",
      "AGENT: '[main 1a2b3c4] Merge branch 'feature-branch''",
      "AGENT: ''",
      "AGENT: '\u2713 The merge is complete and the conflict is resolved! The `feature-branch` has now been successfully merged into `main`.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Used `git status` to identify the file with the merge conflict.'",
      "AGENT: '  \u2713 Opened the file and explained how to interpret the `<<<<<<< HEAD`, `=======`, and `>>>>>>> branch` conflict markers.'",
      "AGENT: '  \u2713 Edited the file to the desired final state and removed the conflict markers.'",
      "AGENT: '  \u2713 Used `git add` to stage the resolved file, signaling to Git that the conflict was fixed.'",
      "AGENT: '  \u2713 Finalized the process with `git commit` to create the merge commit. \ud83e\udd1d'"
    ],
    "commands": [
      "git status",
      "git merge",
      "git add",
      "git commit",
      "cat",
      "git merge --abort"
    ],
    "packages": {
      "debian_ubuntu": [
        "git"
      ],
      "arch": [
        "git"
      ]
    },
    "related_commands": [
      "git rebase",
      "git diff",
      "git log",
      "git mergetool"
    ],
    "complexity": "beginner",
    "tags": [
      "git",
      "vcs",
      "merge-conflict",
      "troubleshooting",
      "beginner",
      "devops"
    ]
  },
  {
    "id": "scenario_python_yaml_json_converter_063",
    "command": "python",
    "scenario": "Convert a file from YAML to JSON format, and vice-versa.",
    "problem": "A user has a configuration file in YAML format but needs to provide it to a tool that only accepts JSON. They need a quick and reliable way to convert between the two formats.",
    "solution": "Create a Python script that uses the `PyYAML` library to parse YAML and the built-in `json` library to dump JSON. The script will be bidirectional, automatically detecting the input file type and converting to the other format.",
    "steps": [
      "USER SAYS: 'I have a `config.yaml` file, but this program I'm using needs a `config.json` file. How can I convert it?'",
      "AGENT: 'That's a very common data transformation task. Since JSON is a subset of YAML, this conversion is usually straightforward. We can build a simple Python script to handle this reliably. Let me research the standard Python library for YAML.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Python YAML and JSON manipulation...'",
      "AGENT: '[Searching] \"python convert yaml to json\"'",
      "AGENT: '[Found] `PyYAML` is the de-facto standard library for parsing and emitting YAML in Python.'",
      "AGENT: '[Found] Python has a built-in `json` library that is highly optimized and easy to use.'",
      "AGENT: '[Concept] The logic is simple: 1. Read the source file. 2. Use the appropriate library (`yaml.safe_load` or `json.load`) to parse it into a Python dictionary. 3. Use the other library (`json.dump` or `yaml.dump`) to write the dictionary to a new file.'",
      "AGENT: '\u2713 Research complete! Let's build a converter tool.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Install `PyYAML` ---'",
      "AGENT: 'The `json` library is built-in, but we need to install the YAML library.'",
      "Run command: `pip install PyYAML`",
      "AGENT: 'Successfully installed PyYAML-6.0.1'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Build the Converter Script ---'",
      "Create tool: `cat > convert.py << 'EOF'`",
      "#!/usr/bin/env python3\n\n\"\"\"A tool to convert files between YAML and JSON formats.\"\"\"\n\nimport sys\nimport json\nimport yaml # from PyYAML\nfrom pathlib import Path\n\ndef convert_file(input_path_str):\n    \"\"\"Converts a YAML file to JSON, or a JSON file to YAML.\"\"\"\n    input_path = Path(input_path_str)\n    if not input_path.is_file():\n        print(f\"Error: Input file not found at '{input_path_str}'\")\n        return\n\n    suffix = input_path.suffix.lower()\n    \n    try:\n        with open(input_path, 'r') as f:\n            if suffix in ['.yaml', '.yml']:\n                # YAML to JSON\n                output_path = input_path.with_suffix('.json')\n                print(f\"Converting {input_path.name} (YAML) -> {output_path.name} (JSON)...\")\n                data = yaml.safe_load(f)\n                with open(output_path, 'w') as out_f:\n                    json.dump(data, out_f, indent=2)\n            elif suffix == '.json':\n                # JSON to YAML\n                output_path = input_path.with_suffix('.yaml')\n                print(f\"Converting {input_path.name} (JSON) -> {output_path.name} (YAML)...\")\n                data = json.load(f)\n                with open(output_path, 'w') as out_f:\n                    yaml.dump(data, out_f, default_flow_style=False, sort_keys=False)\n            else:\n                print(f\"Error: Unsupported file type '{suffix}'. Please use .yaml, .yml, or .json.\")\n                return\n        print(f\"Success! File saved to '{output_path}'.\")\n    except Exception as e:\n        print(f\"An error occurred during conversion: {e}\")\n\nif __name__ == '__main__':\n    if len(sys.argv) != 2:\n        print(\"Usage: python convert.py <input_file>\")\n        sys.exit(1)\n    \n    convert_file(sys.argv[1])\n",
      "EOF",
      "chmod +x convert.py",
      "AGENT: '\u2713 Created the converter script!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Use the Script ---'",
      "AGENT: 'First, let's create a sample YAML file.'",
      "Create tool: `echo -e 'api_key: \"abc-123\"\\nsettings:\\n  retries: 3\\n  ports:\\n    - 80\\n    - 443' > config.yaml`",
      "AGENT: ''",
      "AGENT: 'Now, let's convert it to JSON.'",
      "Run command: `./convert.py config.yaml`",
      "AGENT: 'Converting config.yaml (YAML) -> config.json (JSON)...'",
      "AGENT: 'Success! File saved to 'config.json'.'",
      "AGENT: ''",
      "AGENT: 'Let's check the contents of the new JSON file.'",
      "Run command: `cat config.json`",
      "AGENT: '```json'",
      "AGENT: '{'",
      "AGENT: '  \"api_key\": \"abc-123\",'",
      "AGENT: '  \"settings\": {'",
      "AGENT: '    \"retries\": 3,',",
      "AGENT: '    \"ports\": [',",
      "AGENT: '      80,',",
      "AGENT: '      443',",
      "AGENT: '    ]',",
      "AGENT: '  }',",
      "AGENT: '}'",
      "AGENT: '```'",
      "AGENT: 'Perfect! Now let's try converting it back to YAML.'",
      "Run command: `./convert.py config.json`",
      "AGENT: 'Converting config.json (JSON) -> config.yaml (YAML)...'",
      "AGENT: 'Success! File saved to 'config.yaml'.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched and utilized the standard Python libraries for handling YAML (`PyYAML`) and JSON (built-in `json`).'",
      "AGENT: '  \u2713 Built a smart, reusable command-line tool that automatically detects the input file type and converts to the other format.'",
      "AGENT: '  \u2713 Demonstrated the bidirectional conversion, successfully turning a YAML file into a well-formatted JSON file and back again.'",
      "AGENT: '  \u2713 You now have a handy utility for any data format conversion tasks. \u2194\ufe0f'"
    ],
    "commands": [
      "python3",
      "pip",
      "cat",
      "chmod",
      "echo"
    ],
    "packages": {
      "debian_ubuntu": [
        "python3-pip",
        "python3-yaml"
      ],
      "arch": [
        "python-pip",
        "python-yaml"
      ]
    },
    "related_commands": [
      "yq",
      "jq",
      "json2yaml",
      "yaml2json"
    ],
    "complexity": "beginner",
    "tags": [
      "python",
      "yaml",
      "json",
      "data-conversion",
      "automation",
      "beginner",
      "custom-tool"
    ]
  },
  {
    "id": "scenario_python_ssl_expiry_checker_067",
    "command": "python",
    "scenario": "Check the expiration date of SSL certificates for a list of domains.",
    "problem": "An operator is responsible for many websites and needs to be proactive about renewing SSL certificates before they expire. Manually checking each certificate in a browser is tedious and easy to forget.",
    "solution": "Create a Python script that uses the built-in `ssl` and `socket` libraries to connect to a server, retrieve its SSL certificate, parse the certificate data to find the 'notAfter' date, and report how many days are left until it expires.",
    "steps": [
      "USER SAYS: 'I'm always worried I'll forget to renew an SSL certificate and a website will go down. Can I build a script to check all my domains and warn me if any are expiring soon?'",
      "AGENT: 'That's a vital piece of automation for any system operator. A Python script can do this very reliably without any external dependencies. Let me research the standard library modules for handling SSL connections and parsing certificates.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Checking SSL certificate details in Python...'",
      "AGENT: '[Searching] \"python get ssl certificate expiration date standard library\"'",
      "AGENT: '[Found] The `ssl` module is the key. `ssl.create_default_context()` sets up the connection.'",
      "AGENT: '[Found] The `socket` module is used to establish the underlying TCP connection to the server on port 443.'",
      "AGENT: '[Found] `ssl_socket.getpeercert()`: This method returns the server's certificate as a Python dictionary.'",
      "AGENT: '[Found] The expiration date is in the `notAfter` field of this dictionary. The date format is `'%b %d %H:%M:%S %Y %Z'`, which needs to be parsed into a `datetime` object for comparison.'",
      "AGENT: '\u2713 Research complete! Let's build the checker.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: The Manual `openssl` command (for comparison) ---'",
      "AGENT: 'You could do this from the command line with `openssl`, but parsing the output is cumbersome.'",
      "Run command: `openssl s_client -connect google.com:443 2>/dev/null | openssl x509 -noout -enddate`",
      "AGENT: 'notAfter=Dec 12 10:25:21 2025 GMT'",
      "AGENT: 'This works for one domain, but it's hard to automate for a list and calculate the remaining days. Python will be much better.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Build the Python Certificate Checker ---'",
      "Create tool: `cat > cert_checker.py << 'EOF'`",
      "#!/usr/bin/env python3\n\n\"\"\"Checks the SSL certificate expiration for a list of domains.\"\"\"\n\nimport ssl\nimport socket\nfrom datetime import datetime\nfrom rich.console import Console\nfrom rich.table import Table\n\nDOMAINS_TO_CHECK = [\n    'google.com',\n    'github.com',\n    'expired.badssl.com',\n    'wrong.host.badssl.com',\n    'self-signed.badssl.com',\n]\nWARNING_DAYS = 30\n\ndef get_cert_expiry_date(hostname):\n    \"\"\"Connects to a host and returns the cert's expiry datetime object.\"\"\"\n    context = ssl.create_default_context()\n    try:\n        with socket.create_connection((hostname, 443), timeout=5) as sock:\n            with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n                cert = ssock.getpeercert()\n                expiry_str = cert['notAfter']\n                # Parse the date format: 'Month Day HH:MM:SS YYYY Timezone'\n                return datetime.strptime(expiry_str, '%b %d %H:%M:%S %Y %Z')\n    except (socket.gaierror, socket.timeout, ConnectionRefusedError) as e:\n        return f\"Connection Error: {e}\"\n    except ssl.SSLCertVerificationError as e:\n        return f\"Cert Verify Error: {e.args[0]}\"\n    except Exception as e:\n        return f\"Error: {e}\"\n\ndef main():\n    \"\"\"Checks all domains and prints a report.\"\"\"\n    console = Console()\n    table = Table(title=\"SSL Certificate Expiration Report\")\n    table.add_column(\"Status\", justify=\"center\", style=\"bold\")\n    table.add_column(\"Domain\", style=\"cyan\")\n    table.add_column(\"Expires On\")\n    table.add_column(\"Days Left\", justify=\"right\")\n\n    today = datetime.now()\n\n    for domain in DOMAINS_TO_CHECK:\n        expiry_date = get_cert_expiry_date(domain)\n        \n        if isinstance(expiry_date, datetime):\n            days_left = (expiry_date - today).days\n            if days_left < 0:\n                status, style = \"\ud83d\udd25 EXPIRED\", \"red\"\n            elif days_left < WARNING_DAYS:\n                status, style = \"\u26a0\ufe0f WARN\", \"yellow\"\n            else:\n                status, style = \"\u2705 OK\", \"green\"\n            table.add_row(f\"[{style}]{status}[/{style}]\", domain, expiry_date.strftime('%Y-%m-%d'), str(days_left))\n        else: # It's an error string\n            table.add_row(\"[bold red]\u274c ERROR[/bold red]\", domain, expiry_date, \"N/A\")\n    \n    console.print(table)\n\nif __name__ == '__main__':\n    main()\n",
      "EOF",
      "pip install rich",
      "chmod +x cert_checker.py",
      "AGENT: '\u2713 Created the certificate checker script!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Run and Analyze the Report ---'",
      "Run command: `./cert_checker.py`",
      "AGENT: '[Simulated Rich Table Output]'",
      "AGENT: '                            SSL Certificate Expiration Report                            '",
      "AGENT: '\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513'",
      "AGENT: '\u2503 Status    \u2503 Domain                    \u2503 Expires On   \u2503 Days Left \u2503'",
      "AGENT: '\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529'",
      "AGENT: '\u2502 \u2705 OK     \u2502 google.com                \u2502 2025-12-12   \u2502        66 \u2502'",
      "AGENT: '\u2502 \u26a0\ufe0f WARN    \u2502 github.com                \u2502 2025-10-28   \u2502        21 \u2502'",
      "AGENT: '\u2502 \ud83d\udd25 EXPIRED  \u2502 expired.badssl.com        \u2502 2015-04-12   \u2502     -3830 \u2502'",
      "AGENT: '\u2502 \u274c ERROR    \u2502 wrong.host.badssl.com     \u2502 Cert Verify\u2026 \u2502       N/A \u2502'",
      "AGENT: '\u2502 \u274c ERROR    \u2502 self-signed.badssl.com    \u2502 Cert Verify\u2026 \u2502       N/A \u2502'",
      "AGENT: '\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518'",
      "AGENT: ''",
      "AGENT: '\ud83d\udca1 The report is clear and actionable. `google.com` is fine. `github.com` is expiring in 21 days and needs attention soon. `expired.badssl.com` is long expired. The other two failed with verification errors, which is also useful information.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched the native Python `ssl` and `socket` modules for retrieving TLS certificate information.'",
      "AGENT: '  \u2713 Built a robust script that connects to domains, fetches their certificates, and correctly parses the expiration date.'",
      "AGENT: '  \u2713 The script calculates the number of days remaining and includes logic for OK, WARNING, and EXPIRED states.'",
      "AGENT: '  \u2713 It handles common network and SSL errors gracefully, reporting them clearly in the final output.'",
      "AGENT: '  \u2713 You now have an automated tool that can be run via a cron job to ensure you never miss an SSL certificate renewal. \ud83d\udd12'"
    ],
    "commands": [
      "python3",
      "pip",
      "openssl"
    ],
    "packages": {
      "debian_ubuntu": [
        "python3-pip",
        "openssl"
      ],
      "arch": [
        "python-pip",
        "openssl"
      ]
    },
    "related_commands": [
      "nmap --script ssl-cert",
      "testssl.sh"
    ],
    "complexity": "intermediate",
    "tags": [
      "python",
      "ssl",
      "tls",
      "certificate",
      "monitoring",
      "automation",
      "security",
      "custom-tool"
    ]
  },
  {
    "id": "scenario_fix_too_many_open_files_068",
    "command": "ulimit",
    "scenario": "Diagnose and fix an application failing with a 'Too many open files' error.",
    "problem": "A high-traffic server application, like a web server or database, crashes or starts rejecting connections. Its logs show the error message `(24) Too many open files`. The user doesn't know what this limit is or how to increase it.",
    "solution": "Explain that the 'Too many open files' error relates to per-process file descriptor limits. Use `ulimit -n` to check the current limit, `lsof -p <PID>` to see how many files a process has open, and then show how to permanently increase the limit for a service using a systemd override file or by editing `/etc/security/limits.conf`.",
    "steps": [
      "USER SAYS: 'My Nginx server just crashed! The logs are full of errors saying `accept() failed (24: Too many open files)`. What does this mean and how do I fix it?'",
      "AGENT: 'That's a classic resource limit problem. Every process on Linux has a limit on how many 'file descriptors' it can have open at once. For a busy web server, every connection, log file, and config file counts. Your Nginx process has hit this limit. Let's diagnose and raise it.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Linux 'Too many open files' error...'",
      "AGENT: '[Searching] \"linux fix too many open files nginx\"'",
      "AGENT: '[Found] The error corresponds to the `RLIMIT_NOFILE` resource limit.'",
      "AGENT: '[Found] `ulimit -n` (for 'number of files') shows the current soft limit for the shell.'",
      "AGENT: '[Found] `lsof -p <PID> | wc -l` is a great way to count the number of open files for a specific process.'",
      "AGENT: '[Found] **For systemd services (modern way)**: The limit should be set in the service's unit file or an override file using the `LimitNOFILE` directive.'",
      "AGENT: '[Found] **For older systems/user sessions**: The limit is set in `/etc/security/limits.conf`.'",
      "AGENT: '\u2713 Research complete! Since Nginx is a systemd service, we'll use the modern systemd override method.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Check the Current Limits ---'",
      "AGENT: 'First, let's find the PID of the Nginx master process.'",
      "Run command: `pgrep -o nginx`",
      "AGENT: '1234'",
      "AGENT: ''",
      "AGENT: 'Now, let's check the actual limit for that running process by inspecting its process information.'",
      "Run command: `cat /proc/1234/limits | grep 'Max open files'`",
      "AGENT: 'Max open files            1024                 4096                 files     '",
      "AGENT: 'This shows a 'soft' limit of 1024 and a 'hard' limit of 4096. The soft limit is what's causing the error. 1024 is a very low number for a busy web server.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Count the Open Files ---'",
      "AGENT: 'Let's confirm that Nginx is indeed hitting this limit. We'll use `lsof` to count its open file descriptors.'",
      "Run command: `sudo lsof -p 1234 | wc -l`",
      "AGENT: '1025'",
      "AGENT: '\ud83d\udca1 Yup. The process has exactly 1025 file descriptors open, which is just over the soft limit of 1024. This confirms the diagnosis.'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Increase the Limit via a systemd Override ---'",
      "AGENT: 'The best way to change settings for a systemd service is to create an override file. This keeps your changes separate from the main package file.'",
      "Run command: `sudo systemctl edit nginx`",
      "AGENT: 'This will open an empty text file. Add the following content:'",
      "AGENT: '```ini'",
      "AGENT: '[Service]'",
      "AGENT: '# Increase the number of file descriptors for Nginx'",
      "AGENT: 'LimitNOFILE=65536'",
      "AGENT: '```'",
      "AGENT: 'Save and close the file. This creates a file like `/etc/systemd/system/nginx.service.d/override.conf`.'",
      "AGENT: 'Now, we need to reload the systemd daemon and restart Nginx for the new limit to take effect.'",
      "Run command: `sudo systemctl daemon-reload`",
      "Run command: `sudo systemctl restart nginx`",
      "AGENT: ''",
      "AGENT: '--- Verification ---'",
      "AGENT: 'Let's find the new PID and check the limit again.'",
      "Run command: `pgrep -o nginx`",
      "AGENT: '5678'",
      "Run command: `cat /proc/5678/limits | grep 'Max open files'`",
      "AGENT: 'Max open files            65536                65536                files     '",
      "AGENT: ''",
      "AGENT: '\u2713 Success! The soft and hard limits for Nginx have now been raised to 65,536. This should be more than enough to handle the server's traffic without hitting the limit again.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Explained that the 'Too many open files' error is a per-process resource limit.'",
      "AGENT: '  \u2713 Used `/proc/<PID>/limits` to inspect the running process's actual limits and confirmed a low value of 1024.'",
      "AGENT: '  \u2713 Used `lsof` to count the number of open files and verify that the process was at its limit.'",
      "AGENT: '  \u2713 Followed modern best practices by using `systemctl edit` to create a systemd override file, setting `LimitNOFILE` to a much higher value.'",
      "AGENT: '  \u2713 Reloaded the configuration and restarted the service, then verified that the new, higher limit was successfully applied. \ud83d\udcc2'"
    ],
    "commands": [
      "ulimit",
      "lsof",
      "pgrep",
      "cat",
      "systemctl",
      "wc"
    ],
    "packages": {
      "debian_ubuntu": [
        "nginx",
        "lsof",
        "procps"
      ],
      "arch": [
        "nginx",
        "lsof",
        "procps-ng"
      ]
    },
    "related_commands": [
      "sysctl",
      "prlimit"
    ],
    "complexity": "intermediate",
    "tags": [
      "performance",
      "troubleshooting",
      "ulimit",
      "file-descriptors",
      "nginx",
      "systemd",
      "resource-limits"
    ]
  },
  {
    "id": "scenario_lynis_security_audit_071",
    "command": "lynis",
    "scenario": "Perform an automated security audit of a Linux system.",
    "problem": "A sysadmin has inherited a new server and has no idea about its security posture. They need a tool that can perform a comprehensive security scan and provide actionable recommendations for hardening the system.",
    "solution": "Use `lynis`, an open-source security auditing tool. Run an audit, analyze the report which covers areas like system hardening, malware scanning, and configuration errors, and then implement one of the suggested fixes as an example.",
    "steps": [
      "USER SAYS: 'I need to do a security audit on a new server to make sure it's properly hardened. Where do I even start?'",
      "AGENT: 'That's a great question. Manually checking hundreds of settings is impossible. The best approach is to use an automated auditing tool. One of the most popular open-source tools for this is `lynis`. It checks your system against a wide range of security best practices. Let's get it running.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Automated Linux security auditing with `lynis`...'",
      "AGENT: '[Searching] \"lynis security audit tutorial\"'",
      "AGENT: '[Found] `lynis` is a shell script, making it easy to run on almost any Linux/Unix system.'",
      "AGENT: '[Found] The main command is `lynis audit system`.'",
      "AGENT: '[Found] It doesn't perform any actual hardening; it only provides suggestions. This makes it safe to run.'",
      "AGENT: '[Found] The output is color-coded: green is good, yellow is a suggestion, red is a high-priority issue.'",
      "AGENT: '[Found] The report provides links to external resources for more information on each suggestion.'",
      "AGENT: '\u2713 Research complete! Let's install `lynis` and run our first audit.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Install `lynis` ---'",
      "AGENT: 'The easiest way is often from the system's package manager.'",
      "Run command: `sudo apt-get install lynis -y` (or `sudo pacman -S lynis`)",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Run the System Audit ---'",
      "AGENT: 'Now, let's run the main audit command. This can take a few minutes as it checks many different areas of your system.'",
      "Run command: `sudo lynis audit system`",
      "AGENT: '[ Lynis 3.0.8 ]'",
      "AGENT: ''",
      "AGENT: '>> Initializing...'",
      "AGENT: '... (A lot of output scrolls by as it performs tests) ...'",
      "AGENT: '[+] Hardening index: 65 [#############       ]'",
      "AGENT: ''",
      "AGENT: 'Suggestions (12):'",
      "AGENT: '----------------------------'",
      "AGENT: '  * Harden the system by installing a default firewall [FIRE-4512]'",
      "AGENT: '    https://cisofy.com/lynis/controls/FIRE-4512/'",
      "AGENT: '  * Install a PAM module for password strength testing like pam_cracklib or pam_passwdqc. [AUTH-9262]'",
      "AGENT: '    https://cisofy.com/lynis/controls/AUTH-9262/'",
      "AGENT: '  * Consider hardening SSH configuration [SSH-7408]'",
      "AGENT: '    - Check running SSH daemon configuration: PermitRootLogin'",
      "AGENT: '    https://cisofy.com/lynis/controls/SSH-7408/'",
      "AGENT: '  * To decrease the impact of a full /tmp file system, place /tmp on a separate partition [FILE-6310]'",
      "AGENT: '    https://cisofy.com/lynis/controls/FILE-6310/'",
      "AGENT: '... (many more suggestions) ...'",
      "AGENT: ''",
      "AGENT: 'Log file location: /var/log/lynis.log'",
      "AGENT: 'Report data: /var/log/lynis-report.dat'",
      "AGENT: ''",
      "AGENT: '\ud83d\udca1 The audit is complete! It gives us a 'Hardening Index' score and a prioritized list of suggestions.'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Implement a Suggestion ---'",
      "AGENT: 'Let's pick one of the high-priority suggestions and fix it. `lynis` suggested we harden SSH by disabling root login.'",
      "AGENT: 'Suggestion: `Consider hardening SSH configuration [SSH-7408] ... PermitRootLogin`'",
      "AGENT: ''",
      "AGENT: 'This is a critical security measure. First, ensure you have a non-root user with `sudo` privileges that you can log in with!'",
      "AGENT: 'Now, let's edit the SSH config.'",
      "Run command: `sudo nano /etc/ssh/sshd_config`",
      "AGENT: 'Find the line `#PermitRootLogin prohibit-password` and change it to:'",
      "AGENT: '`PermitRootLogin no`'",
      "AGENT: 'Save the file.'",
      "AGENT: ''",
      "AGENT: 'Now, restart the SSH service.'",
      "Run command: `sudo systemctl restart sshd`",
      "AGENT: ''",
      "AGENT: 'If we run `lynis` again, the suggestion for `PermitRootLogin` will be gone, and our hardening index will increase.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Identified `lynis` as a powerful, safe, and easy-to-use tool for automated security auditing.'",
      "AGENT: '  \u2713 Installed `lynis` and ran a full system audit using `sudo lynis audit system`.'",
      "AGENT: '  \u2713 Analyzed the report, paying attention to the color-coded suggestions and the overall hardening index.'",
      "AGENT: '  \u2713 Acted on a high-priority suggestion by editing the `sshd_config` to disable root login, thereby improving the system's security posture.'",
      "AGENT: '  \u2713 You now have a repeatable process to audit and systematically harden your Linux servers. \ud83d\udee1\ufe0f'"
    ],
    "commands": [
      "lynis",
      "apt-get",
      "nano",
      "systemctl"
    ],
    "packages": {
      "debian_ubuntu": [
        "lynis"
      ],
      "arch": [
        "lynis"
      ]
    },
    "related_commands": [
      "chkrootkit",
      "rkhunter",
      "openscap",
      "aide"
    ],
    "complexity": "intermediate",
    "tags": [
      "security",
      "audit",
      "hardening",
      "lynis",
      "compliance",
      "vulnerability-scanning"
    ]
  },
  {
    "id": "scenario_sftp_chroot_jail_075",
    "command": "sshd_config",
    "scenario": "Create a restricted SFTP user that is jailed to their home directory.",
    "problem": "A user needs to grant a third party access to upload files to a server, but for security reasons, this user must not be able to get a shell login or see any part of the filesystem outside of their own home directory.",
    "solution": "Configure the OpenSSH server to create a `chroot` jail for a specific user or group. This involves creating a dedicated user, modifying the `sshd_config` file to use an internal SFTP server and chroot rules, and ensuring the permissions on the home directory are correct (owned by root).",
    "steps": [
      "USER SAYS: 'I need to create an SFTP account for a client to upload files, but they absolutely cannot be allowed to poke around the server or get a shell. How can I lock them into just their upload directory?'",
      "AGENT: 'That's a very important security requirement. We can achieve this with what's called an 'SFTP chroot jail'. It changes the root directory for that user's session, so to them, their home directory *is* the entire filesystem. Let's research the exact `sshd_config` directives and permission requirements.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: SFTP chroot jail setup...'",
      "AGENT: '[Searching] \"openssh sftp chroot jail setup\"'",
      "AGENT: '[Found] This is a built-in feature of OpenSSH.'",
      "AGENT: '[Found] **Key `sshd_config` directives**:'",
      "AGENT: '  - `Subsystem sftp internal-sftp`: Tells SSH to use its built-in SFTP server, which is required for chroot.'",
      "AGENT: '  - `Match Group sftpusers`: Creates a conditional block that applies rules only to users in the `sftpusers` group.'",
      "AGENT: '  - `ChrootDirectory %h`: Sets the jail to the user's home directory (`%h`).'",
      "AGENT: '  - `ForceCommand internal-sftp`: Forces the user to only be able to use SFTP, disabling shell access.'",
      "AGENT: '  - `AllowTcpForwarding no`, `X11Forwarding no`: Extra security settings.'",
      "AGENT: '[Found] **Crucial Permission Requirement**: The chroot directory (the user's home) *must* be owned by `root` and not be writable by anyone else. The user needs a writable subdirectory inside their home.'",
      "AGENT: '\u2713 Research complete! Let's set it up.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Create the User and Group ---'",
      "AGENT: 'First, let's create a dedicated group for our SFTP users, and then create a new user.'",
      "Run command: `sudo groupadd sftpusers`",
      "Run command: `sudo useradd -m -g sftpusers -s /sbin/nologin client-user`",
      "AGENT: '  - `-m`: Creates the home directory (`/home/client-user`).'",
      "AGENT: '  - `-g sftpusers`: Puts them in our new group.'",
      "AGENT: '  - `-s /sbin/nologin`: Disables shell access as a first layer of defense.'",
      "AGENT: ''",
      "AGENT: 'Now, set a password for the new user.'",
      "Run command: `sudo passwd client-user`",
      "AGENT: 'New password: ...'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Configure Permissions ---'",
      "AGENT: 'This is the most critical and often missed step. The user's home directory must be owned by root. We will then create a writable `uploads` directory inside it that is owned by the user.'",
      "Run command: `sudo chown root:root /home/client-user`",
      "Run command: `sudo chmod 755 /home/client-user`",
      "AGENT: ''",
      "AGENT: 'Now, create the writable subdirectory.'",
      "Run command: `sudo mkdir /home/client-user/uploads`",
      "Run command: `sudo chown client-user:sftpusers /home/client-user/uploads`",
      "Run command: `sudo chmod 755 /home/client-user/uploads`",
      "AGENT: '\u2713 Permissions are now correct. The user will be jailed in `/home/client-user` but will only be able to write inside `/home/client-user/uploads`.'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Configure `sshd_config` ---'",
      "AGENT: 'Now we apply the rules to the SSH server configuration.'",
      "Run command: `sudo nano /etc/ssh/sshd_config`",
      "AGENT: 'First, find the `Subsystem sftp` line and comment it out, then add the `internal-sftp` line.'",
      "AGENT: '`# from:`'",
      "AGENT: '`Subsystem       sftp    /usr/lib/openssh/sftp-server`'",
      "AGENT: '`# to:`'",
      "AGENT: '`#Subsystem      sftp    /usr/lib/openssh/sftp-server`'",
      "AGENT: '`Subsystem       sftp    internal-sftp`'",
      "AGENT: ''",
      "AGENT: 'Now, go to the very end of the file and add the `Match` block:'",
      "AGENT: '```'",
      "AGENT: 'Match Group sftpusers'",
      "AGENT: '    ChrootDirectory %h'",
      "AGENT: '    ForceCommand internal-sftp'",
      "AGENT: '    AllowTcpForwarding no'",
      "AGENT: '    X11Forwarding no'",
      "AGENT: '```'",
      "AGENT: 'Save the file. Now, test the configuration and restart the service.'",
      "Run command: `sudo sshd -t`",
      "AGENT: '(No output means the syntax is OK).'",
      "Run command: `sudo systemctl restart sshd`",
      "AGENT: ''",
      "AGENT: '--- Verification ---'",
      "AGENT: 'Try to SSH as the user. It should be rejected.'",
      "Run command: `ssh client-user@localhost`",
      "AGENT: 'This service allows sftp connections only.'",
      "AGENT: 'Connection to localhost closed.'",
      "AGENT: ''",
      "AGENT: 'Now, try to connect with SFTP.'",
      "Run command: `sftp client-user@localhost`",
      "AGENT: 'Connected to localhost.'",
      "AGENT: 'sftp> ls -l'",
      "AGENT: 'drwxr-xr-x    2 1001     1002         4096 Oct  7 20:00 uploads'",
      "AGENT: 'sftp> pwd'",
      "AGENT: 'Remote working directory: /'",
      "AGENT: 'sftp> cd uploads'",
      "AGENT: 'sftp> put somefile.txt'",
      "AGENT: 'Uploading somefile.txt to /uploads/somefile.txt'",
      "AGENT: 'sftp> '",
      "AGENT: ''",
      "AGENT: '\u2713 Success! The user is logged in via SFTP, their root directory (`/`) is actually `/home/client-user`, and they can only write to the `uploads` folder.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Created a dedicated group and user for SFTP access with `nologin` shell.'",
      "AGENT: '  \u2713 Meticulously configured directory permissions, making the user's home directory owned by `root` to establish the chroot boundary.'",
      "AGENT: '  \u2713 Modified `sshd_config` to use `internal-sftp` and added a `Match Group` block to apply the `ChrootDirectory` and `ForceCommand` rules.'",
      "AGENT: '  \u2713 Verified that shell access was denied and that SFTP access was successfully jailed to the home directory.'",
      "AGENT: '  \u2713 You have successfully created a secure, restricted environment for file uploads. \ud83d\udd12'"
    ],
    "commands": [
      "groupadd",
      "useradd",
      "passwd",
      "chown",
      "chmod",
      "mkdir",
      "nano",
      "sshd",
      "systemctl",
      "ssh",
      "sftp"
    ],
    "packages": {
      "debian_ubuntu": [
        "openssh-server"
      ],
      "arch": [
        "openssh"
      ]
    },
    "related_commands": [
      "vsftpd",
      "proftpd"
    ],
    "complexity": "intermediate",
    "tags": [
      "security",
      "sftp",
      "ssh",
      "chroot",
      "hardening",
      "file-transfer",
      "permissions"
    ]
  },
  {
    "id": "scenario_ebpf_python_bcc_079",
    "command": "python",
    "scenario": "Trace all `open()` system calls in real-time using eBPF.",
    "problem": "A developer needs to understand exactly which files a complex, running application is accessing without using the performance-intensive `strace`. They need a low-overhead way to spy on specific kernel activities.",
    "solution": "Use the BCC (BPF Compiler Collection) framework, which allows you to write Python scripts that attach eBPF programs to kernel functions. We will write a simple BCC script to attach a probe to the `open` syscall, pull the data into Python, and print a formatted output of the PID, command, and filename being opened.",
    "steps": [
      "USER SAYS: 'I want to see every file that's being opened on my system, in real-time. `strace` is too slow and produces too much output. Is there a more modern, efficient way?'",
      "AGENT: 'Yes, absolutely. You're describing a perfect use case for eBPF (extended Berkeley Packet Filter). eBPF lets you run safe, sandboxed programs inside the Linux kernel itself. The easiest way to use it is with a Python framework called BCC. We can write a script to trace the `open` system call with minimal performance impact. This is an advanced topic, so I'll research the setup carefully.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Tracing syscalls with eBPF and BCC...'",
      "AGENT: '[Searching] \"python bcc ebpf trace open syscall\"'",
      "AGENT: '[Found] **BCC (BPF Compiler Collection)**: A toolkit for creating eBPF programs, with a Python frontend that makes it much easier.'",
      "AGENT: '[Found] **Installation**: BCC requires kernel headers and other development tools, as it compiles the eBPF C code on the fly.'",
      "AGENT: '[Found] **Core Concept**: The Python script will contain a small snippet of C code. The BCC library compiles this C code into eBPF bytecode, loads it into the kernel, and attaches it to a 'kprobe' (kernel probe) on the `do_sys_openat2` function (the modern `open` syscall).'",
      "AGENT: '[Found] **Data Transfer**: Data is passed from the kernel eBPF program to the userspace Python script via a BPF 'perf buffer'. The Python script then reads from this buffer in a loop.'",
      "AGENT: '\u2713 Research complete! Let's install the prerequisites and write the script.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Install BCC and Kernel Headers ---'",
      "AGENT: 'Installation can be complex and varies by distribution. On Ubuntu, it's generally done like this.'",
      "Run command: `sudo apt-get update`",
      "Run command: `sudo apt-get install -y bpfcc-tools linux-headers-$(uname -r)`",
      "AGENT: 'This installs the BCC compiler tools and the kernel headers matching your currently running kernel, which are required for compilation.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Write the BCC/Python Tracing Script ---'",
      "Create tool: `cat > opensnoop.py << 'EOF'`",
      "#!/usr/bin/python3\n\n\"\"\"A simple file open snooper using eBPF with BCC.\"\"\"\n\nfrom bcc import BPF\nfrom datetime import datetime\n\n# 1. Define the eBPF C program\n# This program will be compiled and loaded into the kernel.\nbpf_text = \"\"\"\n#include <uapi/linux/ptrace.h>\n#include <linux/fcntl.h>\n\n// Define the data structure to pass from kernel to user space\nstruct data_t {\n    u32 pid;\n    char comm[TASK_COMM_LEN];\n    char fname[256];\n};\n\n// Define the perf buffer to send the data\nBPF_PERF_OUTPUT(events);\n\n// This function is attached to the kernel's open syscall function\nint trace_open(struct pt_regs *ctx) {\n    struct data_t data = {};\n    bpf_get_current_comm(&data.comm, sizeof(data.comm));\n    data.pid = bpf_get_current_pid_tgid() >> 32;\n    \n    // The filename is the second argument to the syscall\n    bpf_probe_read_user_str(&data.fname, sizeof(data.fname), (void *)PT_REGS_PARM2(ctx));\n    \n    // Submit the data to the perf buffer\n    events.perf_submit(ctx, &data, sizeof(data));\n    return 0;\n}\n\"\"\"\n\n# 2. Load the BPF program\nb = BPF(text=bpf_text)\n\n# 3. Attach a kprobe to the syscall function\n# We use 'do_sys_openat2' which is the underlying function for open/openat.\nb.attach_kprobe(event=\"do_sys_openat2\", fn_name=\"trace_open\")\n\nprint(\"Tracing file opens... Press Ctrl-C to end.\")\n\n# 4. Define a Python function to handle events from the perf buffer\ndef print_event(cpu, data, size):\n    event = b[\"events\"].event(data)\n    time_str = datetime.now().strftime(\"%H:%M:%S\")\n    print(f\"{time_str:<10} {event.pid:<7} {event.comm.decode():<16} {event.fname.decode()}\")\n\n# 5. Open the perf buffer and loop, printing events\nb[\"events\"].open_perf_buffer(print_event)\nprint(f\"{'TIME':<10} {'PID':<7} {'COMMAND':<16} {'FILENAME'}\")\nwhile 1:\n    try:\n        b.perf_buffer_poll()\n    except KeyboardInterrupt:\n        exit()\n",
      "EOF",
      "AGENT: '\u2713 Created the eBPF tracing script!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Run the Snoop Script ---'",
      "AGENT: 'This script requires root privileges to load programs into the kernel.'",
      "Run command: `sudo python3 opensnoop.py`",
      "AGENT: 'Tracing file opens... Press Ctrl-C to end.'",
      "AGENT: 'TIME       PID     COMMAND          FILENAME'",
      "AGENT: '20:30:01   12345   gnome-terminal   /etc/ld.so.cache'",
      "AGENT: '20:30:02   5432    cat              /etc/passwd'",
      "AGENT: '20:30:02   5433    ls               /etc/ld.so.cache'",
      "AGENT: '20:30:03   9876    code             /home/user/.config/Code/User/settings.json'",
      "AGENT: '20:30:04   1122    firefox          /home/user/.mozilla/firefox/profiles.ini'",
      "AGENT: '(The script will continue to print a line for every file opened anywhere on the system until you press Ctrl-C).'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Explained the concepts of eBPF and the BCC framework as a modern, high-performance alternative to `strace`.'",
      "AGENT: '  \u2713 Installed the necessary BCC tools and kernel headers.'",
      "AGENT: '  \u2713 Wrote a Python script that embeds a C program, which BCC compiles and attaches to the `open` syscall in the kernel.'",
      "AGENT: '  \u2713 The script successfully captured data from the kernel probe and printed a real-time stream of all opened files across the entire system with very low overhead.'",
      "AGENT: '  \u2713 You have now unlocked the power of eBPF for deep, real-time system observability. \ud83d\udc1d'"
    ],
    "commands": [
      "python3",
      "apt-get",
      "uname",
      "cat"
    ],
    "packages": {
      "debian_ubuntu": [
        "bpfcc-tools",
        "linux-headers-generic"
      ],
      "arch": [
        "bcc"
      ]
    },
    "related_commands": [
      "strace",
      "lsof",
      "perf trace",
      "bpftrace"
    ],
    "complexity": "advanced",
    "tags": [
      "ebpf",
      "bpf",
      "bcc",
      "python",
      "performance",
      "tracing",
      "observability",
      "kernel",
      "advanced"
    ]
  },
  {
    "id": "scenario_rsync_basics_041",
    "command": "rsync",
    "scenario": "Efficiently synchronize a local directory with a remote server.",
    "problem": "A user needs to transfer a large directory of files to a remote server. Using `scp` is inefficient because it copies everything, every time. They need a way to only transfer the files that have changed, which would be much faster for subsequent updates.",
    "solution": "Use `rsync`, a powerful utility designed for efficient file synchronization. Demonstrate the use of its common flags, like `-a` (archive), `-v` (verbose), `-z` (compress), and `--delete` (remove extraneous files from the destination).",
    "steps": [
      "USER SAYS: 'I have to upload my website files to my server every time I make a change. `scp -r ./site user@server:/var/www/html` is so slow because it copies all 10,000 files again, even if I only changed one.'",
      "AGENT: 'That's a perfect use case for `rsync`! It's specifically designed to solve this problem by only transferring the differences. Let me research the most common and effective flags to use.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: `rsync` for directory synchronization...'",
      "AGENT: '[Searching] \"rsync command for deploying website\"'",
      "AGENT: '[Found] `rsync` is the standard tool for this. It uses a delta-transfer algorithm.'",
      "AGENT: '[Found] **Key Flags**:'",
      "AGENT: '  - `-a` (archive mode): A magical flag that's equivalent to `-rlptgoD`. It preserves permissions, timestamps, symbolic links, etc., which is crucial.'",
      "AGENT: '  - `-v` (verbose): Shows you what files are being transferred.'",
      "AGENT: '  - `-z` (compress): Compresses file data during the transfer, which is great for text-based files.'",
      "AGENT: '  - `--delete`: Deletes files on the destination if they no longer exist on the source. Use with caution!'",
      "AGENT: '  - `--dry-run`: Shows what `rsync` *would* do without actually doing it. Excellent for testing.'",
      "AGENT: '\u2713 Research complete! Let's build the command step-by-step.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: The Inefficient `scp` (Baseline) ---'",
      "AGENT: 'Your current command `scp -r ./site user@server:/var/www/html` re-transmits every single byte of every file, which is why it's slow after the initial upload.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: A Safe First `rsync` with `--dry-run` ---'",
      "AGENT: 'Let's construct the `rsync` command but use `--dry-run` first to see what it plans to do. Note the trailing slash on the source directory `./site/` - this is important! It means 'copy the contents of `site`' not 'copy the directory `site` itself'.'",
      "Run command: `rsync -avz --dry-run ./site/ user@server:/var/www/html`",
      "AGENT: 'sending incremental file list'",
      "AGENT: '(DRY RUN) index.html'",
      "AGENT: '(DRY RUN) css/style.css'",
      "AGENT: '(DRY RUN) images/logo.png'",
      "AGENT: '... (lists all 10,000 files) ...'",
      "AGENT: ''",
      "AGENT: 'sent 123,456 bytes  received 789 bytes  82,823.33 bytes/sec'",
      "AGENT: 'total size is 500,123,456  speedup is 4032.10 (DRY RUN)'",
      "AGENT: 'This looks correct for the first run; it plans to send everything.'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: The Real `rsync` and a Second Run ---'",
      "AGENT: 'Now, let's run it for real.'",
      "Run command: `rsync -avz ./site/ user@server:/var/www/html`",
      "AGENT: 'sending incremental file list'",
      "AGENT: 'index.html'",
      "AGENT: 'css/style.css'",
      "AGENT: 'images/logo.png'",
      "AGENT: '... (output scrolls by as it transfers) ...'",
      "AGENT: ''",
      "AGENT: 'Now, let's modify a single file.'",
      "Run command: `touch ./site/index.html`",
      "AGENT: ''",
      "AGENT: 'And run the exact same `rsync` command again.'",
      "Run command: `rsync -avz ./site/ user@server:/var/www/html`",
      "AGENT: 'sending incremental file list'",
      "AGENT: 'index.html'",
      "AGENT: ''",
      "AGENT: 'sent 1,234 bytes  received 35 bytes  2,538.00 bytes/sec'",
      "AGENT: 'total size is 500,123,456  speedup is 394,108.32'",
      "AGENT: '\ud83d\udca1 Look at that! It was almost instantaneous. `rsync` knew that only `index.html` had changed and transferred only that file.'",
      "AGENT: ''",
      "AGENT: '--- Approach 4: Using `--delete` to Keep Directories in Sync ---'",
      "AGENT: 'What if you delete a file locally? It will remain on the server. The `--delete` flag fixes this.'",
      "Run command: `rm ./site/images/old_logo.png`",
      "AGENT: 'Now, run rsync with `--delete` (and always with `--dry-run` first!).'",
      "Run command: `rsync -avz --delete --dry-run ./site/ user@server:/var/www/html`",
      "AGENT: 'sending incremental file list'",
      "AGENT: 'deleting images/old_logo.png'",
      "AGENT: '\ud83d\udca1 The dry run shows it plans to delete the file on the remote end. If that's what you want, run it again without `--dry-run`.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Explained why `scp -r` is inefficient for repeated synchronizations.'",
      "AGENT: '  \u2713 Researched and introduced `rsync` with its most useful flags: `-avz` (archive, verbose, compress).'",
      "AGENT: '  \u2713 Demonstrated the power of `rsync`'s delta-transfer algorithm by showing a subsequent sync is near-instant.'",
      "AGENT: '  \u2713 Introduced the `--delete` flag for maintaining a true mirror and stressed the importance of using `--dry-run` for safety.'",
      "AGENT: '  \u2713 You now have a fast, efficient, and safe way to deploy your files. \ud83d\udd04'"
    ],
    "commands": [
      "rsync",
      "scp",
      "touch",
      "rm"
    ],
    "packages": {
      "debian_ubuntu": [
        "rsync",
        "openssh-client"
      ],
      "arch": [
        "rsync",
        "openssh"
      ]
    },
    "related_commands": [
      "sftp",
      "tar",
      "unison"
    ],
    "complexity": "beginner",
    "tags": [
      "rsync",
      "file-transfer",
      "backup",
      "synchronization",
      "devops",
      "beginner"
    ]
  },
  {
    "id": "scenario_python_password_gen_042",
    "command": "python",
    "scenario": "Create a command-line tool to generate strong, random passwords.",
    "problem": "A user needs to create secure, random passwords for various services. They want a flexible tool that can generate passwords of a specified length and can optionally include numbers, symbols, and mixed case.",
    "solution": "Develop a Python script using the built-in `secrets` module for cryptographically secure random number generation. Use `argparse` to create a user-friendly command-line interface for specifying password complexity rules.",
    "steps": [
      "USER SAYS: 'I need to generate a bunch of strong passwords. Can you help me make a little command-line tool for it?'",
      "AGENT: 'Excellent idea! A custom password generator is a great way to ensure strong, unique passwords. We'll use Python's `secrets` module, which is specifically designed for generating cryptographically secure tokens. Let me do a quick research.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Secure random password generation in Python...'",
      "AGENT: '[Searching] \"python generate secure random password\"'",
      "AGENT: '[Found] The `secrets` module is the recommended choice over the standard `random` module for any security-sensitive application. `secrets.choice()` is perfect for this.'",
      "AGENT: '[Found] The `string` module provides handy pre-defined character sets like `string.ascii_letters`, `string.digits`, and `string.punctuation`.'",
      "AGENT: '[Found] The `argparse` module is the standard way to create powerful command-line interfaces in Python.'",
      "AGENT: '\u2713 Research complete! Let's build the tool.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Simple but Insecure Method (for comparison) ---'",
      "AGENT: 'One could try to use shell tools, but getting truly random distribution can be tricky.'",
      "AGENT: '`head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16; echo ''`'",
      "AGENT: 'This works, but it's less flexible and harder to read than a Python script. Our Python version will be much more user-friendly.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Build the Python Password Generator ---'",
      "Create tool: `cat > pwgen.py << 'EOF'`",
      "#!/usr/bin/env python3\n\n\"\"\"A secure password generator command-line tool.\"\"\"\n\nimport argparse\nimport secrets\nimport string\n\ndef generate_password(length, use_digits, use_symbols):\n    \"\"\"Generates a single secure, random password.\"\"\"\n    # Start with lowercase and uppercase letters\n    alphabet = string.ascii_letters\n    if use_digits:\n        alphabet += string.digits\n    if use_symbols:\n        alphabet += string.punctuation\n\n    # Ensure the password meets complexity requirements by including at least one of each requested type\n    password_chars = []\n    if use_digits:\n        password_chars.append(secrets.choice(string.digits))\n    if use_symbols:\n        password_chars.append(secrets.choice(string.punctuation))\n    # Always include at least one letter\n    password_chars.append(secrets.choice(string.ascii_letters))\n\n    # Fill the rest of the password length\n    remaining_length = length - len(password_chars)\n    for _ in range(remaining_length):\n        password_chars.append(secrets.choice(alphabet))\n    \n    # Shuffle the list to ensure character positions are random\n    secrets.SystemRandom().shuffle(password_chars)\n    \n    return \"\".join(password_chars)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Generate strong, random passwords.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument('-l', '--length', type=int, default=16, help='Length of the password.')\n    parser.add_argument('-n', '--no-digits', action='store_true', help='Exclude digits from the password.')\n    parser.add_argument('-s', '--no-symbols', action='store_true', help='Exclude symbols from the password.')\n    parser.add_argument('-c', '--count', type=int, default=1, help='Number of passwords to generate.')\n    args = parser.parse_args()\n\n    if args.length < 4:\n        print(\"Error: Password length must be at least 4.\")\n        exit(1)\n\n    for i in range(args.count):\n        password = generate_password(\n            length=args.length,\n            use_digits=not args.no_digits,\n            use_symbols=not args.no_symbols\n        )\n        print(password)\n",
      "EOF",
      "chmod +x pwgen.py",
      "AGENT: '\u2713 Created the password generator tool!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Use the Tool ---'",
      "AGENT: 'Let's try it out with a few different options.'",
      "AGENT: 'Generate a default 16-character password:'",
      "Run command: `./pwgen.py`",
      "AGENT: 'w&g(T@kF]z9v!2eJ'",
      "AGENT: ''",
      "AGENT: 'Generate a long, 24-character password with no symbols:'",
      "Run command: `./pwgen.py --length 24 --no-symbols`",
      "AGENT: '4fQh8L7bN2kRjZpYtG3wXc5V'",
      "AGENT: ''",
      "AGENT: 'Generate 5 short passwords with only letters:'",
      "Run command: `./pwgen.py -l 8 --no-digits --no-symbols --count 5`",
      "AGENT: 'fGzXqWbN'",
      "AGENT: 'jRpVdKcE'",
      "AGENT: 'sLhYnBvA'",
      "AGENT: 'uIzOmQpT'",
      "AGENT: 'aXcVbNmZ'",
      "AGENT: ''",
      "AGENT: 'Check the help message generated by `argparse`:'",
      "Run command: `./pwgen.py --help`",
      "AGENT: 'usage: pwgen.py [-h] [-l LENGTH] [-n] [-s] [-c COUNT]'",
      "AGENT: ''",
      "AGENT: 'Generate strong, random passwords.'",
      "AGENT: ''",
      "AGENT: 'optional arguments:'",
      "AGENT: '  -h, --help            show this help message and exit'",
      "AGENT: '  -l LENGTH, --length LENGTH'",
      "AGENT: '                        Length of the password. (default: 16)'",
      "AGENT: '  -n, --no-digits       Exclude digits from the password. (default: False)'",
      "AGENT: '  -s, --no-symbols      Exclude symbols from the password. (default: False)'",
      "AGENT: '  -c COUNT, --count COUNT'",
      "AGENT: '                        Number of passwords to generate. (default: 1)'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched and chose the `secrets` module for cryptographically secure random generation.'",
      "AGENT: '  \u2713 Built a reusable command-line tool in Python with a flexible and user-friendly interface powered by `argparse`.'",
      "AGENT: '  \u2713 The script ensures that generated passwords meet the specified complexity requirements (e.g., they will contain a digit if digits are requested).'",
      "AGENT: '  \u2713 You now have a secure, customizable tool for creating strong passwords. \ud83d\udd11'"
    ],
    "commands": [
      "python3",
      "chmod",
      "cat",
      "head",
      "tr"
    ],
    "packages": {
      "debian_ubuntu": [
        "python3"
      ],
      "arch": [
        "python"
      ]
    },
    "related_commands": [
      "openssl rand",
      "pwgen",
      "gpg"
    ],
    "complexity": "beginner",
    "tags": [
      "python",
      "security",
      "password",
      "automation",
      "secrets",
      "argparse",
      "custom-tool",
      "beginner"
    ]
  },
  {
    "id": "scenario_mtr_packet_loss_048",
    "command": "mtr",
    "scenario": "Diagnose intermittent network packet loss to a remote host.",
    "problem": "A user is experiencing a poor connection to a remote server. `ping` shows that some packets are being dropped, but it doesn't show *where* on the network path the loss is occurring.",
    "solution": "Use `mtr` (My Traceroute), a powerful network diagnostic tool that combines the functionality of `traceroute` and `ping`. By continuously sending packets, `mtr` provides a real-time view of the latency and packet loss across every hop between the local machine and the destination.",
    "steps": [
      "USER SAYS: 'I'm trying to connect to my server at `example.com`, but the connection is really flaky. Sometimes it works, sometimes it times out. `ping` shows packet loss.'",
      "AGENT: 'Intermittent packet loss can be very tricky to diagnose. A simple `ping` only tells you that loss is happening, not where. For this, we need a tool that can inspect the entire network path. The best tool for the job is `mtr`. Let me research its usage and how to interpret its output.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Diagnosing packet loss with mtr...'",
      "AGENT: '[Searching] \"linux troubleshoot packet loss mtr\"'",
      "AGENT: '[Found] `mtr` combines `traceroute` (path discovery) and `ping` (latency/loss measurement).'",
      "AGENT: '[Found] It runs continuously, providing updating statistics for each hop.'",
      "AGENT: '[Found] **Key columns in `mtr` output**:'",
      "AGENT: '  - `Loss%`: The percentage of packets lost at this hop. This is the most important column for this problem.'",
      "AGENT: '  - `Last`, `Avg`, `Best`, `Wrst`: Latency statistics in milliseconds.'",
      "AGENT: '  - `StDev`: Standard deviation of latency, high values indicate an unstable connection.'",
      "AGENT: '\u2713 Research complete! Let's install `mtr` and run a test.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Confirming the Problem with `ping` ---'",
      "AGENT: 'First, let's replicate the symptom you're seeing with `ping`.'",
      "Run command: `ping -c 10 example.com`",
      "AGENT: '...'",
      "AGENT: '--- example.com ping statistics ---'",
      "AGENT: '10 packets transmitted, 7 received, 30% packet loss, time 9026ms'",
      "AGENT: 'rtt min/avg/max/mdev = 12.123/12.456/12.789/0.234 ms'",
      "AGENT: '\ud83d\udca1 Yup, 30% packet loss is very bad. But is it your ISP, the server's provider, or somewhere in between? `ping` can't tell us.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Run `mtr` to Find the Source of the Loss ---'",
      "AGENT: 'Now, let's run `mtr`. The `-r` flag creates a report after 10 cycles, which is good for scripting. Running it without `-r` gives a live, updating view.'",
      "Run command: `sudo apt-get install mtr -y` (or `sudo pacman -S mtr`)",
      "Run command: `mtr -r -c 10 example.com`",
      "AGENT: 'Start: 2025-10-07T19:45:01-0400'",
      "AGENT: 'HOST: my-desktop                 Loss%   Snt   Last   Avg  Best  Wrst StDev'",
      "AGENT: '  1.|-- _gateway                   0.0%    10    0.3   0.4   0.3   0.5   0.1'",
      "AGENT: '  2.|-- my-isp-gateway.net         0.0%    10    5.5   5.6   5.4   5.9   0.2'",
      "AGENT: '  3.|-- some-isp-router.net       20.0%    10   10.1  10.2  10.0  10.5   0.2'",
      "AGENT: '  4.|-- another-router.com        20.0%    10   15.3  15.4  15.2  15.7   0.2'",
      "AGENT: '  5.|-- problematic-peer.net      80.0%    10   25.6  25.7  25.5  26.0   0.2'",
      "AGENT: '  6.|-- still-problematic.net     80.0%    10   25.9  26.0  25.8  26.3   0.2'",
      "AGENT: '  7.|-- final-provider.net         0.0%    10   30.1  30.2  30.0  30.5   0.2'",
      "AGENT: '  8.|-- example.com                0.0%    10   35.5  35.6  35.4  35.9   0.2'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Interpret the `mtr` Report ---'",
      "AGENT: 'This report tells a story. Let's read it:'",
      "AGENT: '  - **Hops 1 & 2**: Your local network and ISP gateway are fine (0.0% loss).'",
      "AGENT: '  - **Hops 3 & 4**: We see 20% loss starting at `some-isp-router.net`. This could be the problem, or it could be a router that is configured to deprioritize ICMP traffic (a common practice).'",
      "AGENT: '  - **Hops 5 & 6**: The loss jumps to **80%** at `problematic-peer.net` and *stays high* at the next hop. This is the **smoking gun**. When loss starts at a hop and persists through subsequent hops, that hop is very likely the source of the problem.'",
      "AGENT: '  - **Hops 7 & 8**: The loss drops back to 0.0% at the final provider and the destination. This is very unusual and likely means the return path is different, but the forward-path problem at hops 5 & 6 is clear.'",
      "AGENT: '\ud83d\udca1 **Conclusion**: The packet loss is occurring in the middle of the internet, at a peering point between two networks (`problematic-peer.net`). It is not an issue with your local connection or with the final destination server itself.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Confirmed the existence of packet loss with `ping`.'",
      "AGENT: '  \u2713 Used the superior `mtr` tool to send continuous probes across the entire network path.'",
      "AGENT: '  \u2713 Analyzed the `mtr` report, focusing on the `Loss%` column to identify where packet loss began and persisted.'",
      "AGENT: '  \u2713 Successfully pinpointed the problematic network segment to a specific router/provider in the middle of the path.'",
      "AGENT: '  \u2713 While you can't fix this router yourself, you now have concrete evidence to provide to your ISP or hosting provider's support team to get them to investigate the peering issue. \ud83d\udce1'"
    ],
    "commands": [
      "mtr",
      "ping",
      "traceroute",
      "apt-get"
    ],
    "packages": {
      "debian_ubuntu": [
        "mtr"
      ],
      "arch": [
        "mtr"
      ]
    },
    "related_commands": [
      "traceroute",
      "ping",
      "netstat",
      "ss"
    ],
    "complexity": "intermediate",
    "tags": [
      "networking",
      "troubleshooting",
      "mtr",
      "packet-loss",
      "latency",
      "performance"
    ]
  },
  {
    "id": "scenario_df_du_discrepancy_052",
    "command": "lsof",
    "scenario": "Figure out why `df` reports a disk is full but `du` shows free space.",
    "problem": "A user gets a 'No space left on device' error. The `df -h` command confirms the disk is 100% full. However, when they run `du -sh /` to sum up the size of all files, the total is significantly less than the disk capacity, leaving gigabytes of space seemingly unaccounted for.",
    "solution": "Diagnose this classic Linux issue by understanding that `du` measures file sizes, while `df` measures allocated blocks. The discrepancy is almost always caused by a process holding a large file open that has since been deleted. Use the `lsof` (list open files) command to find these deleted-but-still-open files and restart the responsible process to reclaim the space.",
    "steps": [
      "USER SAYS: 'This is driving me crazy! My server says the disk is 100% full with `df`, but when I check with `du -sh /var`, it only adds up to about 70% of the disk. Where did my 30GB of free space go?'",
      "AGENT: 'Ah, you've encountered one of the classic Linux mysteries! This almost always means a running process is holding onto a file that has been deleted. The file is gone from the directory listing so `du` can't see it, but its data blocks are still allocated on the disk until the process closes it, so `df` sees them as used. Let's hunt down that process.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: `df` vs `du` disk space discrepancy...'",
      "AGENT: '[Searching] \"df full but du shows space linux\"'",
      "AGENT: '[Found] **Root Cause**: Deleted files held open by a process.'",
      "AGENT: '[Found] **Why it happens**: A common scenario is a log file that is deleted (`rm some.log`) while a service is still actively writing to it. The process keeps its file handle, and the space is not freed until the handle is released.'",
      "AGENT: '[Found] **The Tool**: `lsof` (List Open Files) is the key to solving this. Specifically, `lsof | grep deleted`.'",
      "AGENT: '\u2713 Research complete! Let's use `lsof` to find the culprit.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Confirm the Discrepancy ---'",
      "AGENT: 'First, let's verify the numbers.'",
      "Run command: `df -h /`",
      "AGENT: 'Filesystem      Size  Used Avail Use% Mounted on'",
      "AGENT: '/dev/sda1       100G  100G     0 100% /'",
      "AGENT: ''",
      "AGENT: 'Now, check with `du` (this can take a moment on a large disk).'",
      "Run command: `sudo du -sh /`",
      "AGENT: '72G    /'",
      "AGENT: '\ud83d\udca1 Yep, there's a ~28GB discrepancy. The deleted file is probably that big.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Find the Open Deleted File with `lsof` ---'",
      "AGENT: 'Now for the magic. We'll run `lsof` and grep for the word \"deleted\". This will show us all processes that have an open file handle to a file that no longer exists in the filesystem.'",
      "Run command: `sudo lsof | grep '(deleted)'`",
      "AGENT: 'COMMAND     PID      USER   FD      TYPE             DEVICE  SIZE/OFF       NODE NAME'",
      "AGENT: 'java      12345     myapp  33w      REG              253,1  28991234567   1234567 /var/log/my-app.log (deleted)'",
      "AGENT: 'cron      54321      root    5u      REG                8,1         0          1156 /tmp/tmp.XXXXXXXXXXXXXXXXXXX (deleted)'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Analyze the Output and Resolve ---'",
      "AGENT: 'Let's break down that `lsof` output:'",
      "AGENT: '  - **`java 12345 myapp`**: A command named `java` with PID `12345`, running as user `myapp`.'",
      "AGENT: '  - **`FD 33w`**: It has file descriptor #33 open for writing (`w`).'",
      "AGENT: '  - **`SIZE/OFF 28991234567`**: This is the file's current size on disk, approximately 28.9 GB! This matches our missing space.'",
      "AGENT: '  - **`NAME /var/log/my-app.log (deleted)`**: The original name and path of the file was `/var/log/my-app.log`, and the OS has marked it as deleted.'",
      "AGENT: '\u2713 BINGO! We've found the offender. The `java` application (PID 12345) is holding a massive 28GB log file open that was deleted.'",
      "AGENT: ''",
      "AGENT: 'The solution is to get the process to release the file handle. The most straightforward way to do this is to restart the service.'",
      "Run command: `sudo systemctl restart my-app.service`",
      "AGENT: ''",
      "AGENT: '--- Verification ---'",
      "AGENT: 'Now that the service has been restarted, the old process is gone, and the kernel has freed the disk blocks. Let's check `df` again.'",
      "Run command: `df -h /`",
      "AGENT: 'Filesystem      Size  Used Avail Use% Mounted on'",
      "AGENT: '/dev/sda1       100G   72G   28G  72% /'",
      "AGENT: ''",
      "AGENT: '\u2713 Success! The 28GB of space has been reclaimed, and the usage reported by `df` now matches `du`.'",
      "AGENT: 'To prevent this in the future, use a proper log rotation tool like `logrotate`, which knows how to handle log files for running services without just deleting them (often using a `copytruncate` strategy).'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Understood the difference between `df` (block allocation) and `du` (file sizes) and why they might differ.'",
      "AGENT: '  \u2713 Identified the classic cause: a process holding an open file handle to a deleted file.'",
      "AGENT: '  \u2713 Used `sudo lsof | grep '(deleted)'` to pinpoint the exact process and the size of the phantom file.'",
      "AGENT: '  \u2713 Restarted the offending service to force it to release the file handle, which immediately freed the disk space.'",
      "AGENT: '  \u2713 Recommended using `logrotate` for future log management to prevent recurrence. \ud83d\udc7b\ud83d\udcbe'"
    ],
    "commands": [
      "df",
      "du",
      "lsof",
      "grep",
      "systemctl"
    ],
    "packages": {
      "debian_ubuntu": [
        "lsof",
        "coreutils"
      ],
      "arch": [
        "lsof",
        "coreutils"
      ]
    },
    "related_commands": [
      "fuser",
      "stat"
    ],
    "complexity": "intermediate",
    "tags": [
      "disk-space",
      "troubleshooting",
      "lsof",
      "df",
      "du",
      "deleted-files",
      "logrotate"
    ]
  },
  {
    "id": "scenario_setup_auditd_053",
    "command": "auditctl",
    "scenario": "Monitor critical files for changes using the Linux Audit Daemon (auditd).",
    "problem": "A security administrator needs to be alerted whenever critical system files, such as `/etc/passwd`, `/etc/shadow`, or `/etc/sudoers`, are read or modified. This is a common requirement for security compliance and intrusion detection.",
    "solution": "Use `auditd`, the Linux kernel's native auditing framework. Configure rules using `auditctl` to create a 'watch' on specific files and directories, logging any access or modification events for later review with `ausearch`.",
    "steps": [
      "USER SAYS: 'I need to keep a log of every time someone even looks at my `/etc/passwd` or `/etc/shadow` files. Is this possible?'",
      "AGENT: 'Yes, that is absolutely possible and a very good security practice. The standard tool for this level of monitoring on Linux is `auditd`, the Audit Daemon. It can hook directly into the kernel to watch for specific system calls related to files. Let me research the process for setting up file watches.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Using `auditd` to watch files...'",
      "AGENT: '[Searching] \"auditd monitor file changes example\"'",
      "AGENT: '[Found] `auditd` is the userspace daemon, and `auditctl` is the command-line tool to add/remove rules.'",
      "AGENT: '[Found] Rules can be added temporarily with `auditctl` or made permanent by adding them to `/etc/audit/rules.d/custom.rules`.'",
      "AGENT: '[Found] **Key `auditctl` syntax**:'",
      "AGENT: '  - `-w /path/to/file`: Sets a 'watch' on a file or directory.'",
      "AGENT: '  - `-p warx`: Specifies the permissions to watch for: `w` (write), `a` (attribute change), `r` (read), `x` (execute).'",
      "AGENT: '  - `-k key_name`: Assigns a custom key (tag) to the rule, making it easy to search for related events later.'",
      "AGENT: '[Found] `ausearch -k key_name`: The command to search the audit logs for events matching a specific key.'",
      "AGENT: '\u2713 Research complete! Let's install the tools and create some rules.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Install and Verify `auditd` ---'",
      "Run command: `sudo apt-get install auditd -y` (or `sudo pacman -S audit`)",
      "AGENT: 'Once installed, the service should be running.'",
      "Run command: `sudo systemctl status auditd`",
      "AGENT: '\u25cf auditd.service - Security Auditing Service... Active: active (running)'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Add Audit Rules ---'",
      "AGENT: 'Let's create a new rules file. It's best practice to put your custom rules in a separate file.'",
      "Run command: `sudo nano /etc/audit/rules.d/99-custom.rules`",
      "AGENT: 'Add the following lines to the file:'",
      "AGENT: '```'",
      "AGENT: '# Monitor critical identity and access files for read, write, or attribute changes.'",
      "AGENT: '-w /etc/passwd -p war -k identity_files'",
      "AGENT: '-w /etc/shadow -p war -k identity_files'",
      "AGENT: '-w /etc/group -p war -k identity_files'",
      "AGENT: '-w /etc/gshadow -p war -k identity_files'",
      "AGENT: '-w /etc/sudoers -p war -k sudoers_file'",
      "AGENT: '```'",
      "AGENT: 'Save and close the file. These rules will now be loaded automatically on reboot. To load them immediately, we need to restart the auditd service or use `augenrules`.'",
      "Run command: `sudo augenrules --load`",
      "AGENT: 'No rules'",
      "AGENT: 'enabled 1'",
      "AGENT: '... (rules are loaded) ...'",
      "AGENT: ''",
      "AGENT: 'Let's list the currently loaded rules to verify.'",
      "Run command: `sudo auditctl -l`",
      "AGENT: '-w /etc/passwd -p war -k identity_files'",
      "AGENT: '-w /etc/shadow -p war -k identity_files'",
      "AGENT: '-w /etc/group -p war -k identity_files'",
      "AGENT: '-w /etc/gshadow -p war -k identity_files'",
      "AGENT: '-w /etc/sudoers -p war -k sudoers_file'",
      "AGENT: '\u2713 Our rules are active!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Trigger and Search for an Event ---'",
      "AGENT: 'Now, let's trigger an event by simply reading the `/etc/passwd` file.'",
      "Run command: `cat /etc/passwd > /dev/null`",
      "AGENT: ''",
      "AGENT: 'The action has been logged. Let's find it using `ausearch` with the key we defined.'",
      "Run command: `sudo ausearch -k identity_files -i`",
      "AGENT: '----'",
      "AGENT: 'type=PROCTITLE msg=audit(1728345600.123:456): proctitle=636174002F6574632F706173737764'",
      "AGENT: 'type=PATH msg=audit(1728345600.123:456): item=0 name=/etc/passwd inode=12345 dev=fd:01 mode=0100644 ouid=0 ogid=0 rdev=00:00 obj=system_u:object_r:passwd_file_t:s0 cap_fp=0 cap_fe=0 cap_fver=0 cap_frootid=0'",
      "AGENT: 'type=CWD msg=audit(1728345600.123:456): cwd=/home/user'",
      "AGENT: 'type=SYSCALL msg=audit(1728345600.123:456): arch=c000003e syscall=257 success=yes exit=3 a0=ffffff9c a1=7ffc123abc a2=80000 a3=0 items=1 ppid=1234 pid=5678 auid=1000 uid=1000 gid=1000 euid=1000 suid=1000 fsuid=1000 egid=1000 sgid=1000 fsgid=1000 tty=pts0 ses=1 comm=\"cat\" exe=\"/usr/bin/cat\" subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=\"identity_files\"'",
      "AGENT: ''",
      "AGENT: '\ud83d\udca1 The log is extremely detailed! It shows the command (`cat`), the executable (`/usr/bin/cat`), the user ID (`uid=1000`), the file accessed (`/etc/passwd`), and our custom key (`identity_files`). It tells us exactly what happened.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched the Linux Audit Framework and the roles of `auditd`, `auditctl`, and `ausearch`.'",
      "AGENT: '  \u2713 Created a persistent custom rules file in `/etc/audit/rules.d/` to watch critical system files for read, write, and attribute change events.'",
      "AGENT: '  \u2713 Assigned a custom key (`-k`) to the rules to make searching for events simple and efficient.'",
      "AGENT: '  \u2713 Triggered an audit event and then used `ausearch` to find and interpret the detailed log entry.'",
      "AGENT: '  \u2713 You now have a robust, kernel-level file integrity monitoring system in place. \ud83d\udd75\ufe0f\u200d\u2642\ufe0f'"
    ],
    "commands": [
      "auditctl",
      "ausearch",
      "systemctl",
      "nano",
      "cat",
      "augenrules"
    ],
    "packages": {
      "debian_ubuntu": [
        "auditd"
      ],
      "arch": [
        "audit"
      ]
    },
    "related_commands": [
      "inotifywait",
      "aide",
      "tripwire",
      "ossec"
    ],
    "complexity": "intermediate",
    "tags": [
      "security",
      "auditd",
      "monitoring",
      "compliance",
      "file-integrity",
      "intrusion-detection"
    ]
  },
  {
    "id": "scenario_python_url_healthcheck_054",
    "command": "python",
    "scenario": "Create a script to periodically check the health of multiple web services.",
    "problem": "A DevOps engineer manages several web applications and APIs. They need a simple, automated way to check that each service is online and responding with a successful HTTP status code. They want a report of any failing services.",
    "solution": "Develop a Python script that reads a list of URLs from a configuration file. The script will iterate through the list, make an HTTP HEAD request to each URL to efficiently check its status, and print a color-coded report of which services are UP or DOWN.",
    "steps": [
      "USER SAYS: 'I have about 20 different web services I need to keep an eye on. Is there a simple script I can run to quickly check if they're all online?'",
      "AGENT: 'Absolutely. We can build a great little health checker tool in Python. It can read a list of your services from a config file and give you a quick pass/fail report. Let me research the most efficient way to check HTTP status.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Efficient Python HTTP status checking...'",
      "AGENT: '[Searching] \"python check if url is up requests\"'",
      "AGENT: '[Found] The `requests` library is perfect for this.'",
      "AGENT: '[Found] **Efficiency Tip**: Instead of a `GET` request, we can use a `HEAD` request (`requests.head()`). This asks the server for just the headers, not the full HTML body, making the check much faster and using less bandwidth.'",
      "AGENT: '[Found] **Configuration**: Reading the list of URLs from a separate file (`services.txt`) makes the script reusable and easier to manage.'",
      "AGENT: '[Found] The `rich` library can be used to make a nice, color-coded terminal output.'",
      "AGENT: '\u2713 Research complete! Our plan is to use `requests.head()` and a separate config file.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Create the Configuration File ---'",
      "AGENT: 'Let's create a simple text file with one URL per line. This will be our list of services to check.'",
      "Create tool: `cat > services.txt << 'EOF'`",
      "https://google.com\nhttps://github.com\nhttps://httpstat.us/503\nhttps://nonexistent-domain-12345.com\nhttps://api.github.com\nhttps://httpstat.us/404",
      "EOF",
      "AGENT: '\u2713 Our list of services is ready.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Build the Python Health Checker ---'",
      "AGENT: 'First, install the necessary libraries.'",
      "Run command: `pip install requests rich`",
      "Create tool: `cat > health_checker.py << 'EOF'`",
      "#!/usr/bin/env python3\n\n\"\"\"A tool to check the health of a list of web services.\"\"\"\n\nimport requests\nimport time\nfrom rich.console import Console\nfrom rich.table import Table\n\ndef check_services(file_path):\n    \"\"\"Reads URLs from a file and checks their HTTP status.\"\"\"\n    console = Console()\n    table = Table(title=f\"Web Service Health Check Report - {time.ctime()}\")\n    table.add_column(\"Status\", justify=\"center\", style=\"bold\")\n    table.add_column(\"Service URL\", style=\"cyan\")\n    table.add_column(\"Response Code\", justify=\"center\")\n    table.add_column(\"Response Time (ms)\", justify=\"right\")\n\n    try:\n        with open(file_path, 'r') as f:\n            urls = [line.strip() for line in f if line.strip()]\n    except FileNotFoundError:\n        console.print(f\"[bold red]Error: Configuration file '{file_path}' not found.[/bold red]\")\n        return\n\n    console.print(f\"Checking {len(urls)} services...\")\n    for url in urls:\n        status_text, status_style, response_code, response_time = check_url(url)\n        table.add_row(f\"[{status_style}]{status_text}[/{status_style}]\", url, str(response_code), f\"{response_time:.0f}\")\n\n    console.print(table)\n\ndef check_url(url):\n    \"\"\"Performs a single HEAD request to a URL and returns its status.\"\"\"\n    try:\n        # Use a HEAD request for efficiency\n        response = requests.head(url, timeout=5, allow_redirects=True, headers={'User-Agent': 'HealthChecker/1.0'})\n        response_time = response.elapsed.total_seconds() * 1000\n        response_code = response.status_code\n\n        if 200 <= response_code < 400:\n            return \"\u2705 UP\", \"green\", response_code, response_time\n        else:\n            return \"\ud83d\udd25 DOWN\", \"red\", response_code, response_time\n\n    except requests.exceptions.Timeout:\n        return \"\ud83d\udd25 DOWN\", \"red\", \"Timeout\", -1\n    except requests.exceptions.ConnectionError:\n        return \"\ud83d\udd25 DOWN\", \"red\", \"Connection Error\", -1\n    except Exception:\n        return \"\ud83d\udd25 DOWN\", \"red\", \"Unknown Error\", -1\n\nif __name__ == '__main__':\n    check_services('services.txt')\n",
      "EOF",
      "chmod +x health_checker.py",
      "AGENT: '\u2713 Created the health checker script!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Run the Health Check ---'",
      "Run command: `./health_checker.py`",
      "AGENT: 'Checking 6 services...'",
      "AGENT: '[Simulated Rich Table Output]'",
      "AGENT: '                     Web Service Health Check Report - Tue Oct  7 19:50:00 2025                     '",
      "AGENT: '\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513'",
      "AGENT: '\u2503 Status   \u2503 Service URL                           \u2503 Response Code \u2503 Response Time (ms) \u2503'",
      "AGENT: '\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529'",
      "AGENT: '\u2502 \u2705 UP    \u2502 https://google.com                    \u2502      200      \u2502                 55 \u2502'",
      "AGENT: '\u2502 \u2705 UP    \u2502 https://github.com                    \u2502      200      \u2502                120 \u2502'",
      "AGENT: '\u2502 \ud83d\udd25 DOWN  \u2502 https://httpstat.us/503               \u2502      503      \u2502                 88 \u2502'",
      "AGENT: '\u2502 \ud83d\udd25 DOWN  \u2502 https://nonexistent-domain-12345.com  \u2502 Connection Er \u2502                 -1 \u2502'",
      "AGENT: '\u2502 \u2705 UP    \u2502 https://api.github.com                \u2502      200      \u2502                 95 \u2502'",
      "AGENT: '\u2502 \ud83d\udd25 DOWN  \u2502 https://httpstat.us/404               \u2502      404      \u2502                 85 \u2502'",
      "AGENT: '\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched the use of `requests.head()` for efficient, low-bandwidth HTTP status checks.'",
      "AGENT: '  \u2713 Decoupled the script logic from the list of services by reading URLs from a `services.txt` file.'",
      "AGENT: '  \u2713 Built a Python script that robustly handles various outcomes, including successful responses, HTTP error codes, and network errors like timeouts.'",
      "AGENT: '  \u2713 Used the `rich` library to present the results in a clear, color-coded table that immediately highlights which services are down.'",
      "AGENT: '  \u2713 You now have a simple, fast, and reusable tool for monitoring your application health. You could even run this script on a cron job to get periodic reports. \u2764\ufe0f\u200d\ud83e\ude79'"
    ],
    "commands": [
      "python3",
      "pip",
      "cat",
      "chmod"
    ],
    "packages": {
      "debian_ubuntu": [
        "python3-pip",
        "python3-requests"
      ],
      "arch": [
        "python-pip",
        "python-requests"
      ]
    },
    "related_commands": [
      "curl",
      "wget",
      "nagios",
      "prometheus"
    ],
    "complexity": "intermediate",
    "tags": [
      "python",
      "monitoring",
      "automation",
      "requests",
      "health-check",
      "devops",
      "custom-tool"
    ]
  },
  {
    "id": "scenario_strace_debug_app_059",
    "command": "strace",
    "scenario": "Debug an application that is failing to start with a cryptic error.",
    "problem": "A compiled application or a third-party binary fails to run, giving a vague error like 'Configuration error' or 'Initialization failed' with no further details in its logs. The user has no access to the source code and needs to understand what the application is trying to do.",
    "solution": "Use `strace`, a powerful diagnostic tool that intercepts and records all the system calls a process makes. By analyzing the `strace` output, you can see exactly which files the application is trying to open, which network connections it's attempting, and where the operation is failing (e.g., with an `ENOENT` 'No such file or directory' error).",
    "steps": [
      "USER SAYS: 'I'm trying to run this third-party server application `./cool-server`, but it just exits immediately and says `Error: could not load config`. The documentation says it should look for `config.yaml`, and I have that file in the same directory! I'm stuck.'",
      "AGENT: 'This is a perfect scenario for `strace`. When an application's own error messages are not helpful, `strace` lets us see what's happening 'under the hood' by showing us all the system calls it's making to the kernel. We can see exactly what files it's trying to open. Let me research the best `strace` flags for this.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Using `strace` to debug file access issues...'",
      "AGENT: '[Searching] \"strace find what file an application is trying to open\"'",
      "AGENT: '[Found] `strace <command>` is the basic usage.'",
      "AGENT: '[Found] `-f`: Follow forks, which is useful for complex applications that spawn child processes.'",
      "AGENT: '[Found] `-e trace=open,openat`: A key optimization. This tells `strace` to *only* show us the system calls related to opening files (`open` and `openat`), which filters out a lot of noise.'",
      "AGENT: '[Found] The error code `ENOENT` in the output is the C library's 'Error NO ENTity' and almost always means 'No such file or directory'.'",
      "AGENT: '\u2713 Research complete! We'll use `strace -e trace=open,openat` to find the problem.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Run the Failing Application ---'",
      "AGENT: 'First, let's confirm the application's own error message.'",
      "Run command: `./cool-server`",
      "AGENT: 'Error: could not load config'",
      "AGENT: 'Yes, that's not very helpful. It doesn't tell us *where* it was looking.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Run the Application Under `strace` ---'",
      "AGENT: 'Now, let's prepend the command with `strace`, filtering for just the `open` and `openat` syscalls. The output of `strace` goes to stderr.'",
      "Run command: `strace -e trace=open,openat ./cool-server`",
      "AGENT: 'openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3'",
      "AGENT: 'openat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3'",
      "AGENT: '... (a few more successful library opens) ...'",
      "AGENT: 'openat(AT_FDCWD, \"/etc/cool-server/config.yaml\", O_RDONLY) = -1 ENOENT (No such file or directory)'",
      "AGENT: 'openat(AT_FDCWD, \"/opt/cool-server/config.yaml\", O_RDONLY) = -1 ENOENT (No such file or directory)'",
      "AGENT: 'Error: could not load config'",
      "AGENT: '+++ exited with 1 +++'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Analyze the `strace` Output ---'",
      "AGENT: 'The output is a goldmine! Let's interpret it:'",
      "AGENT: '  - `openat(...) = 3`: These are successful calls. The application is opening system libraries, and the kernel is returning a valid file descriptor (e.g., `3`).'",
      "AGENT: '  - **`openat(..., \"/etc/cool-server/config.yaml\", ...)`**: This is the application trying to open a config file at `/etc/cool-server/config.yaml`.'",
      "AGENT: '  - **`= -1 ENOENT (No such file or directory)`**: THIS IS THE SMOKING GUN. The system call failed. The kernel returned `-1` and set the error code to `ENOENT`.'",
      "AGENT: '  - The application then tries another location, `/opt/cool-server/config.yaml`, which also fails. After failing to find the file in any of its hardcoded paths, it gives up and prints its generic error message.'",
      "AGENT: '\ud83d\udca1 **Conclusion**: The documentation was misleading or incomplete. The application does *not* look for `config.yaml` in the current directory. It looks for it in `/etc/cool-server/` or `/opt/cool-server/`.'",
      "AGENT: ''",
      "AGENT: '--- Solution ---'",
      "AGENT: 'The fix is to move your config file to one of the locations the application expects.'",
      "Run command: `sudo mkdir -p /etc/cool-server`",
      "Run command: `sudo mv ./config.yaml /etc/cool-server/config.yaml`",
      "AGENT: ''",
      "AGENT: 'Now, try running the application again.'",
      "Run command: `./cool-server`",
      "AGENT: 'Config loaded successfully from /etc/cool-server/config.yaml'",
      "AGENT: 'Server starting on port 8080...'",
      "AGENT: ''",
      "AGENT: '\u2713 It works! `strace` allowed us to bypass the unhelpful error message and see the true cause of the failure.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Identified `strace` as the correct tool for debugging opaque application errors by observing system calls.'",
      "AGENT: '  \u2713 Used the `-e trace=open,openat` filter to dramatically reduce the noise and focus only on file-opening operations.'",
      "AGENT: '  \u2713 Analyzed the `strace` output, specifically looking for failed `openat` calls that returned `-1 ENOENT`.'",
      "AGENT: '  \u2713 Discovered the application's hardcoded, undocumented search paths for its configuration file.'",
      "AGENT: '  \u2713 Resolved the issue by moving the configuration file to a location the application was actually searching. \ud83d\udd75\ufe0f\u200d\u2640\ufe0f'"
    ],
    "commands": [
      "strace",
      "mkdir",
      "mv"
    ],
    "packages": {
      "debian_ubuntu": [
        "strace"
      ],
      "arch": [
        "strace"
      ]
    },
    "related_commands": [
      "ltrace",
      "lsof",
      "perf",
      "gdb"
    ],
    "complexity": "advanced",
    "tags": [
      "debugging",
      "troubleshooting",
      "strace",
      "system-calls",
      "binaries",
      "advanced"
    ]
  },
  {
    "id": "scenario_gdb_debug_crash_060",
    "command": "gdb",
    "scenario": "Debug a C++ application that is crashing with a segmentation fault.",
    "problem": "A user is running a custom C++ application that abruptly terminates with the message `Segmentation fault (core dumped)`. They need to find the exact line of code that is causing the crash, but the application doesn't produce any useful logs before it dies.",
    "solution": "Enable core dump generation, then use the GNU Debugger (`gdb`) to load the executable and the generated core dump file. `gdb` can then unwind the stack and provide a full backtrace, showing the exact function and line number where the fault occurred.",
    "steps": [
      "USER SAYS: 'My C++ program crashes with a `Segmentation fault`! How can I possibly find the bug without adding a million print statements?'",
      "AGENT: 'That's what a debugger is for! The `Segmentation fault (core dumped)` message is actually very helpful. That 'core dump' is a file containing a snapshot of the program's memory at the moment of the crash. We can analyze it with `gdb` to find the exact location of the error. Let's research the process.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Debugging core dumps with gdb...'",
      "AGENT: '[Searching] \"gdb analyze core dump segmentation fault\"'",
      "AGENT: '[Found] **Prerequisite 1**: The application must be compiled with debug symbols (`-g` flag in `g++`/`gcc`). Without this, `gdb` can't map memory addresses back to source code lines.'",
      "AGENT: '[Found] **Prerequisite 2**: The system must be configured to allow core dumps. `ulimit -c unlimited` enables this for the current session.'",
      "AGENT: '[Found] **The Command**: `gdb <executable_path> <core_dump_path>`.'",
      "AGENT: '[Found] **Key `gdb` commands**: `bt` (backtrace) is the most important command to show the call stack at the time of the crash.'",
      "AGENT: '\u2713 Research complete! Let's debug this crash.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: The Crashing Code and Compilation ---'",
      "AGENT: 'Let's imagine you have this simple C++ program with a null pointer dereference.'",
      "Create tool: `cat > crash.cpp << EOF`",
      "#include <iostream>\n\nvoid cause_crash() {\n    char *p = nullptr;\n    std::cout << \"About to crash...\\n\";\n    *p = 'A'; // This will cause a segmentation fault\n    std::cout << \"This will never be printed.\\n\";\n}\n\nint main() {\n    cause_crash();\n    return 0;\n}",
      "EOF",
      "AGENT: 'Now, compile it with the `-g` flag to include debug symbols.'",
      "Run command: `g++ -g -o crash_program crash.cpp`",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Enable Core Dumps and Trigger the Crash ---'",
      "AGENT: 'Most systems have core dump generation disabled by default. We'll enable it for our shell session.'",
      "Run command: `ulimit -c unlimited`",
      "AGENT: ''",
      "AGENT: 'Now, run the program to trigger the crash and generate the core file.'",
      "Run command: `./crash_program`",
      "AGENT: 'About to crash...'",
      "AGENT: 'Segmentation fault (core dumped)'",
      "AGENT: ''",
      "AGENT: 'Let's see the generated core file. The name can vary by system configuration.'",
      "Run command: `ls -l core*`",
      "AGENT: '-rw------- 1 user user 245760 Oct 7 19:55 core'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Analyze the Core Dump with `gdb` ---'",
      "AGENT: 'Now we load the executable and the core file into `gdb`.'",
      "Run command: `gdb ./crash_program core`",
      "AGENT: 'GNU gdb (Ubuntu 12.1-0ubuntu1~22.04) 12.1'",
      "AGENT: '... (gdb startup messages) ...'",
      "AGENT: 'Core was generated by `./crash_program`.'",
      "AGENT: 'Program terminated with signal SIGSEGV, Segmentation fault.'",
      "AGENT: '#0  0x000055555555518a in cause_crash () at crash.cpp:6'",
      "AGENT: '6        *p = 'A'; // This will cause a segmentation fault'",
      "AGENT: '(gdb) '",
      "AGENT: ''",
      "AGENT: '\ud83d\udca1 `gdb` has already taken us to the exact line of the crash! The message `#0 ... at crash.cpp:6` is the top of the call stack. Let's use the `backtrace` (or `bt`) command to see the full stack.'",
      "Run command in gdb: `bt`",
      "AGENT: '#0  0x000055555555518a in cause_crash () at crash.cpp:6'",
      "AGENT: '#1  0x00005555555551a9 in main () at crash.cpp:11'",
      "AGENT: '(gdb) '",
      "AGENT: 'The backtrace shows that `main()` (at line 11) called `cause_crash()` (at line 6), which then faulted. We can even inspect the values of variables at that point in time.'",
      "Run command in gdb: `info locals`",
      "AGENT: 'p = 0x0'",
      "AGENT: '(gdb) '",
      "AGENT: 'This confirms that the pointer `p` was null (`0x0`) when the program tried to write to it, which is the definition of a segmentation fault. To exit `gdb`, type `quit`.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Ensured the application was compiled with debug symbols (`-g`).'",
      "AGENT: '  \u2713 Used `ulimit -c unlimited` to enable the creation of a core dump file upon crashing.'",
      "AGENT: '  \u2713 Loaded the application binary and the resulting `core` file into the GNU Debugger (`gdb`).'",
      "AGENT: '  \u2713 Used the `bt` (backtrace) command within `gdb` to get a full stack trace, which pinpointed the exact file and line number (`crash.cpp:6`) causing the segmentation fault.'",
      "AGENT: '  \u2713 You now have a powerful, systematic process for finding the root cause of even the most severe application crashes. \ud83d\udc1e'"
    ],
    "commands": [
      "gdb",
      "ulimit",
      "g++",
      "ls",
      "bt"
    ],
    "packages": {
      "debian_ubuntu": [
        "gdb",
        "build-essential"
      ],
      "arch": [
        "gdb",
        "base-devel"
      ]
    },
    "related_commands": [
      "valgrind",
      "rr",
      "lldb"
    ],
    "complexity": "advanced",
    "tags": [
      "debugging",
      "gdb",
      "c++",
      "segmentation-fault",
      "core-dump",
      "troubleshooting",
      "advanced"
    ]
  },
  {
    "id": "scenario_organize_files_python_021",
    "command": "python",
    "scenario": "A user's 'Downloads' folder is cluttered and needs to be organized by file type.",
    "problem": "The user has hundreds of files of various types (images, documents, archives) mixed together in one directory. Manually sorting them into subfolders like 'Images', 'Documents', etc., would be extremely time-consuming.",
    "solution": "Create a Python script that scans the directory, determines each file's type based on its extension, and moves it into a corresponding subfolder, creating the folder if it doesn't exist.",
    "steps": [
      "USER SAYS: 'My Downloads folder is a complete mess. Can I write a script to automatically put all my PDFs in a 'Documents' folder, JPGs in an 'Images' folder, and so on?'",
      "AGENT: 'That's a fantastic and very practical automation task! A Python script is perfect for this. Let me do a quick research on the best Python libraries for file system operations.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Python file system manipulation...'",
      "AGENT: '[Searching] \"python move file to new directory\"'",
      "AGENT: '[Found] `pathlib` is the modern, object-oriented way to handle filesystem paths. It's much cleaner than `os.path`.'",
      "AGENT: '[Found] Key functions: `Path.iterdir()` to list files, `Path.is_file()`, `Path.suffix` to get the extension, `Path.mkdir(exist_ok=True)` to create directories, and `Path.rename()` to move files.'",
      "AGENT: '\u2713 Research complete! We'll build a script using the `pathlib` module.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Manual Drag-and-Drop (The Problem) ---'",
      "AGENT: 'Manually selecting files, creating folders, and moving them is the baseline. It is slow, boring, and error-prone. We can automate this entire process.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Build the Python File Organizer ---'",
      "AGENT: 'Let's write a script that can be run on any directory to sort it.'",
      "Create tool: `cat > file_organizer.py << 'EOF'`",
      "#!/usr/bin/env python3\n\n\"\"\"Organizes files in a directory into subfolders based on file extension.\"\"\"\n\nimport argparse\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef organize_directory(target_path):\n    \"\"\"Scans a directory and organizes its files.\"\"\"\n    # Define mappings from extension to folder name\n    # Using a defaultdict to handle unknown file types gracefully\n    folder_map = defaultdict(lambda: 'Other')\n    folder_map.update({\n        # Documents\n        '.pdf': 'Documents',\n        '.docx': 'Documents',\n        '.doc': 'Documents',\n        '.txt': 'Documents',\n        '.pptx': 'Documents',\n        '.xlsx': 'Documents',\n        # Images\n        '.jpg': 'Images',\n        '.jpeg': 'Images',\n        '.png': 'Images',\n        '.gif': 'Images',\n        '.svg': 'Images',\n        # Archives\n        '.zip': 'Archives',\n        '.gz': 'Archives',\n        '.tar': 'Archives',\n        # Code & Scripts\n        '.py': 'Code',\n        '.sh': 'Code',\n        '.js': 'Code',\n        # Media\n        '.mp3': 'Audio',\n        '.mp4': 'Video',\n    })\n\n    target_dir = Path(target_path)\n    if not target_dir.is_dir():\n        print(f\"Error: '{target_path}' is not a valid directory.\")\n        return\n\n    print(f\"Scanning '{target_dir.resolve()}'...\")\n    moved_count = 0\n    for file_path in target_dir.iterdir():\n        # Skip directories and the script itself\n        if not file_path.is_file() or file_path.name == 'file_organizer.py':\n            continue\n\n        # Get the destination folder name from our map\n        file_extension = file_path.suffix.lower()\n        dest_folder_name = folder_map[file_extension]\n        dest_dir = target_dir / dest_folder_name\n\n        # Create the destination folder if it doesn't exist\n        dest_dir.mkdir(exist_ok=True)\n\n        # Move the file\n        new_file_path = dest_dir / file_path.name\n        print(f\"Moving '{file_path.name}' -> '{dest_folder_name}/'\")\n        file_path.rename(new_file_path)\n        moved_count += 1\n\n    print(f\"\\n\u2728 Organization complete! Moved {moved_count} files.\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Organize files in a directory.')\n    parser.add_argument('directory', help='The directory to organize.')\n    args = parser.parse_args()\n    organize_directory(args.directory)\n",
      "EOF",
      "chmod +x file_organizer.py",
      "AGENT: '\u2713 Created the file organizer script!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Run the Script ---'",
      "AGENT: 'Let's assume your Downloads folder has these files:'",
      "Run command: `ls ~/Downloads`",
      "AGENT: 'cat_photo.jpg  invoice.pdf  presentation.pptx  archive.zip  unknown.xyz'",
      "AGENT: ''",
      "AGENT: 'Now, run the script on that directory.'",
      "Run command: `python3 file_organizer.py ~/Downloads`",
      "AGENT: 'Scanning '/home/user/Downloads'...' 'Moving 'archive.zip' -> 'Archives/' 'Moving 'cat_photo.jpg' -> 'Images/' 'Moving 'invoice.pdf' -> 'Documents/' 'Moving 'presentation.pptx' -> 'Documents/' 'Moving 'unknown.xyz' -> 'Other/'",
      "AGENT: ''",
      "AGENT: '\u2728 Organization complete! Moved 5 files.'",
      "AGENT: ''",
      "AGENT: 'Let's check the result:'",
      "Run command: `ls -R ~/Downloads`",
      "AGENT: '/home/user/Downloads:'",
      "AGENT: 'Archives  Documents  Images  Other'",
      "AGENT: ''",
      "AGENT: '/home/user/Downloads/Archives:'",
      "AGENT: 'archive.zip'",
      "AGENT: ''",
      "AGENT: '/home/user/Downloads/Documents:'",
      "AGENT: 'invoice.pdf  presentation.pptx'",
      "AGENT: ''",
      "AGENT: '/home/user/Downloads/Images:'",
      "AGENT: 'cat_photo.jpg'",
      "AGENT: ''",
      "AGENT: '/home/user/Downloads/Other:'",
      "AGENT: 'unknown.xyz'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched and chose the modern `pathlib` library for file operations.'",
      "AGENT: '  \u2713 Built a reusable Python script that sorts files into subdirectories based on a configurable mapping of extensions.'",
      "AGENT: '  \u2713 The script automatically creates destination folders and handles unknown file types by placing them in an 'Other' directory.'",
      "AGENT: '  \u2713 Successfully organized the cluttered directory, making it clean and manageable. \ud83e\uddf9'"
    ],
    "commands": [
      "python3",
      "ls",
      "chmod",
      "cat"
    ],
    "packages": {
      "debian_ubuntu": [
        "python3"
      ],
      "arch": [
        "python"
      ]
    },
    "related_commands": [
      "mv",
      "find",
      "mkdir"
    ],
    "complexity": "beginner",
    "tags": [
      "python",
      "automation",
      "file-management",
      "pathlib",
      "beginner",
      "custom-tool"
    ]
  },
  {
    "id": "scenario_high_io_wait_030",
    "command": "iotop",
    "scenario": "Diagnose the cause of high I/O wait on a Linux server.",
    "problem": "A server is very sluggish and unresponsive. The `top` command shows a high `%wa` (I/O Wait) value, indicating that the CPU is idle but waiting for disk operations to complete. The user needs to find which process is causing the heavy disk I/O.",
    "solution": "Use specialized I/O monitoring tools like `iostat` to confirm the problem device and `iotop` to pinpoint the exact process that is causing the high disk read/write activity.",
    "steps": [
      "USER SAYS: 'My application is running extremely slow, but `top` shows CPU usage is low. The `%wa` value is really high, like 70%. What does that mean?'",
      "AGENT: 'A high `%wa` or I/O Wait value means your processes are bottlenecked on disk access. The CPU is ready to work but is stuck waiting for data to be read from or written to the disk. We need to find the process responsible for this. Let me research the right tools for the job.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Linux I/O wait troubleshooting...'",
      "AGENT: '[Searching] \"linux high io wait find process\"'",
      "AGENT: '[Found] `top`/`htop`: Good for identifying the `%wa` symptom.'",
      "AGENT: '[Found] `iostat`: Provides detailed statistics per disk device. Helps confirm which disk is being thrashed.'",
      "AGENT: '[Found] `iotop`: The 'top' for I/O. Shows a live view of disk I/O usage, sorted by process. This is the key tool.'",
      "AGENT: '\u2713 Research complete! Our plan is to use `iostat` to identify the device and `iotop` to find the process.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Confirm with `top` ---'",
      "AGENT: 'First, let's look at `top` again to confirm the symptom.'",
      "Run command: `top -bn1 | head -n 3`",
      "AGENT: 'top - 20:25:01 up 10 days,  4:15,  1 user,  load average: 5.10, 3.50, 2.10'",
      "AGENT: 'Tasks: 123 total,   1 running, 122 sleeping,   0 stopped,   0 zombie'",
      "AGENT: '%Cpu(s):  5.5 us,  3.1 sy,  0.0 ni, 21.0 id, 70.2 wa,  0.0 hi,  0.2 si,  0.0 st'",
      "AGENT: '\ud83d\udca1 Yes, `70.2 wa` confirms the CPU is spending most of its time waiting for I/O.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Identify the Disk with `iostat` ---'",
      "AGENT: 'Now let's see which disk device is under heavy load. `iostat -d -x 2 5` will show extended stats for all devices, every 2 seconds, 5 times.'",
      "Run command: `iostat -d -x 2 5`",
      "AGENT: 'Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz %util'",
      "AGENT: 'sda              0.50    1.00      4.00      8.00     0.00     0.00   0.00   0.00    1.50    2.00   0.01   0.20'",
      "AGENT: 'sdb             50.00  1500.00   2000.00 60000.00     0.00     0.00   0.00   0.00   25.00   50.00  50.00  99.80'",
      "AGENT: ''",
      "AGENT: 'The output is very clear. The `%util` for device `sdb` is at `99.80%`. This disk is completely saturated. It's performing 1500 writes per second (`w/s`).'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Pinpoint the Process with `iotop` ---'",
      "AGENT: '`iotop` will show us exactly which process is writing to `sdb`. It requires root privileges.'",
      "Run command: `sudo apt-get install iotop -y` (or `dnf install iotop`)",
      "Run command: `sudo iotop -o -b -n 1`",
      "AGENT: 'Total DISK READ: 1.95 M/s | Total DISK WRITE: 58.59 M/s'",
      "AGENT: '  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO>    COMMAND'",
      "AGENT: '18345 be/4 mysql      0.00 B/s   55.12 M/s  0.00 % 95.32 % mysqld --defaults-file=/etc/mysql/my.cnf'",
      "AGENT: '  432 be/4 root       0.00 B/s    3.47 M/s  0.00 %  2.15 % [jbd2/sdb1-8]'",
      "AGENT: ' 2109 be/4 root       1.95 M/s    0.00 B/s  0.00 %  0.00 % rsyslogd -n'",
      "AGENT: ''",
      "AGENT: '\u2713 BINGO! The `mysqld` process (PID 18345) is responsible for almost all of the disk write activity (55 MB/s) and is spending >95% of its time waiting on I/O. The journaling process `jbd2` is also active because of the heavy writes.'",
      "AGENT: ''",
      "AGENT: '--- Solution and Next Steps ---'",
      "AGENT: 'We have successfully identified the source of the I/O bottleneck. The problem is with the MySQL database. The next steps are to investigate *why* MySQL is writing so much:'",
      "AGENT: '1.  **Check Slow Query Log**: Enable and check the MySQL slow query log to find inefficient queries that might be creating large temporary tables on disk.'",
      "AGENT: '2.  **Analyze `SHOW PROCESSLIST`**: See what queries are currently running.'",
      "AGENT: '3.  **Tune `my.cnf`**: Review the MySQL configuration for things like `innodb_buffer_pool_size`. If it's too small, the database will constantly have to read from and write to disk instead of using RAM.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Confirmed high I/O wait using `top`.'",
      "AGENT: '  \u2713 Used `iostat` to isolate the high activity to a specific disk device (`sdb`).'",
      "AGENT: '  \u2713 Used `iotop` to conclusively identify the `mysqld` process as the source of the excessive disk writes.'",
      "AGENT: '  \u2713 Provided a clear path for the next phase of debugging, focusing on database-level tuning and query optimization.  disque-o \ud83c\udf89'"
    ],
    "commands": [
      "top",
      "iostat",
      "iotop",
      "apt-get"
    ],
    "packages": {
      "debian_ubuntu": [
        "procps",
        "sysstat",
        "iotop"
      ],
      "arch": [
        "procps-ng",
        "sysstat",
        "iotop"
      ]
    },
    "related_commands": [
      "vmstat",
      "dstat",
      "blktrace",
      "lsof"
    ],
    "complexity": "intermediate",
    "tags": [
      "performance",
      "troubleshooting",
      "io-wait",
      "iotop",
      "iostat",
      "disk",
      "database",
      "mysql"
    ]
  },
  {
    "id": "scenario_docker_build_slow_028",
    "command": "docker",
    "scenario": "Troubleshoot and optimize a slow Docker image build.",
    "problem": "A developer's `docker build` command takes a very long time to complete, especially on subsequent runs when only small code changes were made. This slows down their development and CI/CD pipeline.",
    "solution": "Analyze the Dockerfile and apply best practices to leverage Docker's layer caching, such as ordering instructions from least to most frequently changed, combining `RUN` commands, and using a multi-stage build to create a smaller, more efficient final image.",
    "steps": [
      "USER SAYS: 'My `docker build` takes forever, even when I only change one line of code. How can I speed it up?'",
      "AGENT: 'That's a classic Docker performance issue, and it's almost always related to how Docker's layer cache is being used. A well-structured Dockerfile can be dramatically faster. I'll research the key optimization techniques.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Optimizing Dockerfile build speed...'",
      "AGENT: '[Searching] \"docker build slow optimize cache\"'",
      "AGENT: '[Found] **Layer Caching**: Each instruction in a Dockerfile creates a layer. Docker reuses layers from the cache if the instruction and the files it depends on haven't changed.'",
      "AGENT: '[Found] **Instruction Ordering**: Place instructions that change less frequently (like installing system packages) *before* instructions that change often (like copying source code).'",
      "AGENT: '[Found] **Combining `RUN` commands**: Use `&&` to chain `RUN` commands, reducing the number of layers created.'",
      "AGENT: '[Found] **Multi-stage Builds**: Use one stage for building/compiling and a final, clean stage for the runtime, resulting in smaller and more secure images.'",
      "AGENT: '\u2713 Research complete! Let's analyze and refactor your Dockerfile.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Analyze the Inefficient Dockerfile ---'",
      "AGENT: 'Let's look at a typical inefficient Dockerfile for a Python application:'",
      "AGENT: '```Dockerfile'",
      "AGENT: '# Dockerfile.bad'",
      "AGENT: 'FROM python:3.9-slim'",
      "AGENT: ''",
      "AGENT: 'WORKDIR /app'",
      "AGENT: ''",
      "AGENT: '# 1. Copies all code first. This breaks the cache on EVERY code change.'",
      "AGENT: 'COPY . .'",
      "AGENT: ''",
      "AGENT: '# 2. Installs dependencies after copying code. This runs every time.'",
      "AGENT: 'RUN pip install --no-cache-dir -r requirements.txt'",
      "AGENT: ''",
      "AGENT: 'CMD [\"python\", \"main.py\"]'",
      "AGENT: '```'",
      "AGENT: 'The problem here is that the `COPY . .` instruction is near the top. Any change to any file (even a README) will invalidate the cache for this layer *and all subsequent layers*. This means `pip install` runs over and over, which is very slow.'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Refactor for Optimal Caching ---'",
      "AGENT: 'Let's apply the best practices. We'll copy only the `requirements.txt` file first, install dependencies (which will now be cached), and *then* copy the rest of the source code.'",
      "AGENT: '```Dockerfile'",
      "AGENT: '# Dockerfile.good'",
      "AGENT: 'FROM python:3.9-slim'",
      "AGENT: ''",
      "AGENT: 'WORKDIR /app'",
      "AGENT: ''",
      "AGENT: '# 1. Copy only the requirements file.'",
      "AGENT: 'COPY requirements.txt .'",
      "AGENT: ''",
      "AGENT: '# 2. Install dependencies. This layer will be cached as long as'",
      "AGENT: '#    requirements.txt does not change.'",
      "AGENT: 'RUN pip install --no-cache-dir -r requirements.txt'",
      "AGENT: ''",
      "AGENT: '# 3. Now copy the rest of the application code. This is the layer'",
      "AGENT: '#    that will change most frequently.'",
      "AGENT: 'COPY . .'",
      "AGENT: ''",
      "AGENT: 'CMD [\"python\", \"main.py\"]'",
      "AGENT: '```'",
      "AGENT: 'With this change, `pip install` will only re-run if you explicitly change the `requirements.txt` file. Changes to your Python code will only invalidate the final `COPY` layer, making subsequent builds nearly instantaneous.'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Advanced Optimization with Multi-stage Builds ---'",
      "AGENT: 'For compiled languages or projects with build-time dependencies, a multi-stage build is even better. It keeps the final image lean.'",
      "AGENT: '```Dockerfile'",
      "AGENT: '# Dockerfile. multistage (e.g., for a Node.js app)'",
      "AGENT: '# --- Build Stage ---'",
      "AGENT: 'FROM node:18 AS builder'",
      "AGENT: 'WORKDIR /app'",
      "AGENT: 'COPY package*.json ./'",
      "AGENT: 'RUN npm install'",
      "AGENT: 'COPY . .'",
      "AGENT: 'RUN npm run build'",
      "AGENT: ''",
      "AGENT: '# --- Final Stage ---'",
      "AGENT: 'FROM nginx:1.23-alpine'",
      "AGENT: '# Copy only the built assets from the 'builder' stage'",
      "AGENT: 'COPY --from=builder /app/build /usr/share/nginx/html'",
      "AGENT: 'EXPOSE 80'",
      "AGENT: 'CMD [\"nginx\", \"-g\", \"daemon off;\"]'",
      "AGENT: '```'",
      "AGENT: 'Here, the final image only contains the Nginx server and the static HTML/JS files. It doesn't contain the entire Node.js runtime or any of the source code, making it smaller, faster to pull, and more secure.'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched and explained Docker's layer caching mechanism.'",
      "AGENT: '  \u2713 Identified the primary performance issue in the original Dockerfile: incorrect instruction ordering.'",
      "AGENT: '  \u2713 Refactored the Dockerfile to copy dependencies first (`requirements.txt`) before copying the application source code, maximizing cache hits for faster subsequent builds.'",
      "AGENT: '  \u2713 Introduced the concept of multi-stage builds as an advanced technique for creating smaller and more efficient production images. \ud83d\ude80'"
    ],
    "commands": [
      "docker",
      "pip"
    ],
    "packages": {
      "debian_ubuntu": [
        "docker.io"
      ],
      "arch": [
        "docker"
      ]
    },
    "related_commands": [
      "docker-compose build",
      "podman build",
      "buildah"
    ],
    "complexity": "intermediate",
    "tags": [
      "docker",
      "performance",
      "optimization",
      "devops",
      "caching",
      "multi-stage-build"
    ]
  },
  {
    "id": "scenario_aws_s3_boto3_031",
    "command": "python",
    "scenario": "Automate creating an S3 bucket and uploading a file using Python and Boto3.",
    "problem": "A user needs to programmatically interact with Amazon S3 as part of an application or a deployment script. They want to create a new S3 bucket if it doesn't exist and then upload a local file to it.",
    "solution": "Use Boto3, the official AWS SDK for Python, to create a script that handles AWS authentication, S3 bucket creation (with proper region handling), and file uploads with a progress bar.",
    "steps": [
      "USER SAYS: 'I need to write a Python script that uploads a backup file to an AWS S3 bucket. How can I do that?'",
      "AGENT: 'That's a perfect job for Boto3, the AWS SDK for Python. It makes interacting with services like S3 very straightforward. Let's research the exact steps for creating a bucket and uploading a file.'",
      "AGENT: ''",
      "AGENT: '\ud83d\udd0d RESEARCH PHASE: Using Boto3 for S3 operations...'",
      "AGENT: '[Searching] \"boto3 create s3 bucket and upload file\"'",
      "AGENT: '[Found] `boto3.client('s3')`: The low-level client for S3 operations.'",
      "AGENT: '[Found] `s3_client.create_bucket()`: The function to create a bucket. It requires a `Bucket` name and a `LocationConstraint` (region).'",
      "AGENT: '[Found] `s3_client.upload_file()`: The recommended method for uploading files. It handles multipart uploads for large files automatically.'",
      "AGENT: '[Security] Boto3 can get credentials from environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`), which is more secure than hardcoding them.'",
      "AGENT: '\u2713 Research complete! Let's build the script.'",
      "AGENT: ''",
      "AGENT: '--- Approach 1: Setup and Authentication ---'",
      "AGENT: 'First, you need to install Boto3 and configure your AWS credentials. The most secure way is using environment variables.'",
      "Run command: `pip install boto3 tqdm` (tqdm is for a nice progress bar)",
      "AGENT: ''",
      "AGENT: 'Now, configure your credentials in your shell. You can get these from the IAM section of the AWS console.'",
      "Run command: `export AWS_ACCESS_KEY_ID=\"YOUR_ACCESS_KEY\"`",
      "Run command: `export AWS_SECRET_ACCESS_KEY=\"YOUR_SECRET_KEY\"`",
      "Run command: `export AWS_DEFAULT_REGION=\"us-east-1\"`",
      "AGENT: '\u26a0\ufe0f Never commit these keys to a Git repository!'",
      "AGENT: ''",
      "AGENT: '--- Approach 2: Build the S3 Uploader Script ---'",
      "AGENT: 'Let's create a reusable script for this task.'",
      "Create tool: `cat > s3_uploader.py << 'EOF'`",
      "#!/usr/bin/env python3\n\n\"\"\"A script to create an S3 bucket and upload a file using Boto3.\"\"\"\n\nimport boto3\nimport logging\nfrom botocore.exceptions import ClientError\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef upload_to_s3(file_path_str, bucket_name, region):\n    \"\"\"Creates a bucket if needed and uploads a file to it.\"\"\"\n    s3_client = boto3.client('s3', region_name=region)\n\n    # 1. Create the bucket\n    try:\n        # Note: Buckets in us-east-1 don't need a LocationConstraint\n        location = {'LocationConstraint': region} if region != 'us-east-1' else {}\n        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)\n        logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n            logging.info(f\"Bucket '{bucket_name}' already exists. Skipping creation.\")\n        else:\n            logging.error(f\"An error occurred creating the bucket: {e}\")\n            return False\n\n    # 2. Upload the file\n    file_path = Path(file_path_str)\n    if not file_path.is_file():\n        logging.error(f\"File not found: {file_path_str}\")\n        return False\n\n    object_name = file_path.name\n    file_size = file_path.stat().st_size\n    \n    logging.info(f\"Uploading '{file_path_str}' to '{bucket_name}/{object_name}'...\")\n    try:\n        with tqdm(total=file_size, unit='B', unit_scale=True, desc=object_name) as progress_bar:\n            s3_client.upload_file(\n                str(file_path),\n                bucket_name,\n                object_name,\n                Callback=lambda bytes_transferred: progress_bar.update(bytes_transferred)\n            )\n    except ClientError as e:\n        logging.error(f\"An error occurred uploading the file: {e}\")\n        return False\n    except FileNotFoundError:\n        logging.error(f\"The file was not found: {file_path_str}\")\n        return False\n\n    logging.info(\"\\nUpload complete! \u2705\")\n    return True\n\nif __name__ == '__main__':\n    import os\n    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n    \n    # --- Configuration ---\n    # It's better to get these from args or env vars in a real app\n    BUCKET = 'my-unique-backup-bucket-1234567890' # S3 bucket names must be globally unique!\n    FILE_TO_UPLOAD = 'backup.zip'\n    AWS_REGION = os.environ.get('AWS_DEFAULT_REGION', 'us-east-1')\n\n    # Create a dummy file for the example\n    with open(FILE_TO_UPLOAD, 'wb') as f:\n        f.write(os.urandom(20 * 1024 * 1024)) # Create a 20MB file\n\n    upload_to_s3(FILE_TO_UPLOAD, BUCKET, AWS_REGION)\n",
      "EOF",
      "chmod +x s3_uploader.py",
      "AGENT: '\u2713 Created the S3 uploader script!'",
      "AGENT: ''",
      "AGENT: '--- Approach 3: Run the Script ---'",
      "Run command: `./s3_uploader.py`",
      "AGENT: 'INFO: Bucket 'my-unique-backup-bucket-1234567890' created successfully.'",
      "AGENT: 'INFO: Uploading 'backup.zip' to 'my-unique-backup-bucket-1234567890/backup.zip'...' 'backup.zip: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.0M/20.0M [00:02<00:00, 8.50MB/s]' 'INFO: ' 'Upload complete! \u2705'",
      "AGENT: ''",
      "AGENT: 'Let's run it again to see the 'bucket already exists' logic.'",
      "Run command: `./s3_uploader.py`",
      "AGENT: 'INFO: Bucket 'my-unique-backup-bucket-1234567890' already exists. Skipping creation.'",
      "AGENT: 'INFO: Uploading 'backup.zip' to ...'",
      "AGENT: '(...upload proceeds...)'",
      "AGENT: ''",
      "AGENT: '\u2713 SOLUTION SUMMARY:'",
      "AGENT: '  \u2713 Researched the Boto3 library for S3 operations.'",
      "AGENT: '  \u2713 Set up AWS credentials securely using environment variables.'",
      "AGENT: '  \u2713 Built a Python script that idempotently creates an S3 bucket (it doesn't fail if the bucket already exists).'",
      "AGENT: '  \u2713 Implemented file uploading using `upload_file`, which is efficient for files of any size, and added a `tqdm` progress bar for a better user experience.'",
      "AGENT: '  \u2713 You now have a reusable script to programmatically manage S3 uploads. \u2601\ufe0f'"
    ],
    "commands": [
      "python3",
      "pip",
      "export",
      "cat",
      "chmod"
    ],
    "packages": {
      "debian_ubuntu": [
        "python3-pip",
        "awscli"
      ],
      "arch": [
        "python-pip",
        "aws-cli"
      ]
    },
    "related_commands": [
      "aws s3 cp",
      "aws s3 mb",
      "rclone"
    ],
    "complexity": "intermediate",
    "tags": [
      "python",
      "aws",
      "s3",
      "boto3",
      "cloud",
      "automation",
      "devops",
      "custom-tool"
    ]
  }
]